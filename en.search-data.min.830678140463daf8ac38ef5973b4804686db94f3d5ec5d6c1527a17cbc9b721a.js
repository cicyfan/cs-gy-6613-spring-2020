'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/','title':"Course Introduction",'content':"AI Evolution according to DARPA If engineering difficulty has a pinnacle today this must be in AI domains that combines ML, optimal control and planning. autonomous cars and humanoids from Boston Dynamics fit the bill.\nInitially there were rules.\n In the 1980s knowledge-base systems that hard-coded knowledge about the world in formal languages.  IF this happens, THEN do that.   They failed to get significant traction as the number of rules that are needed to model the real world exploded. However, they are still in use today in vertical modeling domains e.g. fault management. For example Rule Based Engines are used today in many complex systems that manage mission critical infrastructures e.g. ONAP.  The introduction of advanced AI methods few years ago, created a situation we can explain with the following analogy.\nA nautical analogy on where we are today on AI for mission critical systems. Can you notice anything strange with this ship (Cumberland Basin, photo taken April 1844)?\nTo put order into the many approaches and methods for delivering AI in our lives, DARPA classified AI development in terms of \u0026ldquo;waves\u0026rdquo;.\n         Wave I: GOFAI Wave II: Connectionism Wave III: AGI   In the 1980s Rule Based Engines started to be applied manifesting the first wave of AI introduction. In this example you see a system that performs highway trajectory planning. A combination of cleverly designed rules does work and offers real time performance but cannot generalize and therefore have acceptable performance in other environments.\nWave II srarted soon after 2010 - we started to apply a different philosophy in solving intelligent tasks such as object classification. The philosophy of connectionism and the so called deep neural network architectures, dominate today relative simple (and mostly self-contained) tasks.\nWave III is at present an active research area driven primarily from our inability to implement with just deep neural networks things like long-term goal planning, causality, extract meaning from text like humans do, explain the decisions of neural networks, transfer the learnings from one task to another, even similar, task. Artificial General Intelligence is the term usually associated with such capabilities.\nFurther, we will see a fusion of disciplines such as physical modeling and simulation with representation learning to help deep neural networks learn using data generated by domain specific simulation engines.\nReveal the stenosis:Generative augmented physical (Computational Fluid Dynamics) modeling from Computer Tomography Scans\nFor example in the picture above a CFD simulation is used to augment ML algorithms that predict and explain those predictions. I mission critical systems (such as medical diagnostic systems) everything must be explainable.\nConsult the course Syllabus to understand what elements of Wave II AI systems we will cover in this course.\n"});index.add({'id':1,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/numpy-pandas/','title':"Numerical Python (Numpy/Scipy and Pandas) Tutorials",'content':"Standard Python Below is a list of recommended courses you can attend to. We will go over briefly basic Python in this lecture. The tutorials below are self contained and can remind you the basics.\n  CodeAcademy Data Science Path. Take Python modules 4-10. This course contains Numpy and Panda intro as well.\n  Kaggle Python Course\n  Google Python Class This is a bit dated as it covers Python 2, but it is still highly regarded as Python 3 and 2 have few differences.\n  Numpy specific We will go Numpy Tutorial from Stanford\u0026rsquo;s CS231n.\n Numpy Cheatsheet\n Pandas (optional) Pandas may not be needed for the AI course. In any case, this is the effectively \u0026ldquo;official\u0026rdquo; documentation on Pandas: Pandas: powerful Python data analysis toolkit\n"});index.add({'id':2,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/','title':"A systems approach to AI",'content':"The four approaches towards AI The Turing Test Approach A 5-min behavioral intelligence test, where an interrogator chats with the player and at the end it guesses if the conversation is with a human or with a programmed machine. A Turing contest (Loebner Prize) is is held annually since 1991.\nThis course\u0026rsquo;s projects includes the Alexa prize which is not a turing test. The Alexa Prize creates social bots that engage in interesting, human-like conversations, not to make them indistinguishable from a human when compared side-by-side. Social bots may have ready access to much more information than a human.\n       Summary of the Turing Test The Alexa Prize is not a Turing Test   What capabilities we need to have to pass a turing test.\n Natural Language Processing Knowledge Representation Automated Reasoning Machine Learning Computer Vision Robotics  The last two capabilities are not needed for the verbal oriented Turing test but they are needed for what is called the total Turing test. According to this test, the player and the interrogator can communicate physically. For example, there is a hatch where the interrogator can pass objects to the player through. Obviously the player must have perception abilities to understand what object it is (5) and possibly a body that can manipulate the object (6). Embodied AI research is one of the hotter areas of AI today as we will see in the Agents section.\nThe Cognitive Model approach Newell, in his book \u0026ldquo;Unified Theories of Cognition\u0026rdquo;, defines cognition as:\n Memory, learning, skill Perception, motor behavior Problem solving, decision making, routine action Language Motivation, emotion Imagining, dreaming   How do we differentiate a cognitive model from a conceptual or statistical model? “Cognitive science is concerned with understanding the processes that the brain uses to accomplish complex tasks including perceiving, learning, remembering, thinking, predicting, problem solving, decision making, planning, and moving around the environment. The goal of a cognitive model is to scientifically explain one or more of these basic cognitive processes, or explain how these processes interact.”, \u0026ndash;Busemeyer \u0026amp; Diederich (2010)\n These theories of cognitive processes are tested via various cognitive architectures. Its important to realize that much of todays\u0026rsquo; debate about the path ahead in AI maps to the few different architectures. The hybrid architecture (symbolic and connection-oriented) is what is being investigated today by many research institutions.\nThe Syllogism-based approach The translation from Greek of the word syllogism is to support logic.\nThis approach emphasizes how we think the right way and encodes the pattern of logical arguments that can reach correct conclusions from a set of propositions (premises). Problem solving where the problem statement is expressed in a logic notation, matured in the 60s. As we have seen in the course introduction such rule-based systems are still with us and for small and deterministic state spaces can provide a very compact and elegant way to inference.\nLogic-based reasoning is coming back to fashion. One of the most promising areas is their application to interpretability (also known as explainability ) of deep learning based methods for e.g. classification in medical diagnosis. Probabilistic Logic Networks (PLN) are extensions of this approach to address problems with uncertainty.\nThe Rational Agent approach A rational agent acts to achieve the best outcome. The rational approach encompasses the syllogism and Turing-test approaches. We still need provably correct inference and the formal representations of logic as well as the ability to perceive, communicate, and learn to achieve a good outcome. But we need to generalize these approaches to include \u0026ldquo;good-enough\u0026rdquo; inference and adaptation to the changing environment contexts that the agent is facing without giving up on the mathematical formality that utility theory allows us to design such agents.\nThe agent facing a fire is an instructive example. There maybe no time for optimal reasoning to a conclusion (e.g. run) but a simple reflexive plan can offer the best outcome.\nAI as a distributed system approach As its evident from all existing approaches towards AI, multidisciplinary science that aims to create agents that can think and act humanly or rationally. This course starts the new decade filled with the promises of the previous one - AI is not only around the corner and it can take several decades of R\u0026amp;D for it to match human intelligence. Our purpose here is to (a) understand and appreciate the significant progress that certain components of AI have made over the last few years and (b) to be able to synthesize such components into AI systems that can at least solve domain-specific problems. In other words we are not trying to solve the most difficult and general AI problem as we don\u0026rsquo;t know its solution. We also can\u0026rsquo;t wait as we would like to participate in the GAI developments to begin with.\nA substantial part of AI is machine learning (ML) and that component alone is worth of at least a couple semesters. ML nowadays is used to process the visual sensing (computer vision), verbal commands (speech to text) and many other front-end functions using structures known as Deep Neural Networks (DNNs). These functions are usually effective in modeling the reflexive part of human brain. Their performance sometimes hides the enormous efforts by R\u0026amp;D teams to create carefully curated datasets for the task at hand. When supervised datasets are not enough for the design of reflexive agents policies, we need additional tools such as Deep Reinforcement Learning that offer the possibility to learn agent control policies from world models (or even without them) that in many instances means spending considerable time simulating the environment.\nAI is a system with the ability to represent the world and abstract concepts at multiple levels. If we are to draw the architecture of such system, it will have the ability to quickly change depending on the domain and task at hand. Just like origami, AI systems will morph into a suitable architecture, facilitated by high speed interconnections between its subsystems. The controller that controls such changes must be topology aware i.e. knowing the functional decomposition of the AI system and what support for representations and abstractions each subsystem can offer. How these can be combined and ultimately used, is something that needs to be learned. To generalize, such morphic control agents must be able to perform across task domains.\nAI distributed system comprising from a number of high-speed interconnected subsystems that are loosely coupled and communicate via a universal language. Line thickness indicates stronger coupling / dependecies between subsystems for the task at hand at this moment in time.\nIn a limited demonstration of such ability, closed worlds such as games, we have agents that can process thousands of pixels and can create abstractions at the symbolic level. Are they able to generalize ? Doubtful. Which brings us to the very interesting thought. For the vast majority of mission critical industries, we may reach in this decade a good enough performance level. The internet didn\u0026rsquo;t have 1 Gbps at each household even 5 years ago. But the moment we crossed the 1 Mbps level per user, at the hands of innovators, it managed to change the world as we know it despite its many initial performance issues. The internet does not kill, many people will argue but if anyone believes this analogy, todays\u0026rsquo; AI architecture, a bunch of service-oriented silos (APIs) offered by major technology firms, resembles the disconnected/siloed PC before the invention of HTTP and the internet era of the 90s. The protocol and controls that will allow such AI systems to communicate and by doing so demonstrate an ability to synthesize a non-trivial version of intelligence is one of the missing links.\nThe architecture of serf-driving cars in the late 2010s. To avoid wondering around the various disconnected use cases, we need to pick a domain that we can use as an application theme. Given the importance of the mission critical industries in the economy of every country, in this course we have selected robotics / self-driving cars. This domain requires the design of advanced agents that perceive the environment using noisy sensors, make decisions under uncertainty, actuate a host of electronics to execute decisions, communicate with humans in natural language or be able to sense driver psychological state and many more.\n"});index.add({'id':3,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/linear-regression/','title':"Linear Regression",'content':"Linear Regression Now that we have introduced somewhat more formally the learning problem and its notation lets us study a simple but instructive regression problem from Chapter 1 of Bishop\u0026rsquo;s book that is known in the statistics literature as shrinkage. Note that in many figures below the label is denoted as $t$ rather than $y$ as used in the equations below.\nSuppose that we are given the training set $\\mathbf{x} = {x_1,\u0026hellip;,x_m}$ together with their labels, the vectors $\\mathbf{y}$. We need to construct a model such that a suitably chosen loss function is minimized for a different set of input data, the so-called test set. The ability to correctly predict when observing the test set, is called generalization.\nTraining Dataset (m=10) for the Regression Model. The green curve is the uknown target function.\nSince the output $y$ is a continuous variable then the supervised learning problem is called a regression problem (otherwise its a classification problem). The dataset is generated (in data scienece these datasets are called synthetic) by the function $sin(2 \\pi x) + ϵ$ where $x$ is a uniformly distributed random variable and $ϵ$ is $N(\\mu=0.0, \\sigma^2=0.3)$. This target function is completely unknown to us - we just mention it here for completeness.\nLet us now pick the hypothesis set that correspond to polynomials of the following form,\n$$g(\\mathbf{w},x_i) = w_0 + w_1 x_i + w_2 x_i^2 + \u0026hellip; + w_M x_i^M$$\nOur job is to find $\\mathbf{w}$ such that the polynomial above fits the data we are given - as we will see there are multiple hypothesis that can satisfy this requirement. To gauge our investigation, we need to define a metric, an error or loss function in fact, that is also a common metric in regression problems of this nature. This is the Mean Squared Error (MSE) function.\n$$L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m {(g(\\mathbf{w},x_i)-y_i)}^2$$\nThe loss function chosen for this regression problem, corresponds to the sum of the squares of the displacements of each data point and our hypothesis. The sum of squares in the case of Gaussian errors gives raise to an (unbiased) Maximum Likelihood estimate of the model parameters. Contrast this to sum of absolute differences.\nNow our job has become to choose two things: the weight vector $\\mathbf{w^*}$ *and* $M$ the order of the polynomial. **Both** define our hypothesis. If you think about it, the order $M$ defines the model complexity in the sense that the larger $M$ becomes the more the number of weights we need to estimate and store. Obviously this is a trivial example and storage is not a concern here but treat this example as instructive for that it applies in many far for complicated settings.\nObviously you can reduce the training error to almost zero by selecting a model that is complicated enough (M=9) to perfectly fit the training data (if m is small).\nBut this is not what you want to do. Because when met with test data, the model will perform far worse than a less complicated model that is closer to the true model (e.g. M=3). This is a central observation in statistical learning called overfitting. In addition, you may not have the time to iterate over M (very important in online learning settings).\nTo avoid overfitting we have multiple strategies. One straightforward one is evident by observing the wild oscillations of the $\\mathbf{w}$ elements as the model complexity increases. We can penalize such oscillations by introducing the $l_2$ norm of $\\mathbf{w}$ in our loss function.\n$$L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m {(g(\\mathbf{w},x_i)-y_i)}^2 + \\frac{\\lambda}{2} ||\\mathbf{w}||^2$$\nThis type of solution is called regularization and because we effectively shrink the weight dynamic range it is also called in statistics shrinkage or ridge regression. We have introduced a new parameter $\\lambda$ that regulates the relative importance of the penalty term as compared to the MSE. This parameter together with the polynomial order is what we call hyperparameters and we need to optimize them as both are needed for the determination of our final hypothesis $g$.\nThe graph below show the results of each search iteration on the $\\lambda$ hyperparameter.\nLets reflect on the MSE and how model complexity gives raise to various generalization errors.\n$$MSE = \\mathbb{E}[\\hat{y}_i - y_i)^2] = \\mathrm{Bias}(\\hat{y}_i)^2 + \\mathrm{Var}(\\hat{y}_i)$$\nwhich means that the MSE captures both bias and variance of the estimated target variables and as shown in the plots above, increasing model capacity can really increase the variance of $\\hat{y}$. We have seen that as the $\\mathbf{w}$ is trying to exactly fit, or memorize, the data, it minimizes the bias (in fact for model complexity M=9 the bias is 0) but it also exhibits significant variability that is itself translated to $\\hat{y}$. Although the definition of model capacity is far more rigorous, we will broadly associate complexity with capacity and borrow the figure below from Ian Goodfellow\u0026rsquo;s book to demosntrate the tradeoff between bias and variance. What we have done with regularization is to find the $\\lambda$ that minimized generalization error aka. find the optimal model capacity.\nAs capacity increases (x-axis), bias (dotted) tends to decrease and variance(dashed) tends to increase, yielding another U-shaped curve for generalization error (bold curve). If we vary capacity along one axis, there is an optimal capacity, with underﬁtting when the capacity is below this optimum and overﬁtting when it is above.\nOLS as Maximum Likelihood Estimation These notes summarize the Maximum Likelihood approach and are provided here as they are more expansive compared to textbooks and at the same time do make the connection to the Linear Algebra background you should have.\nGeometrical Interpretation of Ordinary Least Squares (OLS)\n"});index.add({'id':4,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/','title':"The Way of Working in AI",'content':"A Positive Reinforcement Loop What are the disciplines that need to cross fertilize to get a system that possesses intelligence? Lets start with a diagram that show not only the disciplines but also a way of working for the many specialists involved.\nThe diagram above highlights three fundamental axes that can deliver a system-based approach to AI. The Z axis is the scientific axis where many disciplines such as psychology, neuroscience, mathematics and others make progress on. The X axis involves the ML/AI communities that borrow ideas from their colleagues in sciences and convert those theories and pragmatic findings into abstractions (models and methods). The model of the neuron, the perceptron, appeared in psychology journals many decades ago and despite its simplicity it is still the unit via which much more complicated neural networks are constructed from. Todays\u0026rsquo; models of Long-Term Short-Term Memory (LSTM), Replay Memory and many others not shown in the diagram (as its currently in draft form) are abstractions (models) of discoveries that scientists produced after tens of years of research. To use however these methods and models effectively, major hardware and software components need to be developed also known as computing and frameworks - these live in the Y axis. They are very important for the development of AI field that is known to be heavily experimental, requiring especially at the perceptive frontend significant computational power and automation.\nAt a very high level, progress in AI is made via the counterclockwise iteration Z -\u0026gt; X -\u0026gt; Y -\u0026gt; Z. AI engineers look at the neuroscience/psychology axis, map discoveries to points in the methods / models axis, and finally develop these methods in hardware architectures and software frameworks. But what can explain the Y -\u0026gt; Z flow? Frameworks in turn help the neuroscientists and psychologists as they can provide generative models of their own discoveries or help them simulate conditions that are not possible using their native tools.\nThis counter-clockwise multidisciplinary iteration acts as a positive feedback loop accelerating the progress in the AI space.\nIn this course we will be focusing on the methods/models and frameworks axis and understand what these models can offer us and how we can apply them in synthesizing an AI system at least for a domain of interest.\nA typical AI stack today As we have seen from the syllabus, this course approaches AI from an applied perspective - this means teaching concepts but at the same time looking how these concepts are applied in the industry to solve real world problems. In this respect here we take an architecture driven AI, presenting the components of AI in a form of a software stack but also how the components are mechanized in what we call ML Pipelines to provide the ML utility to applications. For a complete overview of real world ML pipelines used today go through the TFX paper in its entirety.\nAI Stack circa 2019\nLandscape of the AI ecosystem Due to the complexity and common interest to addresses industrial players are partnering to define and implement the necessary components for the complete automation of AI pipelines. This work is going in within the Linux Foundation AI (sub)Foundationamongst many other open source communities.\n   The four pipelines of an end-to-end ML platform Example of end to end pipeline - serial arrangement\nExample of Data Pipeline\nExample of Model Training Pipeline\nExample of Model Evaluation and Validation Pipeline\nExample of Serving Pipeline\nRoles in AI product development Who data scientists need to interact with, during the development of AI systems?\n \u0026ldquo;Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\u0026rdquo; http://www.melconway.com/Home/Conways_Law.html\n \u0026ldquo;We do research differently here at Google. Research Scientists aren\u0026rsquo;t cloistered in the lab, but instead they work closely with Software Engineers to discover, invent, and build at the largest scale.\u0026rdquo;\nContrast this to an organizational structure that isolates researchers from product development. What about Alphabet\u0026rsquo;s X https://x.company/ ?\n"});index.add({'id':5,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/','title':"Intelligent Agents and Representations",'content':"Agent-Environment Interface  An agent is a computer system that is situated in some environment, and that is capable of autonomous action in this environment in order to meet its design objectives.\n In general sensor data are converted via the agent function (that is implemented via a program) into actions as shown below.\nGeneral Agent-Environment Interface\nThe two most important agent architectures that we will deal with in this course are the utility and learning-based agents architectures. To start with we recognize that most of the problems we will face as agent designers are for agents operating in environments that are:\n Partially Observed (PO). This means that we cant see all the variables that constitute the state and we need to maintain an internal belief of the state variables that we cant perceive. Stochastic. This means that the environment state is affected by random events and can not be determined by the previous state and the actions of the agent (which in this case we are talking about deterministic environments). Such probabilistic characterization of the environment state is the norm in many settings such as as robotics, self-driving cars etc. Sequential As compared to episodic, in sequential environments actions now can have long term effects into the future. Dynamic In this setting, the environment state changes all the time, even while the agent is taking the action based on the sequence of percepts up to this point in time. In most settings the environments we deal will not be static. Continuous When the variables that constitute the environment state are defined in continuous domains. Time is usually considered a special variable and we may have environments where the time variable is discrete while other variables are continuous. Known This refers to the knowledge of the agent rather than the environment. In most instances we are dealing with environments where there is a set of known rules that govern the state transition. In driving for example, we know what steering does.  Architectures Rational Agent Architecture In the rational agent architecture we meet three key concepts:\n The need for the agent to keep internally the environment state (in probabilistic terms a belief). This is needed due to the the partially observed environment the agent is interfacing with. The presence of a world model that helps the agent to update its belief. The presence of a utility function that the agent can use to produce the value (happiness) when its action transitions the environment to a new state. Obvious the agent will try to optimize its actions in what we earlier described stochastic environments and therefore it will try to maximize the value (hapiness) on average (strictly in expectation) where the average is taken across the distribution of all possible states across time.  Learning Agent Architecture The learning agent architecture builds on top of the rational agent (the performance element in the figure below), additional functions that:\n Embeds a learner that learns the various models needed by the rational agent as well as allowing the rational agent to operate on unknown environments. In this respect it learns the world model, some elements of the utility function itself or the desirability of each actions it takes. To enable learning, the rational agent sends training data to the learner. Introduces a critic that transmits a positive or negative reward to the learner based on its own view of how the agent is doing. The learner can modify these models to make the rational agent perform better in the future. Introduces the problem generator that can change the problem statement of the rational agent. Obviously the expected utility objective will not change but the utility function itself may in fact change to lead the agent to perform more exploration (increase its risk) in its environment.  We will see in Deep Reinforcement Learning that this architecture is able to accommodate such end to end learning approach. In that setting the critic is part of the environment - see Solving sparse-reward tasks with Curiosity for an example where the critic is inside the agent generating intrinsic rewards.\nSolution space For each environment and each architecture there is a number of solutions that may be appropriate. The following figure presents the environment type to algorithm mapping that we will cover in this book.\n"});index.add({'id':6,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/knn/','title':"k-Nearest Neighbors (kNN) Classification",'content':"k-Nearest Neighbors (kNN) Classification kNN belongs to the class of algorithms that were extensively treated in pattern recognition literature many years ago. It is still extensively being used today especially in settings that require very fast decision/classifications. The general block diagram governing such systems is shown below.\nOne example that was originally treated by Patrick Winston (MIT) is the conveyor belt classification use case.\nLets assume that in a factory a high speed conveyor belt is carrying hundreds of widgets per minute and a system of cameras is used to classify them and instruct actuators that place them into bins. Obviously the system must satisfy strict throughput requirements. Lets assume for purely instructive purposes that the assignment of each widget is based on two features that shown below.\nExample decision boundaries between labels in the feature space.Stars are the distinct widgets (labels)\nEach pair of prototype widgets can be considered as defining a linear decision boundary - the line perpendicular to the line that connects them. So if the widget was screws, figure below shows two features categories, head-type and length.\nIn a hypothetical case of manufacturing screws with two lengths $l_1, l_2$ and two head types (flat and round) we will have four possible labels (each associated wiht the combination ${(l_1, F), (l_2, F), (l_1,R), (l_2, R)}$ as shown in the picture above and we can draw corresponding decision boundaries. Why the decision boundaries are like this (also called perpendicular bisectors) ? The decision rule is a Euclidean distance metric - any point in that line has the same distance to either of the two labels involved.\nAs screws go through the conveyor depth, manufacturing defects cause each screw to appear almost anywhere in the feature space. One intuitive and straightforward approach is to assign the label associated with the area enclosed by the decision boundaries (and the axes) and classify the screw as the label of the corresponding prototype widget. This is in essence the principle behind the k nearest points (or neighbors) algorithm.\nIn a sightly more formal setting, and to be able to address far more complex decision boundaries than the above, we are given data points in a training set $D = {(x_i,y_i)}, i={1, \u0026hellip;, m}$ and we are asked to classify points that are in the test set. The only variable of the kNN algorithm is the number $k$ which is the number of nearest neighbors that we consider in the classification decision. An example for two classes is shown in the figures below for two cases of $k$. The plot corresponds to the case we have two features like before $x_1, x_2$.\nk=3\nk=1\nThe algorithm effectively positions a sphere on the data point we want to classify whose radius is large as the it needs to be to enclose $k$ closest points irrespectively of their class. Obviously for the dimensions of the examples above, the sphere is a circle. As expected, we see that $k$ controls the degree of smoothing, so that small $k$ produces many small regions of each class, whereas large $k$ leads to fewer larger regions. In essence the algorithm for $k\u0026gt;1$, considers a majority vote between the $k$ closest points to the point we need to classify with ties broken at random.\nOne of the limitations of the knn algorithm is the requirement that the dataset $D$ is stored in memory and that the algorithm itself is dependent on efficient search algorithms that allow it to go over the data and find the nearest neighbors. There are solutions to both of these problems though and if implemented properly we get to be able to train for fairly complex decision boundaries that generalize well without the complexity associated with learning parametric models.\nNOTE: Although not part of this class, think about how to generalize knn. What happens when the votes are not binary ? How the algorithm can be adapted to online learning settings?\n"});index.add({'id':7,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/perceptron/','title':"The Perceptron",'content':"The Perceptron Frank Rosenblatt\nThis section captures the main principles of the perceptron algorithm which is the essential building block for neural networks.\nArchitecture of a single neuron The perceptron algorithm invented 60 years ago by Frank Rosenblatt in Cornell Aeronautical Laboratory. Neural networks are constructed from neurons - each neuron is a perceptron with a specific activation function. A single neuron is itself capable of learning \u0026ndash; indeed,various standard statistical methods can be viewed in terms of single neurons \u0026ndash; so this model will serve as a first and simple example of a supervised neural network.\nA single neuron has a number $n$ of inputs $x_i$ (note the figure is not compatible with this notation) and one output which we will here call $\\hat{y}$. Associated with each input is a weight $w_i$ ($i = 1 ,\\ldots, n$). The additional parameter $w_0$ of the neuron called a bias which we may view as being the weight associated with an input $x_0$ that is permanently set to 1. The single neuron is a feedforward device \u0026ndash; the connections are directed from the inputs to the output of the neuron. Feedforward neural networks are called Multi-Layer Perceptrons (MLPs).\nWhat does the perceptron compute? First, in response to the imposed inputs $\\mathbf{x}$, we compute the activation of the neuron,\n$$a = \\sum_{i=0}^n w_i x_i = \\mathbf{w}^T \\mathbf{x}$$\nSecond, the output (also called the activity of the neuron) is produced by passing the activation through a non linear activation function $\\hat{y} = g(a)$. The activation function of the perceptron is the step function - we will cover more of such functions in the treatment of neural networks.\n$$g(a) = \\begin{cases}1 \u0026amp; \\text{if }\\ a \\ge 0, \\\\ -1 \u0026amp; \\text{otherwise}\\end{cases}$$\nPerceptron Learning Algorithm The algorithm is derived from the application of the SGD to a suitably chosen loss function. The loss function can be easily designed if we start thinking about the class labels as belonging to the set ${+1,-1}$ (rather than the more usual ${0,1}$) and considering the value of the products $\\mathbf{w}^T x_j y_j$. If there are no classification errors for the chosen non-linear activation function above such products will result into positive numbers irrespectively of the class. For these cases we assign zero to the loss function. If there are errors however, these products will be negative and the sum of all these negative product terms we must maximize - or equivalently minimize the negative of such loss as below:\n$$L(\\mathbf{w}) = - \\sum_{j: \\hat{y_j} \\ne y} \\mathbf{w^T}x_j y_j$$\nWe will find the $\\mathbf{w}$ that minimize such loss using the familiar Stochastic Gradient Descent algorithm. Noting that the gradient of the loss function at $\\mathbf{w}$ is $x_j y_j$ we can write the SGD algorithm as follows:\nLet $t$ denote the iteration index and $r$ the learning rate.\n  Initialize the weights and the threshold. Weights may be initialized to zero or to a small random value.\n  For each example in our training set, perform the following steps over the input $\\mathbf{x}_j$ and desired output $y_j$:\n$$\\hat{y}_j(t) = g[\\mathbf{w}(t)^T \\mathbf{x}_j]$$\n  Update the weights:\n$$w_i(t+1) = w_i(t) + r (y_j - \\hat y_j(t)) x_j$$\n  for all features $0 \\leq i \\leq n$.\nThe updated weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.\nInitial parameter vector $\\mathbf w$ shown as a black arrow together with the corresponding decision boundary (black line), in which the arrow points towards the decision region which classified as belonging to the red class. The data point circled in green is misclassified and so its feature vector is added to the current weight vector, giving the new decision boundary shown in the plot below.\nThe next misclassified point to be considered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundary shown in the plot below for which all data points are correctly classified.  NOTE: For offline learning, the second step may be repeated until the iteration error $\\frac{1}{s} \\sum_{j=1}^s |y_j - \\hat{y}_j(t)| $ is less than a user-specified error threshold $\\gamma $, or a predetermined number of iterations have been completed, where \u0026lsquo;\u0026lsquo;s\u0026rsquo;\u0026rsquo; is the size of the training set.\n The perceptron is a linear classifier, therefore it will never get to the state with all the input vectors classified correctly if the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane. In this case, no \u0026ldquo;approximate\u0026rdquo; solution will be gradually approached under the standard learning algorithm, but instead learning will fail completely. Even in the case of linearly separable datasets, the algorithm may exhibit significant variance while it is executing as previously correctly classified examples may \u0026ldquo;fall\u0026rdquo; into the wrong decision region by an update that considers a currently misclassified example.\nFurther, the perceptron solution will depend on the initial choices of the parameters as well as the order of the training dataset presented. Support Vector Machines avoid such pitfalls which can motivate the question why we insisted on learning the perceptron algorithm: both architecturally and the functionally the linear combination of features followed by a non-linearity is the fundamental building block of far more complicated neural networks.\nPerceptron performance For a live demo of perceptron performance see the single neuron example for separable datasets in Tensorflow Playground\n"});index.add({'id':8,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/logistic-regression/','title':"Logistic Regression",'content':"Logistic Regression Logistic regression is used in machine learning extensively - every time we need to provide probabilistic semantics to an outcome e.g. predicting the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.), whether an voter will vote for a given party, predicting the probability of failure of a given process, system or product, predicting a customer\u0026rsquo;s propensity to purchase a product or halt a subscription, predicting the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in Natural Language Processing (NLP).\nBinary case If we consider the two class problem, we can write the posterior probability as,\n$$p(\\mathcal{C}_1|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1)}{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1) + p(\\mathbf{x}|\\mathcal{C}_2) p(\\mathcal{C}_2)} = \\frac{1}{1 + \\exp(-\\alpha)} = \\sigma(\\alpha)$$\nwhere $\\alpha = \\ln \\frac{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1)}{p(\\mathbf{x}|\\mathcal{C}_2) p(\\mathcal{C}_2)}$ and $\\sigma$ is given by the logistic function we met in probability review.\nGiven the posterior distribution above we have for the specific linear activation,\n$$p(\\mathcal{C}_1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x})$$\nThis model is what statisticians call logistic regression - despite its name its a model for classification. The model has significant advantages in that it does require the estimation of far fewer parameters compared to the case where the class conditional distributions involved in the posterior were parametric. For example if we had Gaussian class conditionals we would had to estimate (using Maximum Likelihood) their parameters $\\mathbf \\mu$ and $\\mathbf \\Sigma$ that grow quadratically to the number of features $n$. With logistic regression we only have an evident linear relationship between parameters and features.\nThe figure below shows the corresponding posterior distribution $p(\\mathcal{C}_1|\\mathbf{x})$\nThe class-conditional densities for two classes, denoted red and blue. Here the class-conditional densities $p(\\mathbf{x}|\\mathcal{C}_1)$ and $p(\\mathbf{x}|\\mathcal{C}_2)$ are Gaussian\nThe corresponding posterior probability for the red class, which is given by a logistic sigmoid of a linear function of $\\mathbf{x}$.\nAs we said, with logistic regression we skip the assumption about the class-conditional densities as they add parameters to our problem that grow quadratic to the number of dimensions and we attempt to find the $n$ parameters of the model directly (the number of features) and sure enough we will use ML to do so.\nBy repeating the classical steps in ML methodology i.e. writing down the expression of the likelihood function (this will now be a product of binomials), we can write down the negative log likelihood function as,\n$$L(\\mathbf{w}) = - \\ln p(\\mathbf{y},\\mathbf{w}) = - \\sum_{i=1}^m {y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) }$$\nwhich is called cross entropy error function - probably the most widely used error function in classification due to its advantages such as its probabilistic and information theoretic roots as well as its shape shown in the figure below. Sometimes it is also called log-loss.\nMinimizing the error function with respect to $\\mathbf{w}$ by taking its gradient\n$$\\nabla L = \\sum_{i=1}^m (\\hat{y}_i - y_i) x_i$$\nthat defines the batch gradient decent algorithm. We can then readily convert this algorithm to SGD by considering mini-batch updates.\n"});index.add({'id':9,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/kernels/','title':"Kernels and the Kernel Trick",'content':"Kernels and the Kernel Trick Introduction In linear regression we have seen a simple dataset from an unknown non-linear target function. We then proceeded and chose a hypothesis from the polynomial family that mapped each input example $x$ into a function $g(x, \\mathbf w) = \\mathbf w^T \\phi(\\mathbf{x})$, found the optimal $\\mathbf w$ by maximizing the likelihood (ML) function using the MSE as the loss function. The moment we have $\\mathbf w$ we can use it to do new predictions. In this regression problem we have used a transformation from the raw data $\\mathbf x$ to the feature $\\phi(\\mathbf x)$ and more specifically we have used basis functions $\\phi_i(\\mathbf x)$ from the set of polynomials shown below.\nPolynomial basis functions\nWe could have chosen other sets as well:\nGaussian basis functions\nIn classification, we have seen lastly logistic regression which despite the non-linear (probabilistic) interpretation of its output it is still a linear classifier as it presents a linear decision boundary - linear in the feature space $(\\mathbf{w}^T\\mathbf{x})$. Which posses the question: what we do when we have a problem that has a non-linear decision boundary? The answer is via the concept of kernels that we describe next.\nMotivation Lets revisit the perceptron algorithm where we started from $\\mathbf{w}=\\mathbf{0}$ (or a random vector) and we visited each and every example, changing the direction of the decision boundary when we met a miss-classified example. In the plots of that lecture for example we have seen that after two misclassified examples (the green circled examples indexed 9 and 6) the algorithm converged to a final $\\mathbf{w}$. Note that the indices9 and 6 selected here are arbitrary. These two steps can be written as:\n$\\mathbf{w}^{(1)} = \\mathbf{w}^{(0)} + \\mathbf{x}^{(9)}$\n$\\mathbf{w}^{(2)} = \\mathbf{w}^{(1)} + \\mathbf{x}^{(6)}$\nWe can compress this into $\\mathbf{w}^{(2)} = \\mathbf{w}^{(0)} + {\\mathbf{x}^{(9)} + \\mathbf{x}^{(6)}}$ and in addition this example indicates that we just need to keep the so called sparse representation where we represent the final $\\mathbf{w}$ with the array ${[9, +1], [6, +1]}$, where \u0026ldquo;+1\u0026rdquo; happened to be the class of the two examples involved in the adjustment of the weight. Saying it a bit differently, the algorithm results in the set of points that are miss-classified throughput the journey of visiting each and every of our examples.\nFor non-linear decision boundaries, the straightforward approach we can take is to throw into the problem more features. This is visualized as shown in this animation for one of the datasets we have seen in Tensorflow Playground where we admitted that neither the perceptron or logistic regression cannot separate the two classes.\n In the visualization above we added the feature $\\mathbf{x}^2$ and therefore expanded the feature space $\\phi$ to ${\\mathbf{x}, \\mathbf{x}^2}$ and surely in the expanded space there is a linear decision boundary (a plane) that can separate the previously the linearly inseparable classes. Moreover this plane can be found by various linear classification methods and for the perceptron we can then using the same reasoning as above we can end up with a sparse representation for the weight that is normal to the plane and can be written as:\n${\\phi(\\mathbf{x}^{(k)}), \u0026hellip;, \\phi(\\mathbf{x}^{(d)})}$\nthat we can also programmatically represent as a linked list as before.\nThe problem with adding more features is that in high dimensional spaces (e.g. medical imaging) we may end up with problems of very high number of dimensions as we start from a large number of dimensions to begin with. For example, in imaging we are dealing with at least as many dimensions as the number of pixels in the image and sometimes many more. So although with the sparse representation we have avoiding writing down $\\mathbf{w}$ explicitly in the high dimensional space as we can represented as a linked list of indices and labels of the examples of interest, we still need to make operations on those high dimensional examples and therefore the problem now becomes how to avoid the explicit specification of $\\phi(\\mathbf{x})$. This is what we do with the kernel trick described next.\n Where in the perceptron algorithm we need to make operations in high dimensional (expanded) space? Hint: How do we determine if the example is a misclassified during training? How we make inference predictions?\n Kernel trick Now that we have motivated the need for the trick lets see what the trick is after all.\nThe dot product $\\mathbf{w}^T\\phi(\\mathbf{x})$ will be needed throughout training as well as inference.Given the sparse representation obtained earlier we realize that we will be dealing with lots of dot products of the form $\\phi(\\mathbf{x}^{(k)})^T \\phi(\\mathbf{x})$.\nThe kernel trick at a high level is this: instead of doing these dot products in the expanded high dimensional space, we will do them in the original lower dimensional space saving significant computations (even ensuring feasibility in some cases). The trick, no matter how unbelievable it may look at a first glance, is becoming a reality by choosing suitable kernels\n$$k(\\mathbf x_i, \\mathbf x_j) = \\phi(\\mathbf x_i)^T \\phi(\\mathbf x_j)$$\n$$= \\sum_{i=1}^M \\phi_i(\\mathbf x_i)^T \\phi_i(\\mathbf x_j)$$\nwhere $\\phi_i(\\mathbf x)$ are basis functions. Lets look at an example to see in practice the properties of such kernel functions.\nLets look at the kernel function we used in the previous example where we expanded the space to include a quadratic term but now we include all quadratic terms that correspond to basis function from the polynomial set. This means $\\phi(\\mathbf x) = (x_1^2, x_2^2, x_1 x_2)$. Lets us now form the dot product at the feature space and see if this choice of basis function has the property that this dot product can be written as a dot product of terms in the original space.\n$$k(\\mathbf x, \\mathbf z) = \\phi(\\mathbf x)^T \\phi(\\mathbf z)$$ $$=(x_1^2, \\sqrt{2} x_1 x_2, x_2^2) (z_1^2, \\sqrt{2} z_1 z_2, z_2^2)^T = x_1^2z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2$$ $$= (x_1 z_1 + z_2 z_2)^2 = (\\mathbf x^T \\mathbf z)^2$$\nSo this kernel has the desired property that allows us to execute the perceptron algorithm (or other classification algorithms as we will see shortly) at the expanded feature space, without requiring for us to do the processing at the feature space but at the original data space. We call such kernels, valid. Apart from polynomial basis functions we have just seen, the most trivial kernel is as you can imagine $k(\\mathbf x, \\mathbf z) = \\mathbf x^T \\mathbf z$ which by definition is not doing any space expansion and therefore degenerates to a linear decision boundary. This kernel can be used though to generate other kernels. One of them is the so called Gaussian kernel that is worth learning about:\n$k(\\mathbf x, \\mathbf z) = \\exp(-||\\mathbf x - \\mathbf z||^2 / {2\\sigma^2})$\n We can see kernels functions from another perspective as well. Kernel functions that possess the properties required by the kernel trick, i.e. dot products in the high dimensional space that degenerate to dot product in the original data space are similarity functions because dot products are just that: they express how similar (direction wise) a vector is to another.\n "});index.add({'id':10,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/face-recognition/','title':"Face Recognition - SVM Case Study",'content':" "});index.add({'id':11,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/clustering/','title':"K-means Clustering",'content':"K-means Clustering Up to now in this lecture series we have seen parametric models using in regression and classification. The complexity of such models was not very high despite the heany sometimes math. Most models we have seen took the form $\\mathbf w^T \\phi(\\mathbf x)$. Now we switch to a very frequently met case where we dont have any labels $y$. Most of the treatment here is from Bishop section 9.1 - the equivalent section in Tan\u0026rsquo;s book is section 7.2.\nSuppose we have a data set $D={ \\mathbf x_1,\u0026hellip;,\\mathbf x_m}$ consisting of $m$ observations of a random n-dimensional Euclidean variable $\\mathbf x$. Our goal is to partition the data set into some number $K$ of clusters, where we shall suppose for the moment that the value of $K$ is given. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. This is shown in the figure below.\nWe can formalize this notion by first introducing a set of n-dimensional vectors $\\mathbf \\mu_k$ where $k ={1,\u0026hellip;,K}$, in which $\\mathbf \\mu_k$ is a prototype associated with the kth cluster. We can think of the $\\mathbf \\mu_k$ as representing the centres of the clusters. Our goal is then to find an assignment of data points to clusters, as well as a set of prototypes. To do so we need to define as before a loss function.The loss function is selected to be the sum of the squares of the distances of each data point to its closest vector $\\mathbf \\mu_k$ is a minimum.\n$L = \\sum_{i=1}^m \\sum_{k=1}^K r_{ik} ||\\mathbf x_i - \\mathbf \\mu_k||^2$\nEverything is as expected in the loss function above expept from the variable $r_{ik}$ which is a binary *indicator* variable that will simply add the squared Euclidean distance between the point $\\mathbf x_i$ and the candidate cluster prototype if the point is assigned to that cluster or add 0.0 to the loss if it doesnt. As an example for two cluster prototypes (as evidently will be the number of clusters in the above example) $r_k \\in {(0 1), (1 0)}$. This is also called one-hot encoding of the corresponding category where category \u0026ldquo;cluster 1\u0026rdquo; is coded as $(0 1)$ and category \u0026ldquo;cluster 2\u0026rdquo; is coded as $(1 0)$.\nThe K-means algorithm is iterative and each iteration includes two steps after an initialization of the $\\mathbf \\mu_k$ to random locations.\nStep 1 We go over each data point $\\mathbf x_i$ and we assign it to the closest custer center for this iteration.\nIteration 1, Step-1: Assignment of data points to cluster centers\nMathematically this means determining:\n$$r_{ik} = \\begin{cases}1, \u0026amp; \\text{if } k = \\arg \\min_j ||\\mathbf x_i - \\mathbf \\mu_j||^2\\ 0, \u0026amp; \\text{otherwise. } \\end{cases}$$\nStep 2 In the second step we move each cluster center to the average of the data points assigned to that cluster. Mathematically this is:\n$$ \\mathbf \\mu_k = \\frac{\\sum_i r_{ik} \\mathbf x_i}{\\sum_i r_ik}$$\nIteration 1, Step-2: Moving the cluster centers to the mean of the data points assigned to each cluster\nThe following figures show subsequent steps until its becoming clear that the algorithm will convergence.\nIteration 2, Step-1: Assignment of data points to cluster centers\nIteration 2, Step-2: Moving the cluster centers to the mean of the data points assigned to each cluster\nThe algorithm will converge as shown below:\nBlue circles (Step 1), interleaved with red circles (step 2) towardds k-means alg convergence\n"});index.add({'id':12,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/','title':"Support Vector Machines",'content':"Support Vector Machines In the development of the concept of kernels, we mentioned that these can be used to derive non-linear decision boundaries. What we haven\u0026rsquo;t addressed, is how good these can be - for example in separable datasets there can be many (or infinite) number of boundaries that separate the two classes but we need a metric to gauge the quality of separation. This metric is called the margin and will be geometrically explained next.\nClassification Margin Intuitively margin is the distance between the classification boundary and the closest data point(s) of the two classes, as shown below:\nClassification Margin\nAssuming a linear classifier of $g(\\mathbf x) = \\mathbf w^T \\phi(\\mathbf x) + b$ and a separable dataset for the moment, the maximization of the margin leads to a decision boundary, $y(\\mathbf x)=0$ that depends only on a subset of data that are calling support vectors as shown below for the specific dataset.\nSupport Vectors\nJust like in the perceptron case, since\n$$y( \\mathbf x_i ) = \\begin{cases}\u0026gt;0 \u0026amp; \\text{if} \\ y_i = +1, \\\\ \u0026lt;0 \u0026amp; \\text{if} \\ y_i = -1 \\end{cases}$$\nfor all training data we have,\n$$y_i(\\mathbf w^T x_i + b) \u0026gt; 0$$\nSo we are after a $\\mathbf w \\in \\mathbb{R}^n$ that satisfies this constraint. Before we proceed, let us define geometrically what the margin looks like. Let us define two parallel lines on either side of the decision boundary as shown in the above figure,\n$\\mathbf w^T \\phi(\\mathbf x) + b = 1$\n$\\mathbf w^T \\phi(\\mathbf x) + b = -1$\nLet us assume now that there is a data point that is very close to the decision boundary and just marginally satisfied the inequality $y_i(\\mathbf w^T x_i + b) \u0026gt; 0$ for example $y_i(\\mathbf w^T x_k + b) = 0.2$. If we scale both sides we can reach an equivalent condition that is appealing mathematically $y_i(\\mathbf w^T x_i + b) \u0026gt; 1$ - equivalent as it will result in the same $\\mathbf w$. In addition to this scaling, it is very useful to normalize $\\mathbf w$ as\n$\\hat \\mathbf w = \\mathbf w / ||\\mathbf w||$\nFor a point $\\mathbf z$ that is in the margin defining line $\\mathbf w^T \\phi(\\mathbf x) + b = 1$ we can calculate given a margin distance $\\gamma$ the point it projects to the decision boundary line which is defined by the normalized normal vector $\\mathbf w$ as, $\\mathbf z - \\gamma \\hat \\mathbf w$. Then we can write two equations that satisfy these two points:\n$\\mathbf w^T \\phi(\\mathbf z) + b = 1$\n$\\mathbf w^T \\phi(\\mathbf z - \\gamma \\hat \\mathbf w) + b = 1$\ngiven that we are dealing with linear decision boundary in this specific problem, we can eliminate $\\phi$ as there is no transformation involved and subtract the two equations. Solving for the margin $\\gamma$ we obtain,\n$\\gamma = \\frac{1}{||w||}$\nOptimization problem statement We are now ready to write the optimization problem that will maximize $\\gamma$ or minimize $||\\mathbf w||$ or equivalently minimize a monotonic function of $||\\mathbf w||$.\n$\\min \\frac{1}{2}||w||^2$\n$\\ \\text{s.t.} \\ y_i\\mathbf w^T x_i + b \u0026gt; 1$\nThis is a convex optimization problem (we have linear constraints and convex objective function) and therefore there are efficient ways to solve it. In practice, ML frameworks have built in solvers that can provide the $\\mathbf w$ given a dataset. Understanding the solution is instructive but it requires background in convex optimization theory and the concept of Langrange multipliers. For this reason, the math in the \u0026ldquo;Under the hood\u0026rdquo; section of chapter 5 of Geron\u0026rsquo;s book will not be in the midterm or final exams but the intuition behind maximizing the margin is in the scope of the final exam.\nTo provide an intuition of how the kernels and the SVM classifiers are connected, it suffices to write down the dual form of the optimization problem and surely enough in this form the solution involves dot products in the data space. In general, any SVM problem can be kernelized if we expand the data space using a suitable kernel which means that SVM is able via the kernel trick to find efficiently max margin decision boundaries of any shape.\nFor non separable datasets the optimization problem statement can be written using slack variables $\\xi_i$ that relax the constraints and therefore result into $\\mathbf w$ and $b$ that tolerate some classification errors for good generalization ability.\n$\\min \\frac{1}{2}||w||^2 + C \\sum_i \\xi_i$\n$\\ \\text{s.t.} \\ y_i\\mathbf w^T x_i + b \\ge 1 - \\xi_i$\n$ \\ \\ \\ \\ \\ \\ \\ \\xi_i \\ge 0$\nThe slack variable defining how much on the wrong side the 𝑖th training example is. If $\\xi =0$, the point was classified correctly and by enough of a margin; if it\u0026rsquo;s between 0 and 1, the point was classified correctly but by less of a margin than the SVM wanted; if it\u0026rsquo;s more than 1, the point was classified incorrectly. A geometrical view of the above is useful.\nOnce again the ML frameworks that you will work with, provide in their documentation explicit mention about variables of the optimization problem statement above (such as $C$). However as $C$ must be explicitly set by the data scientist, or optimized using hyperparameter optimization techniques, it is instructive to comment that $C$ is equivalent to the $1/\\lambda$ regularization parameter we met in the very beginning of these lectures where we penalized the weights of linear regression to avoid overfitting. $C$ as you can see from the optimization problem objective function if set too high, even a slightest ammount of slack will greatly penalize (add) to the objective and therefore will drive the decision boundary towards \u0026ldquo;hard\u0026rdquo; SVM decisions. This is not good news as the SVM must in many practical cases avoid over-sensitivity to outliers - too large $C$ can lead to driving the decision boundary towards outliers significantly increasing the test (generalization) error. If $C$ is set to $C=0$ then any ammount of slack will be tolerated and then the decision boundary may result into too many misclassifications.\n"});index.add({'id':13,'href':'/cs-gy-6613-spring-2020/docs/syllabus/','title':"Syllabus",'content':"Syllabus The course schedule below highlights our journey to understand the multiple subsystems and how they can be connected together to create compelling but, currently, domain specific forms of intelligence.\nBooks Artificial Intelligence: A Modern Approach, by Stuart Russell, 3rd edition, 2010 and also here.\nThe publisher is about to release the 4th edition (2020) of this classic. We will be monitoring availability in bookstores but it does not seem likely this edition to appear on time for the Spring 2020 class.\nOther recommended texts are:\n(a) DRL: \u0026ldquo;Foundations of Deep Reinforcement Learning\u0026rdquo;, by Graesser \u0026amp; Keng, 2020.\n(b) GERON: \u0026ldquo;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\u0026rdquo;, 2nd Edition, by Geron, 2019.\n(c) DL: https://www.deeplearningbook.org/ (free)\nSchedule The schedule is based on Academic Calendar Spring 2020:\nPart I: Perception and Machine Learning   Lecture 1 (1/27/2020) We start with an introduction to AI and present a systems approach towards it. We develop a map that will guide us through the rest of the course as we deep dive into each component embedded into AI agents. Reading: AIMA Chapters 1 \u0026amp; 2.\n  Lecture 2 (2/3/2020) The perception subsystem is the first stage of many AI systems including our brain. Its function is to process and fuse multimodal sensory inputs. Perception is implemented via a number of reflexive agents that map directly perceived state to an primitive action such as regressing on the frame coordinates of an object in the scene. We present the supervised learning problem both for classification and regression, starting with classical ML algorithms. Reading: AIMA Chapter 18.\n  Lecture 3 (2/10/2020) We expand into Deep neural networks. DNNs are developed bottom up from the Perceptron algorithm. MLPs learn via optimization approaches such as Stochastic Gradient Descent. We deep-dive into back-propagation - a fundamental algorithm that efficiently trains DNNs. Reading: DL Chapter 6\n  3/16/2020 Enjoy President\u0026rsquo;s Day holiday.\n Lecture 4: (2/24/2020) We dive into the most dominant DNN architecture today - Convolutional Neural Networks (CNNs). Reading: DL Chapter 9 \u0026amp; 10.\n  Lecture 5: (3/2/2020) When agents move in the environment they need to abilities such as scene understanding. We will go through few key perception building blocks such as Object Detection, Semantic and Instance Segmentation. Some of these building blocks (autoencoders) are instructive examples of representations learning that will be shown to be an essential tool in the construction of environment state representations. Reading: Various papers\n  Part II: Reasoning and Planning Lecture 6: (3/9/2020) In this lecture we introduce probabilistic models that process the outputs of perception (measurement / sensor model) and the state transitions and understand how the agent will track / update its belief state over time. This is a achieved with probabilistic recursive state estimation algorithms and dynamic bayesian networks. Reading: AIMA Chapters 14 \u0026amp; 15.  3/16/2020 Enjoy your Spring Break.\nLecture 7: (4/13/2020) After the last lecture, the agent has a clear view of the environment state such as what and where the objects that surround it are, its able to track them as they potentially move. It needs to plan the best sequence of actions to reach its goal state and the approach we take here is that of problem solving. In fact planning and problem solving are inherently connected as concepts. If the goal state is feasible then the problem to solve becomes that of search. For instructive purposes we start from simple environmental conditions that are fully observed, known and deterministic. This is where the A* algorithm comes in. We then relax some of the assumptions and treat environments that are deterministic but the agent takes stochastic actions or when both the environment and agent actions are stochastic. We also investigate what happens when we do not just care about reaching our goal state, but when we, in addition, need to do so with optimality. Optimal planning under uncertainty is perhaps the cornerstone application today in robotics and other fields. Readings: Reading: AIMA Chapters 3 \u0026amp; 4 (problem solving) AIMA Chapters 10 \u0026amp; 11 (planning) and selected papers.  3/30/2020 - This is your Midterm Test (2h)\nPart III: Reinforcement Learning Lecture 8: (4/6/2020) We now make a considerable extension to our assumptions: the utility of the agent now depends on a sequence of decisions and, further, the stochastic environment offers a feedback signal to the agent called reward. We review how the agent\u0026rsquo;s policy, the sequence of actions, can be calculated when it fully observes its current state (MDP) and also when it can only partially do so (POMDP). We conclude with the basic taxonomy of the algorithmic space for RL problems. Readings: AIMA Chapter 16 \u0026amp; 17, DRL Chapter 1. This lecture will be delivered by my colleague Gurudutt Hossangadi.  4/13/2020 Good luck with your midterm.\n Lecture 9: (4/20/2020) The algorithms that learn optimal policies in such settings are known as Reinforcement Learning (RL). In this lecture we establish the connection between MDP and RL, by introducing the Bellman expectation backup and Bellman optimality equations. We then use these equations to derive the policy iteration algorithm that is behind the policy-based REINFORCE algorithm that is empowered by approximating the policy function using the Deep Neural Networks that we met in the perception subsystem. DRL Chapter 2.\n  Lecture 10: (4/27/2020) In this lecture we aim to expand our DRL treatment on value-based methods and the principle of action-value iteration. We then look at algorithm incarnations of value-based methods such as State Action Reward State Action (SARSA) and if time allows off-policy DQN and understand how they learn the value functions. Readings: DRL Chapter 3 / 4.\n  Part IV: Natural Language Processing and Representations  Lecture 11: (5/05/2020) NLP is the pinnacle of applied AI in every day life - we are all using natural language as the prime means of communicate between us and increasingly between us and robots. In this lecture we pose the NLP problem, understand its components and their mechanics. We then talk extensively about language modeling and start with an approach based on the RNN / LTSM architecture. The later is used far beyond language modeling and expands into every use case that involves sequences. AIMA Chapter 23 and DL Chapter 10.\n  Lecture 12: (5/11/2020) We introduce the concept of attention and go through the Transformer framework - perhaps the most successful architecture in NLP today. We also cover some key aspects of the course to prepare for the final test.\n  5/18/2020 Good luck with your final test.\n"});index.add({'id':14,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/mnist/','title':"MNIST Classification - SVM Case Study",'content':"MNIST Classification - SVM Case Study  "});index.add({'id':15,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/iris/','title':"Iris Classification - SVM Case Study",'content':"Iris Classification - SVM Case Study  "});index.add({'id':16,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/','title':"Lecture 1 - Introduction to AI",'content':""});index.add({'id':17,'href':'/cs-gy-6613-spring-2020/docs/lectures/learning-problem/','title':"Lecture 2a - The Learning Problem",'content':"The Learning Problem We have seen various agent designs but the ones that we will concentrate on this course are ones that can form partially observed (PO) environment states using various sensing architectures. The perception block we have seen in the case where the agent is an autonomous car achieves that for example, and perception is rich in what is called Machine Learning (ML). ML is a substantial subset of AI today and is not limited to just the perception part of an agent.\nAlmost all machine learning algorithms depend heavily on the representation of the data they are given. Each piece of, relevant to the problem, information that is included in the representation is known as a feature. Today\u0026rsquo;s ML approaches such as deep learning actually learn the most suitable representations for the task at hand (still with a some help from experts) - an example is shown in the picture below.\nHierarchical Feature Learning\nThe Supervised Learning Problem Statement Let us start with a classic formal definition of the supervised learning problem.\nVapnik\u0026rsquo;s formulation of the learning problem (enhanced with notation from the Deep Learning book)\nThe description below is taken from Vadimir Vapnik\u0026rsquo;s classic book Statistical Learing Theory, albeit with some enhancements on terminology to make it more in line with this book.\nThe generator is a source of situations that determines the environment in which the target function (he calls it supervisor) and the learning algorithm act. Here we consider the simplest environment: the data generator generates the vectors $\\mathbf{x} \\in \\mathcal{X}$ independently and identically distributed (i.i.d.) according to some unknown (but fixed) probability distribution function $p(x)$.\nThese vectors are inputs to the target function (or operator); the target operator returns the output values $\\mathbf{y}$. The target operator which transforms the vectors $\\mathbf{x}$ into values y, is unknown but we know that it exists and does not change.\nThe learning algorithm observes the training dataset,\n$${ (\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_m, y_m) }$$\nwhich contain input vectors $\\mathbf{x}$ and the target response $\\mathbf{y}$. During this period, the learning algorithm constructs some operator which will he used for prediction of the supervisor\u0026rsquo;s answer $y_i$ on any specific vector $\\mathbf{x}_i$ generated by the generator. The goal of the learning algorithm is to construct an appropriate approximation of the target function - we will call this a hypothesis. The hypothesis can be iteratively constructed so the final hypothesis is the one that is used to produce the label $\\hat{y}$.\nTo be a mathematically correct, this general scheme of learning from examples needs some clarification. First of all, we have to describe what kind of operators are used by the target function. In this book. we suppose that the target function returns the output $\\mathbf{y}$ on the vector $\\mathbf{x}$ according to a conditional distribution function $p(\\mathbf{y} | \\mathbf{x})$ (this includes the case when the supervisor uses some function $\\mathbf{y} = f(\\mathbf{x}))$.\nThe learning algorithm observes the training set which is drawn randomly and independently according to a joint distribution function $p(\\mathbf{x} , \\mathbf{y}) = p(\\mathbf{x}) p(\\mathbf{y} | \\mathbf{x})$. Recall that we do not know this function but we do know that it exists. Using this training set, the learning algorithm constructs an approximation to the unknown function. The ability to correctly predict / classify when observing the test set, is called generalization.\nA couple of examples of supervised learning are shown below:\nExamples from the MNIST training dataset used for classification\nBirdseye view of home prices - Zillow predicts prices for similar homes in the same market. This is a regression problem.\nUnsupervised Learning In unsupervised learning, we present a training set ${ \\mathbf{x}_1, \\dots, \\mathbf{x}_m }$ without labels. The most common unsupervised learning method is clustering. We construct a partition of the data into a number of $K$ clusters, such that a suitably chosen loss function is minimized for a different set of input data (test).\nClustering showing two classes and the exemplars per class\nSemi-supervised Learning and Active Learning Semi-supervised learning stands between the supervised and unsupervised methods. One of the hottest methods in this category is the so called Active learning. In many practical settings we simply cannot afford to label /annotate all $\\mathbf x$ for very large $m$, and we need to select the ones that greedily result into the biggest performance metric gain (e.g. accuracy).\nReinforcement Learning In reinforcement learning, in this course we will treat RL in detail later, a teacher is not providing a label (as in supervised learning) but rather a reward that judges whether the agent\u0026rsquo;s action results on favorable environment states. In reinforcement learning we can learn end to end optimal mappings from perceptions to actions.\n"});index.add({'id':18,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/','title':"Lecture 2b - Regression",'content':""});index.add({'id':19,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/','title':"Lecture 2c - Linear Classification",'content':"Linear Classification This section captures the main principles of linear classification which is a fundamental ability for the perception system of an AI agent.\nClassification Principle There are three broad classes of methods for determining the parameters $\\mathbf{w}$ of a linear classifier:\n  Discriminative Models, which form a discriminant function that maps directly test data $\\mathbf{x}$ to classes $\\mathcal{C}_k$. In this case, probabilities play no role. Examples include the Perceptron and Support Vector Machines (SVMs).\n  Probabilistic Discrimitative Models, First solve the inference problem of determining the posterior class probabilities $p(\\mathcal{C}_k|\\mathbf{x})$ and then subsequently assign each new $\\mathbf{x}$ to one of the classes. Approaches that model the posterior probabilities directly are called discriminative models. Examples of discriminative training of linear classifiers include:\n Logistic regression—maximum likelihood estimation of $\\mathbf{w}$ assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.    Probabilistic Generative Models, which infer the posterior $p(\\mathcal{C}_k|\\mathbf{x})$ using Bayessian approach and we therefore generate the class-conditional density $p(\\mathbf{x}|\\mathcal{C}_k)$ and the prior $p(\\mathcal{C}_k)$. Examples of such algorithms include:\n Linear Discriminant Analysis (or Fisher\u0026rsquo;s linear discriminant) (LDA)—assumes Gaussian conditional density models Naive Bayes classifier with multinomial or multivariate Bernoulli event models.    Discriminative Models Synthetic Dataset with multiple discriminant functions\n  The learning algorithm is asked to assign the input vector $\\mathbf{x} \\in \\mathbb{R}^n$ to one of the $k$ classes. The learning algorithm will need to produce a function $\\mathbf{y}=g(\\mathbf{x})$ also called discriminant function.\n  In binary classification, the target variable (label) $y$ belongs to either of the two classes ${\\mathcal{C}_1, \\mathcal{C}_2}$ - for multi-class it is taking a value out of a finite set of classes. Contrast this to regression where $y$ is taking any value out of a infinite possible set of values ($y \\in \\mathbb{R}$). For example, we can choose convenient labels $y=0$ for class $\\mathcal{C}_1$, and $y=1$ for class $\\mathcal{C}_2$.\n  The Bayesian setting is now very handy, $ p(\\mathcal{C}_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{p(\\mathbf{x})}$ where $p(\\mathcal{C}_k)$ is the prior probability for the corresponding class. The equation above is similar to what we have seen in regression, and can be used to update the posterior probability $p(\\mathcal{C}_k|\\mathbf{x})$ given the likelihood function, prior and evidence. As an example, we look at the CT-scan of a patient and obtain the posterior probability based on the formula above and can now diagnose the patient as a cancer free if $p(\\mathcal{C}_0|\\mathbf{x}) \u0026gt; p(\\mathcal{C}_1|\\mathbf{x})$ where $\\mathcal{C}_0$ is the cancer free class.\n  Joint probabilities involved in binary classification. By sweeping the discrimination function (which in this example is the threshold $\\hat{x}$) left or right we are adjusting the areas shown as red blue and green that are the main determinants of classifier performance.\nReceiver Operating Curve and Classification Metrics Obviously the criticality of estimating the right posterior for the treatment of the patient is very high - we have therefore developed metrics that gauge such detection. It is extremely important to understand the Receiver Operating Curve that gives raise to many classification quality metrics. Where the name comes from? The top of this page is decorated by a photo of one of the hundreds of RADAR towers installed in England just before the WWII. The job of the RADAR was to detect incoming Nazi air bombings. RADAR was one of the key technologies that won the war (the other was cryptography).\nLet us consider a two-class prediction problem (binary classification), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p.\nTo get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:\nJoint probabilities and ROC Curve (Wikipedia)\nConfusion Matrix (Wikipedia)\nFor an instructional example of determining the confusion matrix of classification models using scikit-learn see the Iris case study.\n"});index.add({'id':20,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/','title':"Lecture 3 - Deep Neural Networks",'content':"Deep Neural Networks DNNs are the implementation of connectionism, the philosophy that calls for algorithms that perform function approximations to be constructed by an interconnection of elementary circuits called neurons. There are two main benefits that DNNs brought to the table, on top of their superior perfomance in large datasets.\nAutomated Feature Construction (Representations) Given a synthetic dataset like this, there is a thought process that a data scientist must undergo to transform the input into a suitable representation e.g. $(x,y)=(r\\cos\\theta,r\\sin\\theta)$ for a linear classifier to be able to provide a solution.\nThe DNN (even as small as a single neuron) will find such representation automatically.\nHierarchical representations On top of the automated feature construction advantage, that even shallow networks can provide, features can be represented hierarchically as shown below.\nIn the next sections, we provides some key points on the following questions:\n What DNNs look like How they work (backpropagation)  We will use a combination of material from Ian Goodfellow\u0026rsquo;s book chapter 6 and CS331n. Another excellent resource is Nielsen\u0026rsquo;s tutorial treatment of the subject.\nNOTE: You can use this to depict several DNN architectures - the fully connected ones can be used to draw rudimentary NNs and pencil them with back propagation equations.\n Architecture Feedforward networks consist of elementary units that resemble the perceptron. These units are stacked up into layers.\nThere are multiple layers:\n The input layer One or more hidden layers The output layer  A typical DNN consists a trivial placeholder layer that feeds the network with input data $\\mathbf x$ via an input layer. One or more hidden layers that that employ one ore more activation functions and output layer that usually takes the shape for classification problems of a softmax function.\nActivation Functions There are several possible but we limit the discussion just three here.\n  The perceptron activation function which we have seen here:\n$$g(a) = \\begin{cases}1.0 \u0026amp;\\text{if } a \\geq 0 \\\\ -1.0 \u0026amp;\\text{if } a \u0026lt; 0 \\end{cases}$$\n  The sigmoid activation function that we have also seen during logistic regression.\n$$g(a) = \\sigma(a) = \\frac{1}{1+e^{-a}} \\hspace{0.3in} \\sigma(a) \\in (0,1)$$\nTowards either end of the sigmoid function, the $\\sigma(a)$ values tend to respond much less to changes in a vanishing gradients. The neuron refuses to learn further or is drastically slow.\n  The Rectified Linear Unit activation function - very popular in Deep Learning.\nThe RELU is very inexpensive to compute compared to sigmoid and it offers the following benefit that has to do with sparsity: Imagine an MLP with random initialized weights to zero mean ( or normalised ). Almost 50% of the network yields 0 activation because of the characteristic of RELU. This means a fewer neurons are firing (sparse activation) making the the network lighter and more efficient. On the other hand for negative $a$, the gradient can go towards 0 and the weights will not get adjusted during descent.\n  Softmax Output units The softmax output unit is a generalization of the sigmoid for problems with more than two classes.\n$$\\text{softmax}(\\mathbf z)_i = \\arg \\max_i \\frac{\\exp (z_i)}{\\sum_i \\exp(z_i)}$$\nwhere $i$ is over the number of inputs of the softmax function.\nFrom a neuroscientiﬁc point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it: the softmax outputs always sum to 1 so an increase in the value of one unit necessarily corresponds to a decrease in the value of others. This is analogous to the lateral inhibition that is believed to exist between nearby neurons in the cortex. At the extreme (when the diﬀerence between the maximal and the others is large in magnitude) it becomes a form of winner-take-all(one of the outputs is nearly 1, and the others are nearly 0).\n"});index.add({'id':21,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-intro/','title':"Introduction to Backpropagation",'content':"Introduction to Backpropagation The backpropagation algorithm brought back from the winter neural networks as it made feasible to train very deep architectures by dramatically improving the efficiency of calculating the gradient of the loss with respect to all the network parameters.\nIn this section we will go over the calculation of gradient using an example function and its associated computational graph. The example does not have anything to do with DNNs but that is exactly the point. The essence of backpropagation was known far earlier than its application in DNN.\nFor students that need a refresher on derivatives please go through Khan Academy\u0026rsquo;s lessons on partial derivatives and gradients.\nCalculating the Gradient of a Function Our goal is to compute the components of the gradient of the function $\\nabla f = [ \\partial f / \\partial x , \\partial f / \\partial y ]^T$ where,\n$$f(x, y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$$\nThe computational graph of this function is shown below. Its instructive to print this graph and pencil in all calculations for both this example and others in the backpropagation section. You may need to review derivative tables from Calculus e.g. http://cs231n.stanford.edu/handouts/derivatives.pdf\nForward Pass In the forward pass, the algorithm works bottom up (or left to right depending how the computational graph is represented) and calculates the values of all \u0026ldquo;gates\u0026rdquo; (gates are the elementary functions that synthesize the function) of the graph and stores their values into variables as they will be used by the backwards pass. There are eight values stored in this specific example.\n1 2 3 4 5 6 7 8 9 10  sigy = 1.0 / (1 + exp(-y)) #(1) num = x + sigy # numerator #(2) sigx = 1.0 / (1 + exp(-x)) #(3) xpy = x + y #(4) xpysqr = xpy**2 #(5) den = sigx + xpysqr # denominator #(6) invden = 1.0 / den #(7) f = num * invden #(8)   Backwards Pass In the backwards pass, we reverse direction and start at the top or rightmost node (the stored variables) of the graph and compute the input (in the reverse direction) derivative of input of each gate using the template depicted below:\nBackpropagation template\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # gradient at the top df = 1.0 # backprop f = num * invden dnum = 1.0 * invden # gradient on numerator #(8) dinvden = 1.0 * num #(8) # backprop invden = 1.0 / den  dden = (-1.0 / (den**2)) * dinvden #(7) # backprop den = sigx + xpysqr dsigx = (1) * dden #(6) dxpysqr = (1) * dden #(6) # backprop xpysqr = xpy**2 dxpy = (2 * xpy) * dxpysqr #(5) # backprop xpy = x + y dx = (1) * dxpy #(4) dy = (1) * dxpy #(4) # backprop sigx = 1.0 / (1 + exp(-x)) dx += ((1 - sigx) * sigx) * dsigx # ATTENTION !! # backprop num = x + sigy dx += (1) * dnum #(2) dsigy = (1) * dnum #(2) # backprop sigy = 1.0 / (1 + exp(-y)) dy += ((1 - sigy) * sigy) * dsigy #(1)   As the previous example indicated, the essence of backpropagation algorithm is that local gradients can be calculated symbolically (using calculus) as they only depend on the simple gate structures and these gradients act as valves in the gradient flow that starts from the top of the network (the value of the flow there is always 1.0) and propagates to the bottom of the graph. Think about it as valves controlling the water flowing down a river delta. Individual flows may also merge like in the example above.\nThis example allowed us to understand how the gates control the flow. We have met add, split, multiply and non-linear gates.\nThe add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged).\nThe multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly non intuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^Tx_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples $x_i$ by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.\nIn neural networks we will also meet the max gate (ReLU) and its instructive to outline here its effect on the flow.\nThe max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values.\n"});index.add({'id':22,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-dnn/','title':"Backpropagation in Deep Neural Networks",'content':"Backpropagation in Deep Neural Networks Following the introductory section, we have persuaded ourselves that backpropagation is a procedure that involves the repetitive application of the chain rule, let us look more specifically its application to neural networks and the gates that we usually meet there. In DNNs we are dealing with vectors, matrices and in general tensors and therefore its required to review first how we can expand on the template above for these data types.\nDNN Gates In the following we heavily borrow from this text. The basic building block of vectorized gradients is the Jacobian Matrix. In the introductory section we dealt with functions $f: \\mathbb{R}^2 \\to \\mathbb{R}$. Suppose that we have a more complicated function $\\bm f: \\mathbb{R}^n \\to \\mathbb{R}^m$ that maps a vector of length $n$ to a vector of length $m$:\n$$\\bm f(\\bm x) = [f_1(x_1, \u0026hellip;, x_n), f_2(x_1, \u0026hellip;, x_n), \u0026hellip;, f_m(x_1, \u0026hellip;, x_n)]$$.\nThen its Jacobian is:\n$$\\frac{\\partial \\bm f}{\\partial \\bm x} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\dots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots \u0026amp; \\dots \u0026amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\dots \u0026amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$$\nThe Jacobian matrix will be useful for us because we can apply the chain rule to a vector-valued function just by multiplying Jacobians.\nAs a little illustration of this, suppose we have a function $f(\\mathbf x) = [f_1(x), f_2(x)]$ taking a scalar to a vector of size 2 and a function $g(\\mathbf y) = [g_1(y_1, y_2), g_2(y_1, y_2)]$ taking a vector of size two to a vector of size two. Now let\u0026rsquo;s compose them to get $g(x) = [g_1(f_1(x), f_2(x)), g_2(f_1(x), f_2(x))]$. Using the regular chain rule, we can compute the derivative of $g$ as the Jacobian\n$$\\frac{\\partial g}{\\partial x} = \\begin{bmatrix} \\frac{\\partial}{\\partial x}g_1(f_1(x), f_2(x)) \\\\ \\frac{\\partial} {\\partial x}g_2(f_1(x), f_2(x)) \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial{g_1}}{\\partial f_1}\\frac{\\partial{f_1}}{\\partial x} + \\frac{\\partial{g_1}}{\\partial f_2}\\frac{\\partial{f_2}}{\\partial x} \\\\ \\frac{\\partial{g_2}}{\\partial f_1}\\frac{\\partial{f_1}}{\\partial x} + \\frac{\\partial{g_2}}{\\partial f_2}\\frac{\\partial{f_2}}{\\partial x} \\end{bmatrix}$$\nAnd we see this is the same as multiplying the two Jacobians:\n$$\\frac{\\partial{g}}{\\partial x} = \\frac{\\partial{ g}}{\\partial f}\\frac{\\partial{f}}{\\partial x} = \\begin{bmatrix} \\frac{\\partial{g_1}}{\\partial f_1} \u0026amp; \\frac{\\partial{g_1}}{\\partial f_2} \\\\ \\frac{\\partial{g_2}}{\\partial f_1} \u0026amp; \\frac{\\partial{g_2}}{\\partial f_2} \\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial{f_1}}{\\partial x} \\\\ \\frac{\\partial{f_2}}{\\partial x} \\\\ \\end{bmatrix}$$\nThis is also another instructive summary that help us understand how to calculate the local gradients involved and the gate templates (identities) summarized below that are routinely found in neural network backpropagation calculations. Assume that with $\\mathbf W \\in \\mathbb{R}^{n \\times m}, \\mathbf x \\in \\mathbb{R}^m$.\nTables of Gates and Gradients used in the backpropagation of deep neural networks\n   Gate Solution     $\\mathbf z = \\mathbf W \\mathbf x$ $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\mathbf W$   $\\mathbf z = \\mathbf x \\mathbf W$ $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\mathbf W^T$   $\\mathbf z = \\mathbf x$ $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\mathbf I$   $\\mathbf z = f(\\mathbf x)$ element-wise $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\text{Diag}[ f\u0026rsquo;(\\mathbf x) ]$   $\\mathbf z = \\mathbf W \\mathbf x$, $\\mathbf \\delta = \\frac{\\partial L}{\\partial \\mathbf z}$ $\\frac{\\partial L}{\\partial \\mathbf W} = \\mathbf \\delta^T \\mathbf x^T$   $\\mathbf z = \\mathbf x \\mathbf W$, $\\mathbf \\delta = \\frac{\\partial L}{\\partial \\mathbf z}$ $\\frac{\\partial L}{\\partial \\mathbf W} = \\mathbf x^T \\mathbf \\delta$   $\\mathbf z = \\mathbf W \\mathbf x$, $\\hat \\mathbf y = \\mathtt{softmax}(\\mathbf z)$, $L=CE(\\mathbf y , \\hat \\mathbf y )$ $\\frac{\\partial L}{\\partial \\mathbf z} = \\hat \\mathbf y - \\mathbf y$    During the lecture we will go through an NN example on the whiteboard that will use these gate gradients for the estimation of the gradient of the loss with respect to its parameters using backpropagation.\nBackprop behavior during training As documented here and in Geron\u0026rsquo;s textbook, you also need to be watchful of the effects of the various non-linear gates on the gradient flow.\nFor sigmoid gate, if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, $z*(1-z)$, which is **local** gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.\nFor ReLU gates, if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead.\nTensorflow can create a computational graph from the DNN model specification (python). These graphs can be visualized on the web UI with Tensorboard. Use the playground when you first learn about DNNs to understand the principles but dive into the Fashion MNIST using Tensorflow use case to understand the Tensorflow mechanics and how to debug Tensorflow python scripts both syntactically and logically. Logical debugging should happen using Tensorboard visualizations. Similarly with Pytorch if this is your choice.\n  "});index.add({'id':23,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/regularization/','title':"Regularization in Deep Neural Networks",'content':""});index.add({'id':24,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/fashion-mnist-case-study/','title':"Fashion MNIST Case Study",'content':"Fashion MNIST Case Study This notebook is a case study on MNIST Fashion dataset that due to the almost perfect classification of MNIST is now the new point of reference dataset to learn and try ML algorithms on.\nClick on the button \u0026ldquo;Run in Google Colab\u0026rdquo; to run the notebook and step through the code.\n"});index.add({'id':25,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/','title':"Lecture 4 - Convolutional Neural Networks",'content':""});index.add({'id':26,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-intro/','title':"Introduction to Convolutional Neural Networks",'content':"Introduction to Convolutional Neural Networks As elaborated here, humans build up a more schematic version of the environment across eye fixations than was previously thought. This schematic version of the environment is typically known as scene gist. It contains conceptual information about the scene’s basic category – is it natural, human-made, a cityscape? – and general layout, maybe limited to a few objects and/or features. This schematic version of the environment is a far cry from the “picture in the head” scenario. But it’s this schematic information that guides us from one eye fixation to the next, during which more detailed information can be sampled. In the picture above, the brain will first detect the cityscape schematic and then process one of the scene fixations - e.g. the yellow cab.\nAs we will see shortly images-based datasets are almost exclusively used for instructive purposes in CNNs. The reason is simple. With images we are able to \u0026ldquo;see\u0026rdquo; the effects of a number of important algorithms that are related to classification, object detection etc.\nFocusing at the \u0026ldquo;fixation\u0026rdquo; stage of human vision, CNNs are biologically inspired from the structure that the neuroscientists David Hubel and Torsten Wiesel saw in the so called V1 region of the brain - the region at the back of our head that is responsible for the processing of visual sensory input signals coming from the eye\u0026rsquo;s retina.\nColor images as functions A grayscale picture can be seen as a function $f(x,y)$ of light intensities, where x and y are the rows and columns of pixels in the image.\nUsually we have border constraints in the range of the input pixels e.g. $x \\in [a,b], y \\in [c,d]$ but also in the output intensity values (typically 8-bit encoding is assumed that limits the values to [0, 255]).\nThe city scape color image above can also be seen as a vector function:\n$$f(x,y)= \\begin{bmatrix} r(x,y) \\ g(x,y) \\ b(x,y) \\end{bmatrix}$$\nwith its elements capturing the channels Red, Green and Blue functions, the mixture (superposition) of which can generate the pixel color of the original image.\nPlease note the convention to refer to the columns $j$ of the matrix that represents each function with $x$ and to the rows $i$ with $y$. This may cause some confusion at first.\nThe Convolution \u0026amp; Cross-Correlation Operation The key operation performed in CNN layers is that of 2D convolution. In fact in practice they are 4D convolutions as we try to learn many filters and we also consider many input images (mini-batch) in the iteration of our SGD optimizer.\nWe cover first the 1-dimensional case.\n1D Convolution By definition the convolution between two functions in one dimension is given by:\n$$(a * b)[n] = \\sum_{m=-\\inf}^{\\inf} a[m]b[n-m]$$\n1D-convolution example. The operation effectively slides the flipped version of $b$ across $a$\nThe result can be calculated as follows:\n$c[0] = \\sum_m a[m]b[-m] = 0 * 0.25 + 0 * 0.5 + 1 * 1 + 0.5 * 0 + 1 * 0 + 1 * 0 = 1$\n$c[1] = \\sum_m a[m]b[1-m] = 0 * 0.25 + 1 * 0.5 + 0.5 * 1 + 1 * 0 + 1 * 0 = 1$\n$c[2] = \\sum_m a[m]b[2-m] = 1 * 0.25 + 0.5 * 0.5 + 1 * 1 + 1 * 0 + 1 * 0 = 1.5$\n$c[3] = \\sum_m a[m]b[3-m] = 1 * 0 + 0.5 * 0.25 + 1 * 0.5 + 1 * 1 = 1.625$\nand so on.\nConvolution and cross-correlation are very close - see this 1D example (source to persuade yourself that this is the case.\nConvolution and cross-correlation are very similar quantities\n2D Convolution In 2D the same principle applies. First, the convolution operation in some frameworks is the flipped version - this is perfectly fine as the convolution operation is commutative.\n$$S(i,j) = \\sum_m \\sum_n x(m, n) h(i-m,j-n) = \\sum_m \\sum_n x(i-m, j-n)h(m,n)$$\nwhere $x$ is the input of the convolution and $h$ is the kernel or filter typically of smaller spatial dimensions.\nMany ML frameworks don\u0026rsquo;t even implemented convolution but they do the very similar cross-correlation operation and they sometimes call it convolution to add to the confusion. Tensorflow implements the cross-correlation operation under the hood.\n$$S(i,j) = \\sum_u \\sum_v x(i+u, j+v)h(u,v)$$\nIf you learn the flipped version of the kernel or not is irrelevant to the task of predicting the right label. Therefore you should not be concerned with the framework implementation details, the thing that is important for you to grasp is the essence of the operation. Lets look some effects that convolution has on input signals such as images.\nEffects of 2D filtering operations Moving Average Lets go through the simplest possible 2D filtering operation as shown below. Note that these implement the convolution.\nCompleted 2D MA Filtering\nThe MA filter causes \u0026ldquo;box\u0026rdquo; blurring of input images\n2D Gaussian Using a Gaussian Blur filter before edge detection aims to reduce the level of noise in the image, which improves the result of the susually subsequent edge-detection algorithms. We will meet again this 2D Gaussian filter in the object detection section, where it is used to help in the initial segmentation in RCNN architectures.\nGaussian Filter\nBlurring is evident in this picture\nBlurring is used to improve edge-detection\nThe above filtering operations are obviously deterministic. We are given the filter but in CNNs as we will see in the next section we are learning such filters. To keep the terminology aligned with the dense neural networks layers we will be denoting the filter with $\\mathbf w$ - the weights that need to be learned through the training process.\n"});index.add({'id':27,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-arch/','title':"CNN Architectures",'content':"CNN Architectures Convolutional Layer In the convolutional layer the first operation a 3D image with its two spatial dimensions and its third dimension due to the primary colors, typically Red Green and Blue is at the input layer, is convolved with a 3D structure called the filter shown below.\nEach filter is composed of kernels - source\nThe filter slides through the picture and the amount by which it slides is referred to as the stride $s$. The stride is a hyperparameter. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\nIt\u0026rsquo;s common to have more than one filter. Different filters pick up different qualities of the receptive field. Apart from the stride, the spatial dimensions of the filter (height, width, num_kernels) the number of filters is another hyperparameter of a convolutional layer.\nThis means that typically we are dealing with volumes (3D tensors) and of course if someone adds the fact that we do processing in minibatches we are typically dealing with 4D tensors that contain input feature maps. Lets look at a single feature map visualization below of the convolution operation.\nConvolutional layer with a single feature map. We can see the strides $(s_h, s_w)$, the zero padding as well as the receptive field in the produced feature map.\nIn the figure below the authors of this paper have also animated the operation. Blue maps are inputs, and cyan maps are outputs.\n       No padding, no strides Arbitrary padding, no strides Half padding, no strides Full padding, no strides         No padding, strides Padding, strides Padding, strides (odd)    In general though in practice we are dealing with volumes due to the multiple feature maps \u0026amp; kernels involved. Its important to understand the figure below. Its a direct extension to the single feature map figure above. The difference is that each neuron in each feature map of layer $l$ is connected to all neurons of the corresponding receptive field of layer $l-1$ just as before but now these connections extend to all feature maps of layer $l-1$. in other words we connect each neuron in the feature map of layer $l$ to the corresponding receptive volume (3D array) of neurons in the layer below.\nIn the class we will go through the example below.\nThere are two steps involved. Notice that the number of input feature maps is $M_{l-1} = 2$, while the number of output feature maps is $M_{l}=3$. We therefore have three filters of spatial dimension $[3x3]$ and depth dimension of 2. In the first step each of the three filters generates a correlation result for each of the 2 input feature maps.\n$z(i,j) = \\sum_u^{height} \\sum_v^{width} x(i+u, j+v) w(u,v)$\nIn the second step we sum over the correlations for each of the three filters separately. Equivalently is like taking a volume cross correlation and extend the equation above accordingly.\n$z(i,j,k_l) = \\sum_u^{height} \\sum_v^{width} \\sum_{k_{l-1}=1}^{M_i} x(i+u, j+v, k_{l-1}) w(u, v, k_{l-1}, k_l)$\nThe figure below illustrates the input feature map to output feature map mapping expressed directly i.e. without the intermediate step of the example above.\nConvolutional layers with multiple feature maps. We can see the receptive field of each column of neurons of the next layer. Each column is produced by performing multiple convolutions (or cross correlation operations) between the volume below and each of the filters.\nIn each layer we can have in other words, as was shown in the example above, input and output feature maps of different depths.\n2D convolution that produces a feature map with different depth than the input feature map\nZero Padding Each feature map \u0026ldquo;pixel\u0026rdquo; that results from the above convolution is followed by a RELU non-linearity i.e. RELU is applied element-wise. Few words about padding. There are two types: same padding where we add zeros at the edges of the picture and valid padding where we dont. The reason we pad with zeros is to maintain the original spatial dimensions from one convolution layer to the next. If we dont very soon we can end up with deep architectures with just a one \u0026ldquo;pixel\u0026rdquo;.\nLets see a complete animated example that includes padding. You can press the toggle movement button to stop the animation and do the calculations with pencil and paper.\n source: CS231n The output spatial dimension (assuming square) is in general given by $⌊ \\frac{i+2p-k}{s} ⌋ + 1$, where $p$ is the amount of passing, $k$ is the square kernel size, $s$ is the stride. In the animation above, $p=1, k=3, s = 2$.\nWhat the convolution / operation offers There are two main consequences of the convolution operation: sparsity and parameter sharing. With the later we get as a byproduct equivariance to translation. These are explained next.\nSparsity In DNNs, every output unit interacts with every input unit. Convolutional networks, however, typically have sparse interactions(also referred to as sparse connectivity or sparse weights). This is accomplished by making the kernel smaller than the input as shown in the figure above. For example,when processing an image, the input image might have thousands or millions of pixels, but we can detect small, meaningful features such as edges with kernels that occupy only tens or hundreds of pixels.\nSparse connectivity, viewed from below. We highlight one input unit,$x_3$, and highlight the output units in that are aﬀected by this unit. (Top) When is formed by convolution with a kernel of width 3, only three outputs are aﬀected by x. (Bottom)When is formed by matrix multiplication, connectivity is no longer sparse, so all the outputs are aﬀected by $x_3$.\nParameter sharing In CNNs, each member of the kernel is used at every feasible position of the input. The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for every location, we learn only one set.\nParameter sharing. Black arrows indicate the connections that use a particular parameter in two diﬀerent models. (Top)The black arrows indicate uses of the central element of a 3-element kernel in a convolutional model. Because of parameter sharing, this single parameter is used at all input locations. (Bottom)The single black arrow indicates the use of the central element of the weight matrix in a fully connected model. This model has no parameter sharing, so the parameter is used only once.\nThe particular form of parameter sharing causes the layer to have a property called equivariance to translation. This means that a translation of input features results in an equivalent translation of outputs. So if your pattern 0,3,2,0,0 on the input results in 0,1,0,0 in the output, then the pattern 0,0,3,2,0 might lead to 0,0,1,0\nAs explained here this should not be confused with invariance to translation. The later means that a translation of input features does not change the outputs at all. So if your pattern 0,3,2,0,0 on the input results in 0,1,0 in the output, then the pattern 0,0,3,2,0 would also lead to 0,1,0.\nFor feature maps in convolutional networks to be useful, they typically need both equivariance and invariance in some balance. The equivariance allows the network to generalise edge, texture, shape detection in different locations. The invariance allows precise location of the detected features to matter less. These are two complementary types of generalization for many image processing tasks.\nIn a nutshell in images, these properties ensure that the CNN that is trained to detect an object, can do its job irrespective on where the object is located in the image.\nDimensionality Reduction Pooling Layer Pooling was introduced to reduce redundancy of representation and reduce the number of parameters, recognizing that precise location is not important for object detection.\nThe pooling function is a form of non-linear function that further modifies the result of the RELU result. The pooling function accepts as input pixel values surrounding (a rectangular region) a feature map location (i,j) and returns one of the following\n the maximum, the average or distance weighted average, the L2 norm.  A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. You may alternatively pool over the depth dimension, in which case the image’s spatial dimensions (height and width) remain unchanged, but the number of channels is reduced.\nDespite receiving ample treatment in Ians Goodfellows\u0026rsquo; book, pooling has fallen out of favor. Some reasons are:\n Datasets are so big that we\u0026rsquo;re more concerned about under-fitting. Dropout is a much better regularizer. Pooling results in a loss of information - think about the max-pooling operation as an example shown in the figure below.  Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\nTo further understand the latest reservations against pooling in CNNs, see this summary of Hinton\u0026rsquo;s capsules concept. Capsules are not in scope for this course and by extension for the final test.\n1x1 Convolutional layer The 1x1 convolution layer is met in many network architectures (e.g. GoogleNet) and offers a number of modeling capabilities. Spatially, the 1x1 convolution is equivalent to a single number multiplication of each spatial position of the input feature map (if we ignore the non-linearity) as shown below. This means that leaves the spatial dimensions of the input feature maps unchanged unlike the pooling layer.\n1x1 convolution of a single feature map is just scaling - the 1x1 convolution is justified only when we have multiple feature maps at the input.\nThe most straightforward way to look at this layer is as a cross feature map pooling layer. When we have multiple input feature maps $M_{l-1}$ and 1x1 filters 1x1x$M_{l-1}$ (note the depth of the filter must match the number of the input feature maps) then we form a dot product between the feature maps at the spatial location $(i,j)$ of the 1x1 filter followed by a non-linearity (ReLU). This operation is in other words the same operation of a fully connected single layer neural network whose neurons are those spanned by the single column at the $(i,j)$ coordinate. This layer will produce a single output at each visited $(i,j)$ coordinate.\nThis idea can be expanded to multiple layers as described in this paper.\nWhen we have multiple $M_l$ layers of size 1 x 1 x $M_{l-1}$ then, effectively, we produce multiple feature maps one for each 1x1 layer and this is a good way to reduce the number of feature maps at the output of this layer, with benefits in computational complexity of the deep network as a whole.\nKnown CNN Architectures A summary of well known CNN networks are here article with this update as a reference. This summary will be important to you as a starting point to develop your own understanding of very well known CNNs and after you read the corresponding papers in arxiv you will be able to recall key design patterns and why those patterns came to be.\n"});index.add({'id':28,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-sizing/','title':"CNN Sizing",'content':"CNN Sizing Sizing is an exercise that will help you how to specify hyperparameters in tf.keras such as the height, width, depth of filters, feature map sizes etc. Sizing is needed so that you can stitch all the layers together correctly.\nSizing Example We will use an toy network for such exercise.\nThe example CNN architecture above has the following layers:\n INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B. CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters. RELU layer will apply an elementwise activation function, such as the $\\max(0,x)$ thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]). POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12]. FC (i.e. fully-connected) layer, also known as dense, will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10 dataset. As you recall in FC layers each neuron in this layer will be connected to all the neurons in the previous volume.  The impact of padding on the sizing of the produced feature map is shown in the following numerical example. The example is for [28x28x3] input layer but results can be extrapolated for [32x32x3]\nNumber of parameters and memory "});index.add({'id':29,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/visual-attention/','title':"Attention Mechanisms in CNNs",'content':""});index.add({'id':30,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/scene-understanding-intro/','title':"Introduction to Scene Understanding",'content':"Introduction to Scene Understanding In the previous chapters we have treated the perception subsystem mainly from starting the first principles that govern supervised learning to algorithms that enable classical as well as deep learning machines. Now we are synthesizing these algorithms to pipelines that can potentially enable the holly grail of perception - our understanding of the scene. As discussed in the introduction to computer vision, humans has a unique to interpret scenes based on their ability to infer (reason) what they dont see. This is the reason why the scene understanding involves far more than just perception. In this chapter we will cover algorithms that allow us to:\n Detect objects in an image. Object detection is demonstrated in this short video clip that shows the end result of the algorithm.   Its important to understand the difference between classification and object detection as shown below.\nDifference between classification and detection\nIn classification we are given images (we can consider video clips as a sequence of images) and we are asked to produce the array of labels assigned to objects that are present in the frame. Typically in many datasets there is only one class and the images are cropped around the object. In localization, in addition to classification we are interested in locating (using for example a bounding box) each class in the frame. In object detection we are localizing multiple objects (some objects can be of the same class.) Localization is a regression problem fundamentally. Mathematically we have,\n$$y = p_{data}(x)$$\nWe try to come up with a function approximation to the true function $p_{data}$ that maps the image $x$ to the location of the bounding box $y$. We can uniquely represent the bounding box by the (x,y) coordinates of its upper left corner and its width and height $[x,y,w,h]$. Being a regression problem, as $y$ is a floating point vector, we can use well known loss functions e.g. CE $≡$ MSE where the error is the Euclidean distance between the coordinates of the true bounding box and the estimated bounding box. However, the regression approach does not work well in practice and has been superceded by the algorithms described later in this chapter.\nAssign semantic labels to each pixel in this image.  Sementic Segmentation in medical, robotic and sports analytics applications\nBoth of these abilities enable the reflexive part of perception where the inference ends up being a classification or regression or search problem and in practice, depending on the algorithm, it can range from few ms to 100s of ms. Both of these reflexive inferences are essential parts of many mission critical almost real time applications such as robotics e.g. self driving cars.\nThere are other abilities that we need for scene understanding that don\u0026rsquo;t cover until later in this book. Our ability to recognize the attribute of uniqueness in an object and assign a symbol to it, is fundamental to our ability to reason very quickly at the symbolic level. At that level we can use a whole portfolio of symbolic inference algorithms developed over the last few decades. But before we reach this level we need to solve the supervised learning problem for the relatively narrow task of bounding and coloring objects. This needs annotated data and knowing what kind of data we have at our disposal is an essential skill.\nDatasets for scene understanding tasks COCO Typical example for Detection and Image Captioning Tasks\nAfter its publication by Microsoft, the COCO dataset has become the reference dataset to train models in perception tasks and it is constantly evolving through yearly competitions. The competitions are challenging as compared to earlier ones (e.g. VOC) (see performance section) since many objects are small. COCO\u0026rsquo;s 330K images are annotated with\n 80 object classes. These are the so-called thing classes (person, car, elephant, \u0026hellip;). 91 stuff classes. These are the co-called stuff classes (sky, grass, wall, \u0026hellip;). Stuff classes cover the majority of the pixels in COCO (~66%.). Stuff classes are important as they allow to explain important aspects of an image, including scene type, which thing classes are likely to be present and their location (through contextual reasoning), physical attributes, material types and geometric properties of the scene. 5 captions per image Keypoints for the \u0026ldquo;person\u0026rdquo; class  Common perception tasks that the dataset can be used for, include:\n Detection Task: Object detection and semantic segmentation of thing classes. Stuff Segmentation Task: Semantic segmentation of stuff classes. Keypoints Task: Localization of person\u0026rsquo;s keypoints (sparse skeletal points). DensePose Task: Localization of people\u0026rsquo;s dense keypoints, mapping all human pixels to a 3D surface of the human body. Panoptic Segmentation Task: Scene segmentation, unifying semantic and instance segmentation tasks. Task is across thing and stuff classes. Image Captioning Task: Describing with natural language text the image. This task ended in 2015. Image captioning is very important though and other datasets exists to supplement the curated COCO captions.  Even in a world with so much data, the curated available datasets that can be used to train models are by no means enough to solve AI problems in any domain. First, datasets are geared towards competitions that supposedly can advance the science but in many instances deaderboards become \u0026ldquo;academic exercises\u0026rdquo; where 0.1% mean accuracy improvement can win the competition but definitely does not progress AI. The double digit improvements can and these discoveries create clusters of implementations and publications around them that fine tune them. One of these discoveries is the RCNN architecture described in the object detection section that advanced the accuracy metric by almost 30%. Secondly, the scene understanding problems that AI engineers will face in the field, e.g. in industrial automation or drug discovery, involve domain specific classes of objects. Although we cant directly use curated datasets, engineers can transfer learning, worthy of a chapter by itself, where a dataset is used to train a model for a given task whose weights can be reused to train a model for fairly similar task.\nDetection/Segmentation Task Evaluation Metrics The evaluation metrics for detection with bounding boxes and segmentation masks are identical in all respects except for the IoU computation (which is performed over boxes or masks, respectively). Therefore we omit any evaluation discussion in the semantic segmentation chapter.\nTo understand the calculation of mAP see this write up.\n"});index.add({'id':31,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/','title':"Lecture 5 - Scene Understanding",'content':""});index.add({'id':32,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/feature-extraction-resnet/','title':"Feature Extraction via Residual Networks",'content':"Feature Extraction via Residual Networks The ResNet Architecture 34 layers deep ResNet architecture (3rd column) vs earlier architectures\nResNets or residual networks, introduced the concept of the residual. This can be understood looking at a small residual network of three stages. The striking difference between ResNets and earlier architectures quoted in the linked articles in http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-arch/ are the skip connections. Shortcut connections are those skipping one or more layers. The shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers. Identity shortcut connections add neither extra parameter nor computational complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries without modifying the solvers.\nHinton showed that dropping out individual neurons during training leads to a network that is equivalent to averaging over an ensemble of exponentially many networks. Entire layers can be removed from plain residual networks without impacting performance, indicating that they do not strongly depend on each other.\nEach layer consists of a residual module $f_i$ and a skip connection bypassing $f_i$. Since layers in residual networks can comprise multiple convolutional layers, we refer to them as residual blocks. With $y_{i-1}$ as is input, the output of the i-th block is recursively defined as\n$y_i = f_i(y_{i−1}) + y_{i−1}$\nwhere $f_i(x)$ is some sequence of convolutions, batch normalization, and Rectified Linear Units (ReLU) as nonlinearities. In the figure above we have three blocks. Each $f_i(x)$ is defined by\n$f_i(x) = W_i^{(1)} * \\sigma(B (W_i^{(2)} * \\sigma(B(x))))$\nwhere $W_i^{(1)}$ and $W_i^{(2)}$ are weight matrices, · denotes convolution, $B(x)$ is batch normalization and $\\sigma(x) ≡ max(x, 0)$. Other formulations are typically composed of the same operations, but may differ in their order.\nDuring the lecture we will go through this paper analysis of the unrolled network to understand the behavior of ResNets that are inherently scalable networks.\nResNets introduced below - are commonly used as feature extractors for object detection. They are not the only ones but these networks are the obvious / typical choice today and they can also be used in real time video streaming applications achieving significant throughput e.g. 20 frames per second.\n "});index.add({'id':33,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/object-detection/','title':"Object Detection",'content':"Object Detection In the introductory section, we have seen examples of what object detection is. In this section we will treat the detection pipeline itself, summarized below:\nObject detection pipeline.\nWork on object detection spans 20 years and is impossible to cover every algorithmic approach in this section - the interested reader can trace these developments by reading in this paper.\nAs expected, since 2014, deep learning has surpassed classical ML in the detection competitions - we therefore focus only on such architectures. More specifically we will be focusing on the so called two stage detectors that employ two key ingredients:\n Recognition using regions that we will explain shortly. CNNs that we covered earlier.  Object detection, involves three main stages: the feature extraction stage, the classification stage and the detection or localization stage. In the literature the feature and classification stages are counted as one, called the classification stage and people refer to such architecture as _two stage.\nWe also need to insert an additional requirement: to be able to detect objects in almost real time (20 frames per second) - a significant subset of what we call mission critical applications require it. Therefore will focus a specific family that is considered to be the canonical CNN architecture for detection - the family of Region CNNs.\nRegion CNN (RCNN) Region Proposals We can think about the detection problem as a classification problem of all possible portions (windows/masks) of the input image since an object can be located at any position and scale in the image. It is natural to search therefore everywhere and an obvious method to generate region proposals, is to slide various width-height ratio windows slide around the image and using a metric to declare that the window contains one or more blob of pixels. Obviously, such method is computationally infeasible and we need think of how to reduce this number by having some means of gauging where to look in the image.\nRCNN can accommodate a number of efficient algorithms that can produce region proposals. The baseline RCNN employs instead selective search via hierarchical grouping.\nThe algorithm contains another algorithm that segments the image into initial regions.\nGraph-based segmentation We perform segmentation in the image using an efficient graph-based algorithm to obtain the set $R=\\{r_1, \\dots, r_n \\}$ of initial regions. The segmentation algorithm starts by formulating the image as a graph.\nLet G = (V, E) be an undirected graph with vertices $v_i \\in V$ , the set of elements to be segmented, and edges $(v_i, v_j) ∈ E$ corresponding to pairs of neighboring vertices. Each edge has a corresponding weight $w((v_i, v_j ))$, which is a non-negative measure of the dissimilarity between neighboring elements $v_i$ and $v_j$. In the case of image segmentation, the elements in V are pixels and the weight of an edge is some measure of the dissimilarity between the two pixels connected by that edge (e.g., the difference in intensity, color, motion, location or some other local attribute).\nIn the graph-based approach, a segmentation $S$ is a partition of $V$ into components such that each component (or region) $C ∈ S$ corresponds to a connected component in a graph $G\u0026rsquo; = (V, E\u0026rsquo;)$, where $E\u0026rsquo; ⊆ E$. In other words, any segmentation is induced by a subset of the edges in $E$. There are different ways to measure the quality of a segmentation but in general we want the elements in a component to be similar, and elements in different components to be dissimilar. This means that edges between two vertices in the same component should have relatively low weights, and edges between vertices in different components should have higher weights.\nFor example, a pixel $p_i$ corresponds to a vertex $v_i$ and it has 8 neighboring pixels. We can define the weight to be the absolute value of the dissimilarity between the pixel intensity $I(p_i)$ and its neighbors.\n$$w((v_i, v_j )) = |I(p_i) − I(p_j )|$$\nBefore we compute the weights we use the 2D Gaussian kernel / filter we met in the introductory section to CNNs - the end result being a smoothing of the image that helps with noise reduction. For color images we run the algorithm for each of the three channels.\nThere is one runtime parameter for the algorithm, which is the value of $k$ that is used to compute the threshold function $\\tau$ . Recall we use the function $τ(C) =k/|C|$ where $|C|$ is the number of elements in C. Thus k effectively sets a scale of observation, in that a larger k causes a preference for larger components.\nThe graph algorithm can also accommodate dissimilarity weights between neighbors at a feature space rather at pixel level based on a Euclidean distance metric with other distance functions possible.\nGraph representation of the segmentation problem\nNotice that the initial segments may be many and do not necessarily accurately represent distinct objects as shown below:\nResult of the initial segments produced by the graph-based algorithm, $k=300$\nGrouping After the initial regions are produced, we use a greedy algorithm to iteratively group regions together. This is what gives the hierarchical in the name of the algorithm.\nFirst the similarities between all neighboring regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbors. The process of grouping the most similar regions is repeated until the whole image becomes a single region.\nFor the similarity $s(r_i ,rj)$ between region $r_i$ and $r_j$ we apply a variety of complementary measures under the constraint that they are fast to compute. In effect, this means that the similarities should be based on features that can be propagated through the hierarchy, i.e. when merging region $r_i$ and $r_j$ into $r_t$, the features of region $r_t$ need to be calculated from the features of $r_i$ and $r_j$ without accessing the image pixels. Such feature similarities include color, texture, size, fill - we create a binary sum of such individual similarities.\n$$ s(r_i ,r_j) = a_1 s_{color}(r_i ,r_j)+a_2 s_{texture}(r_i ,r_j) + a_3 s_{size}(r_i ,r_j) + a_4 s_{fill}(r_i ,r_j) $$\nwhere $a_i ∈ {0,1}$ denotes if the similarity measure is used or not.\nThe end result of the grouping is a hierarchy of regions ranked according to creation time. As earlier created regions may end up being the largest some permutation in the ranking is applied.\nExample of hierarchical grouping\nThe above selective search strategy is diversified (1) by using a variety of colour spaces with different invariance properties, (2) by using different similarity measures $s(r_i, r_j)$, and (3) by varying our initial regions. Each strategy results in a different hierarchy and after a permutation that randomizes the ranking the final list of regions is produced. For RCNN we use 2000 such region proposals.\nFinal proposals - in reality we have 2000 of such regions.\nCNN Features and SVM Classification Each of these proposals can be fed into a CNN (e.g. ResNet). Since regions are of various rectangular shapes, we warp the regions to a fixed size (CNNs can process fixed input tensors) of 227 x 227 pixels and the CNN produces a 4096-dim feature vector from each of the regions. Note that this representation of each region by 4096 elements is considered quite compact and more importantly the features are shared across classes.\nUsing these features we use a binary SVM classifier that produces a positive or negative on whether this feature contains the class of interest or not. We train a separate binary SVM classifier per class (binary classification).\nThe sharing of features allows us to parctically aggregate into a matrix all features of all regions (2000 x 4096) we well as aggregate the SVM weights into another matrix (4096 x K), where K is the number of classes we have trained for, and do this combined operation via a matrix-matrix product.\nAfter the scoring of each proposed region by the SVM we apply a greedy Non-Maximum Suppression (NMS) algorithm for each class independently, that rejects a region if it has an Intersection over Union (IoU) metric higher than a threshold with a higher scoring region. This threshold was empirically determined to be 0.3 for the task outlined in the paper. But it is expected to be a hyperparameter in practice.\nFinally, a bounding box regressor, predicts the location of the bounding boxes using the proposal boxes and nearby ground truth box data so that we can adjust the final detection box and avoid situations that whole objects are detected but partially included in the detection box.\nThe whole process is partially shown below and the solution is called Region-CNN (RCNN).\nRCNN pipeline\nRCNN detection result after processing multiple region proposals\nTraining RCNN training is complex as involves a multi-stage pipeline.\n R-CNN first fine pretrains a CNN on a classification task without using any bounding box ground truth. It then adapts the trained CNN by replacing the classification layer with another randomly initialized layer of span K+1, where K are the number of classes in the domain of interest (+1 is due to the fact that we consider the background). We continue training with SGD using (a) as inputs the warped images as determined by the region proposals (b) using this time the ground truth bounding boxes and declaring each region proposal with IoU ? 0.5 relative to the ground truth bounding box as true positive. Then, it fits SVMs to CNN features. These SVMs act as object detectors, replacing the softmax classifier learnt by fine-tuning.  .\nFast RCNN Fast-RCNN is the second generation RCNN that aimed to accelerate RCNN. Apart from the complex training of RCNN, its inference involved a forward pass for each of the 2000 proposals.\nFast RCNN Architecture\nA Fast RCNN network takes as input an entire image and a set of proposals $R$. The set of proposals is produced by the selective search alg used in RCNN and its similarly around 2000 per image.\n  The network first processes the whole image with a CNN having several convolutional (experiments were done for 5-13 layers) and 5 max pooling layers to produce a feature map. The selective search met in RCNN, produces region proposals and for each proposal, a region of interest (RoI) pooling layer (see below) extracts a fixed-length feature vector from the feature map. This is in contrast to the RCNN that fed the different proposals to the CNN. Now we only have one feature map and we elect regions of interest from that.\n  Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.\n  The key element in the architecture is the RoI pooling layer.\nWhat is an RoI? An RoI is a rectangular window into a feature map. Each RoI is defined by a four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w).\nThe RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g.7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI. RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling\nAs you may have noticed we have replaced the SVN and the training can happen end to end starting from a pretrained CNN and using a multi-task (classification, regression) loss function. NMS is maintained just like in RCNN to produce the final box.\nFaster RCNN With Faster RCNN, we are not making changes to Fast RCNN detector itself bur rather to the input of the CNN. The selective search algorithm that is considered slow and computationally expensive is replaced with a neural network called the Region Proposal Network (RPN) that as the name implies proposes regions.\nFaster RCNN Architecture - the RPN tells the Fast RCNN detector where to attend to\nTherefore, in this architecture there is one CNN network that does not only produces a global feature map but also it produces proposals from the feature map itself rather than the original image.\nRegion Proposals as generated by the RPN network\nIt is doing so by sliding a window n x n over the feature map. At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal.\nThe k proposals are parameterized relative to k reference boxes, which we call anchor boxes. The size can be changed but the original paper used anchor size of (128 × 128, 256 × 256, 512 × 512) and three aspect ratios (1:1, 1:2 and 2:1). An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio. By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a feature map of a size W × H (typically ∼2,400), there are $W xH x k$ anchors in total.\nThe RPN network produces a classification score i.e. how confident we are that there is an object for each of the anchor boxes as well as the regression on the anchor box coordinates.\nFinally, the following curve (focus on COCO) presents the relative performance between "});index.add({'id':34,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/semantic-segmentation/','title':"Semantic Segmentation",'content':""});index.add({'id':35,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/','title':"Lecture 6 - Probabilistic Graphical Models",'content':""});index.add({'id':36,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/pgm-intro/','title':"Introduction to Probabilistic Reasoning",'content':"Introduction to Probabilistic Reasoning In the scene understanding chapter we started putting together the perception pipelines that resulted in us knowing where are the objects of interest in the image coordinate frame but we stopped short of any advanced methods that can lead to what we humans call understanding. We hinted that assigning an object an symbolic identity is an essential ability that allows an embodied AI agent to reason about the scene using symbolic reasoning approaches researched by the AI community over many decades.\nPositioning of probabilistic reasoning subsystem\nStaying in this trajectory, we introduce the topic of reasoning via a probabilistic lens. We argue that enhancing the environment state as determined by the perception subsystem, includes another subsystem that we will call probabilistic reasoning subsystem that allow us to:\n infer the hidden state of the environment and learn the state that the agents internally maintains via appropriate to the task representations.  In a subsequent chapter we will enhance the model to include past actions rather than just past percepts. Let us now start with the rule that is the cornerstone of probabilistic modeling and reasoning.\nThe Bayes Rule Thomas Bayes (1701-1761)\nLet $\\mathbf{\\theta}$ denote the unknown parameters, $D$ denote the dataset and $\\mathcal{H}$ denote the hypothesis space that we met in the learning problem chapter. The Bayes rule states:\n$$ P(\\mathbf{\\theta} | D, \\mathcal{H}) = \\frac{P( D | \\mathbf{\\theta}, \\mathcal{H}) P(\\mathbf{\\theta} | \\mathcal{H}) }{ P(D|\\mathcal{H})} $$\nThe Bayesian framework allows the introduction of priors $p(\\theta | \\mathcal{H})$ from a wide variety of sources: experts, other data, past posteriors, etc. It allows us to calculate the posterior distribution from the likelihood function and this prior subject to a normalizing constant. We will call belief the internal to the agent posterior probability estimate of a random variable as calculated via the Bayes rule. For example,a medical patient is exhibiting symptoms x, y and z. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor (the expert) has beliefs about the underlying disease, but a second doctor may have slightly different beliefs.\nProbabilistic Graphical Models Let us now look at a representation, the probabilistic graphical model (pgm) (also called Bayesian network when the priors are captured) that can be used to capture the structure of such beliefs and in general capture dependencies between the random variables involved in the modeling of a problem. We can use such representations to efficiently compute such beliefs and in general conditional probabilities. For now we will limit the modeling horizon to just one snapshot in time - later we will expand to capture problems that include time $t$ as a variable.\nBy convention we represent in PGMsas directed graphs, with nodes being the random variables involved in the model and directed edges indicating a parent child relationship, with the arrow pointing to a child, representing that the child nodes are probabilistically conditioned on the parent(s).\nIn a hypothetical example of a joint distribution with $K=7$ random variables,\nPGM governing the joint distribution $p(x_1, x_2, \u0026hellip;, x_7)=p(x_1)p(x_2)p(x_3)p(x_4|x_1, x_2, x_3)p(x_5|x_1, x_3) p(x_6|x_4)p(x_7|x_4, x_5)$\nIn general,\n$$p(\\mathbf x)= \\prod_{k=1}^K p(x_k | \\mathtt{pa}_k)$$\nwhere $\\mathtt{pa}_k$ is the set of parents of $x_k$.\nNote that we have assumed that our model does not have variables involved in directed cycles and therefore we call such graphs Directed Acyclic Graphs (DAGs).\n"});index.add({'id':37,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/bayesian-inference/','title':"Inference in Graphical Models",'content':"Inference in Graphical Models Bayesian Linear Regression The PGM representation should not feel foreign - lets consider the simplest possible example of a graphical model and see how it connects to concepts we have seen before. Any joint distribution $p(\\bm x, y)$ can be decomposed using the product rule (we drop the data qualifier)\n$$p(\\bm x, y) = p(\\bm x) p(y|\\bm x)$$\nand such distribution can be represented via the simple PGM graph (a) below.\nSimplest possible PGM example\nWe introduce now a graphical notation where we shade, nodes that we consider observed. Let us know assume that we observe $y$ as shown in (b). We can view the marginal $p(\\bm x)$ as a prior over $x$ and and we can infer the posterior distribution using the Bayes rule\n$$p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$$\nwhere using the sum rule we know $p(y) = \\sum_{x\u0026rsquo;} p(y|x\u0026rsquo;) p(x\u0026rsquo;)$. This is a very innocent but very powerful concept.\nTo see why lets consider an online learning problem where the underlying target function is $p_{data}(x, \\mathbf a) = a_0 + a_1 x + n$ - this is the equation of the line. In this example its parametrized with $a_0=-0.3, a_1=0.5$ and $n \\in \\mathcal N(0, \\sigma=0.2)$. To match the simple inference exercise that we just saw, we draw the equivalent PGM\nBayesian Linear Regression example - please replace $t$ with $y$ to match earlier notation in these notes\nThe Bayesian update of the posterior can be intuitively understood using a graphical example of our model of the form: $$g(x,\\mathbf{w})= w_0 + w_1 x$$ (our hypothesis). The reason why we pick this example is illustrative as the model has just two parameters and is amendable to visualization. The update needs a prior distribution over $\\mathbf w$ and a likelihood function. As prior we assume a spherical Gaussian\n$$p(\\mathbf w | \\alpha) = \\mathcal N(\\mathbf w | \\mathbf 0, \\alpha^{-1} \\mathbf I)$$\nwith $\\alpha = 0.2$. We starts in row 1 with this prior and at this point there are no data and the likelihood is undefined while every possible linear (line) hypothesis is feasible as represented by the red lines. In row 2, a data point arrives and the the Bayesian update takes place: the previous row posterior becomes the prior and is multiplied by the current likelihood function. The likelihood function and the form of the math behind the update are as shown in Bishop\u0026rsquo;s book in section 3.3. Here we focus on a pictorial view of what is the update is all about and how the estimate of the posterior distribution $p(\\mathbf w | \\mathbf y)$ ultimately (as the iterations increase) it will be ideally centered to the ground truth ($\\bm a$).\nInstructive example of Bayesian learning as data points are streamed into the learner. Notice the dramatic improvement in the posterior the moment the 2nd data point arrives. Why is that?\nBayesian vs Maximum Likelihood In the linear regression section we have seen a simple supervised learning problem that is specified via a joint distribution $\\hat{p}_{data}(\\bm x, y)$ and are asked to fit the model parameterized by the weights $\\mathbf w$ using ML. Its important to view pictorially perhaps the most important effect of Bayesian update:\n In ML the $\\mathbf{w}$ is treated as a known quantity with an estimate $\\hat{\\mathbf{w}}$ that has a mean and variance resulting from the distribution of $y$. In the Bayesian setting, we are integrating over the distribution of $\\mathbf{w}$ given the data i.e. we are not making a point estimate of $\\mathbf{w}$ but we marginalize out $\\mathbf{w}$. $$p(\\mathbf{w}|y) = \\frac{p(y|\\mathbf{w}) p(\\mathbf{w})}{\\int p(y|\\mathbf{w}) p(\\mathbf{w}) d\\mathbf{w}}$$  We get at the end a posterior (predictive) distribution rather than a point estimate. As such it can capture the effects of sparse data producing more uncertainty via its covariance in areas where there are no data as shown in the following example which is exactly the same sinusoidal dataset fit with Bayesian updates and Gaussian basis functions.    ML frameworks have been enhanced recently to deal with Bayesian approaches and approximations that make such approaches feasible for both classical and deep learning. TF.Probability and PyTorch Pyro are examples of such enhancements.\n This example is instructive beyond the habit of having coin flip examples in every textbook in probability theory and statistics. It is useful to understand the conjugate prior distribution being discussed in Bishop's section 2.1.1 and Figure 3 that the code above replicates. Armed with this understanding, we can now treat the Bayesian update for linear regression as described in the [linear regression section](/docs/lectures/regression/linear-regression). -- "});index.add({'id':38,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/recursive-state-estimation/','title':"Recursive State Estimation",'content':"Recursive State Estimation In Inference in Graphical Models section we have seen how sequential data belonging to just two evidential variables (captured via $p(x,y)$) can be treated by probabilistic models to infer (reason) about values of the posterior. Now we will expand on two fronts:\n  Introduce the concept of state $s$ that encapsulates multiple random variables and consider dynamical systems with non-trivial non-linear dynamics (state transition models) common in robotics, medical diagnosis and many other fields.\n  Introduce the time index $t$ explicitly in the aforementioned state evolution as represented via a graphical model.\n  The perception subsystem, that processes sensor data produces noisy estimates (object detections etc.) - these can be attributed to the algorithmic elements of the subsystem or to imperfections in the sensors themselves. The model that captures the perception pipeline from sensors to estimates will be called measurement model or sensor model. So in summary we have two abstractions / models that we need to be concerned about: the transition model of the environment state and the sensor model.\nSuch expansion, will allow us to form using the Bayes rule, perhaps one of the most important contributions to the probabilistic modeling of dynamical systems: the recursive state estimator also known as Bayes filter that affords the agent the ability to maintain an internal belief of the current state of the environment.\nBayes Filter We are introducing this algorithm, by considering a embodied agent (a robot) that moves in an environment characterized by the so called Environment Type 4 in the taxonomy presented in the agent chapter.\nAgent belief and environment interactions\nThe state of such environment contain variables that capture dynamics such as pose (6D) that includes location and orientation, agent velocity, other objects poses, etc., as well as static state variables such as location of obstacles, walls etc. Most practical algorithms for state estimation assume that the stochastically evolving environment is not affected from state variables prior to $s_t$. This is the Markovian assumption and is key in making such algorithms tractable. Note that the assumption does not constraint the actual time internal that it is impactful for the future as we are free to define anyway we want the state $s_t$. It may for example use a super-state that consists of two states in corresponding time intervals $s_t=[s_{t-1}, s_t]$. We call this the Markov order - in this case the order is two. In the figure below you can see the PGM that corresponds to the Markov assumption.\nDynamic Bayesian Network that characterizes the Markov evolution of states, measurements and controls - in the text we use for states the letter $s$ instead of $x$ and for actions the letter $a$ instead of $u$.\nThe above graph decomposes as follows:\n$$p(z_t|s_{0:t}, z_{1:t}, a_{1:t})=p(z_t|s_t)$$ $$p(s_t|s_{1:t-1}, z_{1:t}, a_{1:t})=p(s_t|s_{t-1}, a_t)$$\nIn the following we will use $z_{t_1:t_2}$ to represent sensing estimates of the perception subsystem acquired between $t_1$ and $t_2$. The measurement or sensor model is given by the conditional probability distribution $p(z_t|x_t)$. Note a couple of important points: as measurements arrive over time, the knowledge of the agent increases and there may not be dependency on time for the measurement model.\nWe will also use the conditional probability to represent the state transition $p(s_t | s_{t-1}, a_t)$ where $a_t$ is the control action variable that the agent executes causing a state change in the environment. By convention, we execute first a control action $a_1$ and then measure $z_1$.\nThe belief is the posterior distribution over the state $s_t$ conditioned on all past measurements and actions.\n$$\\mathtt{bel}(s_t) = p(s_t | z_{1:t}, a_{1:t})$$\nIt would also be useful to define the belief just after we took action $a_t$ but before considering the measurement $z_t$\n$$\\mathtt{\\hat{bel}}(s_t) = p(s_t | z_{1:t-1}, a_{1:t})$$\nThe Bayes filter is a recursive algorithm that involves two steps:\n(a) the prediction step that estimates the belief $\\mathtt{\\hat{bel}}(s_t)$ from the belief of the previous recursion $\\mathtt{bel}(s_{t-1})$\n(b) the _measurement update_ step that that weighs the belief $\\mathtt{\\hat{bel}}(s_t)$ with the probability that measurement $z_t$ was observed.\nBayes Filter\n $\\mathtt{bel}(s_t)$ = bayes_filter($\\mathtt{bel}(s_{t-1}), a_t, z_t)$\nfor all $s_t$ do:\n$→ \\mathtt{\\hat{bel}}(s_t) = \\int p(s_t | a_t, s_{t-1}) \\mathtt{bel}(s_{t-1}) ds_{t-1}$ (prediction)\n$→ \\mathtt{bel}(s_t) = \\eta p(z_t | s_t) \\mathtt{\\hat{bel}}(s_t)$ (measurement update)\nendfor\n To illustrate how the Bayes filter is useful, lets look at a practical example. This example was borrowed from Sebastian Thrun\u0026rsquo;s book, \u0026ldquo;Probabilistic Robotics\u0026rdquo;, MIT Press, 2006.\nDoor state estimation The problem we are considering is estimating the state of a door using an agent (robot) equipped with a monocular camera.\nFor simplicity lets assume that the door can be in any of two possible states (open or closed) and that the agent does not know the initial state of the door. Therefore initially, its beliefs are:\n$$\\mathtt{bel}(s_0=open) = 0.5$$ $$\\mathtt{bel}(s_0=closed) = 0.5$$\nMeasurement Model No real agent has ideal sensing abilities so the sensor or measurement model is noisy and lets assume for simplicity that its given by:\n   Description Probabilistic Model     if its open, agent can sense it as such with prob 60% $p(z_t = sense-open | s_t = open) = 0.6$   if its closed, agent can sense it as such with prob 40% $p(z_t = sense-closed | s_t = open) = 0.4$   if its closed, agent senses it open with prob 20% $p(z_t = sense-open | s_t = closed) = 0.2$   if its closed, agent can sense it as such with prob 80% $p(z_t = sense-closed | s_t = closed) = 0.8$    The values in the measurement model above are not necessarily chosen randomly as computer vision algorithms (or LIDAR) may find it easier to detect a closed door from an open door, since with an open door the camera sees the clutter inside the room and the LIDAR may confuse the clutter returns with a closed door.\nTransition Model Lets also assume that the agent is using a arm manipulator to push the door open if its closed. Note So we have the following transition distribution:\n   Transition description Probabilistic Finite State Machine     if its open, a push leaves it open $p(s_t = open | a_t=push, s_{t-1} = open) = 1$   if its open, a push does not close it $p(s_t = closed | a_t=push, s_{t-1} = open) = 0$   if its closed, a push opens it with probability 80% $p(s_t = open | a_t=push, s_{t-1} = closed) = 0.8$   if its closed, a push leaves it closed with probability 20% $p(s_t = closed | a_t=push, s_{t-1} = closed) = 0.2$   if its open, doing nothing leaves it open $p(s_t = open | a_t=inaction, s_{t-1} = open) = 1$   if its open, doing nothing does not close it $p(s_t = closed   if its closed, doing nothing does not open it $p(s_t = open   if its closed, doing nothing leaves it closed $p(s_t = closed    As we mentioned before, by convention the agent first acts and then senses. If you reverse sensing and action you arrive in the same equations with just some index differences.\nLets assume that at $t=1$, the agent takes no action but senses the door is open. The two steps of RSE are as follows:\nRecursive State Estimation at $t=1$ - Step 1: Prediction $$\\mathtt{\\hat{bel}}(s_1) = \\int p(x_1 | a_1, s_0) ds_0 = \\sum_{s_0} p(s_1 | a_1, s_0) \\mathtt{bel}(s_0)$$ $$ = p(s_1 | a_1 = inaction, s_0 = open) \\mathtt{bel}(s_0=open) + p(s_1 | a_1 = inaction, s_0 = closed) \\mathtt{bel}(s_0=closed)$$\nFor all possible values of the state variable $s_1$ we have\n$$\\mathtt{\\hat{bel}}(s_1 = open) = 1 * 0.5 + 0 * 0.5 = 0.5$$\nThe fact that the belief at this point equals the prior belief (stored in the agent) is explained from the fact that inaction shouldn\u0026rsquo;t change the environment state and the environment state does not change itself over time in this specific case.\nRecursive State Estimation at $t=1$ - Step 2: Measurement Update In this step we are using the perception subsystem to adjust the belief with what it is telling us:\n$$ \\mathtt{bel}(s_1) = \\eta p(z_1 = sense-open| s_1) \\mathtt{\\hat{bel}}(s_1) $$\nFor the two possible states at $t=1$ we have\n$$\\mathtt{bel}(s_1=open) = \\eta p(z_1 = sense-open| s_1=open) \\mathtt{\\hat{bel}}(s_1=open)$$ $$ = \\eta 0.6 * 0.5 = \\eta 0.3 $$\n$$\\mathtt{bel}(s_1=closed) = \\eta p(z_1 = sense-open| s_1=closed) \\mathtt{\\hat{bel}}(s_1=closed)$$ $$ = \\eta 0.2 * 0.5 = 0.1$$\nThe normalizing $\\eta$ factor can now be calculated: $\\eta = 1/(0.3 + 0.1) = 2.5$.\nTherefore:\n$$\\mathtt{bel}(s_1=open) = 0.75$$\n$$\\mathtt{bel}(s_1=closed) = 0.25$$\nIn the next time step lets assume that the agent pushes the door and senses that its open. Its easy to verify that\n$$\\mathtt{\\hat{bel}}(s_2 = open) = 0.95$$ $$\\mathtt{\\hat{bel}}(s_2 = closed) = 0.05$$\n$$\\mathtt{bel}(s_2 = open) = 0.983$$ $$\\mathtt{bel}(s_2 = closed) = 0.0017$$\nThis example, although simplistic is indicative of the ability of the Bayes filter to incorporate perception and action into one framework. Although the example was for an embodied agent with a manipulator, the notion of action is optional. Beliefs can be recursively updated even if the action is not taken explicitly by the agent. Your cell phones have the ability to localize themselves using exactly the same Bayesian filter with different sensing (RF signals) despite the fact that they don\u0026rsquo;t move by themselves but are carried by you in their environment.\n"});index.add({'id':39,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/localization/','title':"Localization and Tracking",'content':"Localization and Tracking In the recursive state estimation section we have seen the formulation of the Bayes filter and its application in a simple problem of trying to maintain a latent (internal to the agent) belief about the state of the environment $s$ given measurements $z$ and agent actions $a$. Here we are closing the loop: we started with the perception system giving us poses of objects it detects, effectively localizing the objects at the pixel coordinate system (within the image). Now we need to use the Bayes filter to localize the object globally as well as to be able to track it, since it can also move across space, so that we can recognize it over time as the same unique object and be able to assign a symbol to it.\nIn order to localize the object globally we need to localize the agent globally and so we will focus here on a situation where the initial pose of the agent is unknown as it is initially placed somewhere in its environment, but it lacks knowledge of where it is. When we have the global pose of the sensor, for example a camera, we can use straightforward techniques for the global localization of objects e.g. calibrated stereo reconstruction or triangulation, section 14.6.\nEven without global localization we can track an object as it moves within the viewpoint of the agent for the sole purpose to predict its subsequent location within the image and assign a local instance identity to it. For example as an agent moves in the space surrounding the object, its perception subsystem makes a memoryless detection decision but its probabilistic reasoning subsystem should stitch such decisions together to reason as to the object\u0026rsquo;s identity over time: is it the same object that was seen a frame ago? By knowing the pose of the agents sensors (a camera) we should be able to make such inferences.\nLocalization GPM for localization of mobile agent - notice the map $m$ is known. In the text we use for states the letter $s$ instead of $x$ and for actions the letter $a$ instead of $u$.\nFor localization we are also given a map $m$ that can take multiple shapes. Most common maps are as shown below:\nMap assumed known for agent localization - notice that beliefs may be ambiguous for certain maps and locations\nThe Bayes filter for the localization problem must consider the map and it becomes:\nBayes Filter for Localization and Tracking\n $\\mathtt{bel}(s_t)$ = bayes_filter($\\mathtt{bel}(s_{t-1}), a_t, z_t, m)$\nfor all $s_t$ do:\n$→ \\mathtt{\\hat{bel}}(s_t) = \\int p(s_t | a_t, s_{t-1}, m) \\mathtt{bel}(s_{t-1}) ds_{t-1}$ (prediction)\n$→ \\mathtt{bel}(s_t) = \\eta p(z_t | s_t, m) \\mathtt{\\hat{bel}}(s_t)$ (measurement update)\nendfor\n What distinguishes localization from tracking is that for localization, the initial belief is:\n$$ \\mathtt{be}(s_0) = 1 / |S|$$\nwhere $|S|$ is the number of states (poses) that the robot can have.\nTo illustrate the localization problem solution with probabilistic reasoning lets consider an simple environment with a corridor and three doors.\nExample environment for agent localization - a hallway with three identical doors\n  Our initial belief is obviously uniform $\\mathtt{\\hat{bel}}(s_0) = 1 / |S|$ over all poses, as illustrated in row 1.\n  The agent uses its perception that indicates that it is adjacent to one of the doors, which in effect means that the shape of $p(z_1 | s_0, m)$ is as shown in the 2nd row.\n  It multiplies its previous belief with this distribution during measurement update producing the new belief $\\mathtt{bel}(s_1)$ as shown in the 2nd row (black pdf). The resulting belief is multi-modal, reflecting the residual uncertainty of the agent at this point.\n  As the agent moves to the right, it convolves (the integral in the Bayes filter is a convolution) its belief with the transition (motion) model $p(s_2 | a_2, s_1)$. It still does not know where it is - all it knows that its belief needs to follow what its odometry sensors told it which is a move to the right by so many meters. The effect is visualized in row 3 that shows a shifted (by the odometric information) and slightly less peaked (movement reduces confidence) belief.\n  The final measurement update that multiplies $\\mathtt{bel}(s_1)$ with $p(z_2 | s_2, m)$ results into obtaining a belief $\\mathtt{bel}(s_2)$ where most of the probability mass is focused on the correct pose.\n  As the agent moves further to the right, the transition (motion) model convolutions reduce the confidence of the agent as the perception system is effectively inactive due to the lack of distinguishing features in its locale.\n  Tracking Just like localization, tracking can refer to location prediction of the agent itself or the objects that the agents perceives in its environment. For tracking we are given the initial pose $s_0$ that is approximately known. It is common to initialize the belief with a narrow Gaussian distribution around $\\hat{s}_0$.\n$$\\mathtt{bel}(s_0) = \\mathcal{N}(s_0-\\hat s_0, \\Sigma)$$\nLets look at a similar example to the localization problem:\nExample environment for tracking - a hallway with three identical and labeled doors\nThe original belief is a normal distribution centered around door 1 where the agent is initially placed. As the robot moves to the right, its belief is convolved with the Gaussian transition / motion model. The resulting belief is a shifted Gaussian of increased width. Now suppose the robot detects that it is in front of door 2. Folding this measurement probability into the robot’s belief yields the posterior shown in (c). Note that the variance of the resulting belief is smaller than the variances of both the robot’s previous belief and the observation density. This is natural, since integrating two independent estimates should make the robot more certain than each estimate in isolation. After moving down the hallway, the robot’s uncertainty in its position increases again, since the tracker continues to incorporate motion uncertainty into the robot’s belief.\nDuring the lecture we will go through a white board single dimensional problem solved via a specific parametrization of the Bayes filter with Gaussians - this filter has a famous name: Kalman filter.\n"});index.add({'id':40,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/','title':"Lecture 7 - Planning",'content':"Planning In recursive state estimation chapter we made two advances in our modeling tool set:\n We introduced sequenced over time events and the concept of a varying state over such sequences. We saw how the agent state as dictated by an underlying dynamical model and and how to estimate it recursively using a graphical model that introduced the Bayesian filter. We saw that many well known estimation algorithms such as the Kalman filter are specific cases of this framework.  With this probabilistic reasoning in place, we can now assign symbols that represent e.g. objects in the scene since we can ground their unique attributes (e.g. location, embeddings) and track them. But probabilistic reasoning is not enough. In many problems we need to:\n Be able to accumulate knowledge (static rules and percepts) over a periods of time. Reason beyond what we perceive. Be able to express the domain and problem at hand in a suitably expressive language that can interface with domain-independent solvers to find solutions (usually via search algorithms).  These are implemented in the planning subsystem that is our next logical step positioned after the probabilistic reasoning to provide the best sequence of actions to reach our goal.\n"});index.add({'id':41,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/propositional-logic/','title':"Propositional Logic",'content':"Propositional Logic We have seen in the route planning module that for some problems agents do not need to possess significant representational abilities - its enough to represent the environment states as atomic.\nWe also have seen in an earlier chapter where we introduced a dynamical system governing the state evolution of the environment that a state is composed of variables and such factored representations are key in allowing reasoning at the belief space that are efficiently computed using recursive state estimators. We called this subsystem probabilistic reasoning subsystem and we positioned it immediately after the reflexive perception for a good reason. It is processing a very high \u0026lsquo;bandwidth\u0026rsquo; state information that is noisy and needs to be filtered by - you guessed it - Bayesian filters to make it very useful for the subsequent upstream services of the AI system.\nIn this chapter we will see another powerful representation, internal to the agent, that can help agents to reason by expressing assertions about the world. These assertions are called sentences and are expressed in a knowledge representation language that is used to build a Knowledge Base (KB) - a concept central to AI. The KB starts with prior knowledge about the domain and problem at hand and incrementally is updated with the output of the probabilistic reasoning subsystem.\nLets see a concrete example as described in the figure below,\nImagine that the agent is a security robot at an airport and is moving around an environment it hasnt seen before. The problem it tries to solve is to identify an abandoned backpack / luggage.\nDespite the simplicity of the problem statement this is a very challenging problem to solve. To begin with the robot must be creating static maps of the environment to be able to navigate it. Closer to the core task, we will need to preprogram the agent with all sorts of deep learning models to be able to even recognize a backpack in its perception system. Unique object ID must be assigned (symbolic segmentation) and a very accurate scene graph must also be generated by its probabilistic reasoning system. It does not stop there, the agent must be able to distinguish a situation where the backpack that was left behind by its carrier to a companion traveler to e.g. visit the restroom from the situation where the backpack was left behind maliciously. Relational information must therefore be generated to associate objects (e.g. carriers and pieces of luggage) and such information must be stored in the KB but perhaps more importantly the agent must be able to reason using the stored information in the KB.\nFor example, for the sentence $\\beta$ that declares that the backpack was separated from the owner, we need to entail a sentence $\\alpha$ (and we denote it as $\\alpha \\models \\beta$) that the backpack was handed over to $\\mathtt{nobody}$ and therefore justify an action to sound the security alarm. Note the direction in notation: $\\alpha$ is a stronger assertion that $\\beta$.\nWorld Models For each problem like the abandoned luggage above, we can define a number of world models each representing every possible state (configuration) of the environment that the agent may be in. The specific world model $m$ is, in other words, a mathematical abstraction that fixes as TRUE or FALSE each of the sentences it contains and as you understand, depending on the sentences, it may or may not correspond to reality. For example, the sentence\n$$c = \\text{\u0026ldquo;backpack is hanging from the ceiling\u0026rdquo;}$$\nmay be part of a model but it does not correspond to reality and chances are that this sentence will not be satisfied in many models that contain it. We denote the set of models that satisfy sentence $\\alpha$ as $M(\\alpha)$. We also say that $m$ is a model of $\\alpha$. Now that we have defined the world model we can go back to the definition of entailment in the earlier example and write:\n$$ \\alpha \\models \\beta \\iff M(\\alpha) ⊆ M(\\beta)$$\nThe representation language used in the KB sentences must offer two features: clear syntax and clear meaning (semantics). To describe both we will use a very simple world known in the literature as the Wumpus World (a cave) shown below.\nWumpus World. It was inspired by a video game Hunt the Wumpus by Gregory Yob in 1973. The Wumpus world is a cave which has 4/4 rooms connected with passageways. So there are total 16 rooms which are connected with each other. We have a agent who will move in this world. The cave has a room with a beast which is called Wumpus, who eats anyone who enters the room. The Wumpus can be shot by the agent, but the agent has a single arrow. There are pit rooms which are bottomless, and if agent falls in such pits, then he will be stuck there forever. The agent\u0026rsquo;s goal is to find the gold and climb out of the cave without falling into pits or eaten by Wumpus. The agent will get a reward if he comes out with gold, and he will get a penalty if eaten by Wumpus or falls in the pit.\nThe Performance Environment Actuators and Sensors (PEAS) description is summarized in the table.\n   Attribute Description     Environment Locations of gold and wumpus are uniformly random while any cell other than the starting cell can be a pit with probability of 0.2. One instantiation of the environment that is going to be fixed throughout the problem solving exercise is shown in the figure above. The agent always start at [1,1] facing to the right.   Performance +1000 points for picking up the gold. This is the goal of the agent.    −1000 points for dying. Entering a square containing a pit or a live Wumpus monster.    −1 point for each action taken so that the agent should avoid performing unnecessary actions.    −10 points for using the arrow trying to kill the Wumpus.   Actions Turn 90◦ left or right    Forward (walk one square) in the current direction    Grab an object in this square    Shoot the single arrow in the current direction, which flies in a straight line until it hits a wall or the Wumpus    Climb out of the cave - this is possible only in the starting location [1,1]   Sensors Stench when the Wumpus is in an adjacent square — directly, not diagonally    Breeze when an adjacent square has a pit    Glitter when the agent perceives the glitter of the gold in the current square    Bump when the agent walks into an enclosing wall (and then the action had no effect)    Scream when the arrow hits the Wumpus, killing it.    To use the definitions we saw in the introductory chapter the environment is:\n Static: It is static as Wumpus and Pits are not moving. Discrete: The environment is discrete. Partially observable: The Wumpus world is partially observable because the agent can only perceive the close environment such as an adjacent room. Sequential: The order is important, so it is sequential.  But why this problem requires reasoning? Simply put, the agent needs to infer the state of adjacent cells from its perception subsystem in the current cell and the knowledge of the rule of the wumpus world. These inferences will help the agent to only move to an adjacent cell when it has determined that the cell is OK for it move into.\nAgent A moving in the environment infering the contents of adjacent cells. (a) Initial environment after percept [Stench, Breeze, Glitter, Bump, Scream]=[None, None, None, None, None]. (b) After one move with percept [Stench, Breeze, Glitter, Bump, Scream]=[None, Breeze, None, None, None].\n   Environment State Description     (a) Agent A starts at [1, 1] facing right. The background knowledge β assures agent A that he is at [1, 1] and that it is OK = certainly not deadly.    Agent A gets the percept [Stench, Breeze, Glitter, Bump, Scream]=[None, None, None, None, None].    Agent A infers from this percept and β that its both neighboring squares [1, 2] and [2, 1] are also OK: “If there was a Pit (Wumpus), then here would be Breeze (Smell) — but isn’t, so. . . ”. The KB enables agent A to discover certainties about parts of its environment — even without visiting those parts.   (b) Agent A is cautious, and will only move to OK squares. Agent A walks into [2, 1], because it is OK, and in the direction where agent A is facing, so it is cheaper than the other choice [1, 2]. Agent A also marks [1, 1] Visited.    Agent A perceives [Stench, Breeze, Glitter, Bump, Scream]=[None, Breeze, None, None, None]. Agent A infers: “At least one of the adjacent squares [1, 1], [2, 2] and [3, 1] must contain a Pit. There is no Pit in [1, 1] by my background knowledge β. Hence [2, 2] or [3, 1] or both must contain a Pit.” Hence agent A cannot be certain of either [2, 2] or [3, 1], so [2, 1] is a dead-end for a cautious agent like A.    Agent moving in the environment infering the contents of adjacent cells. (a) After the 3rd move and percept [Stench, Breeze, Glitter, Bump, Scream]=[Stench, None, None, None, None]. (b) After the 5th move with percept [Stench, Breeze, Glitter, Bump, Scream]=[Stench, Breeze, Glitter, None, None].\n   Environment State Description     (a) Agent A has turned back from the dead end [2, 1] and walked to examine the other OK choice [1, 2] instead.    Agent A perceives [Stench, Breeze, Glitter, Bump, Scream]=[Stench, None, None, None, None].    Agent A infers using also earlier percepts: “The Wumpus is in an adjacent square. It is not in [1, 1]. It is not in [2, 2] either, because then I would have sensed a Stench in [2, 1]. Hence it is in [1, 3].”    Agent A infers using also earlier inferences: “There is no Breeze here, so there is no Pit in any adjacent square. In particular, there is no Pit in [2, 2] after all. Hence there is a Pit in [3, 1].”    Agent A finally infers: “[2, 2] is OK after all — now it is certain that it has neither a Pit nor the Wumpus.”   (b) Agent A walks to the only unvisited OK choice [2, 2]. There is no Breeze here, and since the square of the Wumpus is now known too, [2, 3] and [3, 2] are OK too.    Agent A walks into [2, 3] and senses the Glitter there, so it grabs the gold and succeeds.    Model-Checking Inference The wumpus world despite its triviality, contains some deeper abstractions that are worth summarizing.\n  Logical inference can be done via an internal representation that are sentences - their syntax and semantics we will examined next.\n  Sentences may be expressed in natural language. Together with the perception and probabilistic reasoning subsystems that can generate symbols associated with a task, the natural language can be grounded and inferences can be drawn in a \u0026lsquo;soft\u0026rsquo; or probabilistic way at the symbolic level.\n  The reasoning algorithm regarding the possible state of the environment in surrounding cells that the agent performed informally above, is called model checking because it enumerates all possible models to check that a sentence $a$ is supported by the KB i.e. $M(KB) ⊆ M(\\alpha)$.\nPossible Models in the presence of pits in cells [1,2],[2,2] and [3,1]. There are $2^3=8$ possible models. The KB when the percepts indicate nothing in cell [1,1] and a breeze in [2,1] is shown as a solid line. With dotted line we show all models of $a_1=\\text{\u0026ldquo;not have a pit in [1,2]\u0026quot;}$ sentence.\nSame situation as the figure above but we indicate with dot circle a different sentence $a_2$. What is this sentence?\nPropositional Logic Syntax The syntax defines the allowable sentences that can be complex. Each atomic sentence consists of a single (propositional) symbol and the fixed symbols TRUE \u0026amp; FALSE. In BNF the atomic sentences or formulas are also called terminal elements. Complex sentences can be constructed from sentences using logical operators (connectives that connect two or more sentences). In evaluating complex sentences the operator precedence shown in the figure below must be followed.\nBNF grammar of propositional logic\nOut of all the operators, two are worthy of further explanation.\n imply (⇒) operator: the left hand side is the premise and the right hand side is the implication or conclusion or consequent. This is an operator of key importance known as rule. Its an if-then statement. if and only if (⇔) operator: its expressing an equivalence (≡) or a biconditional.  Propositional Logic Semantics The semantics for propositional logic specify how to compute the value of any sentence given a model. To do so we use the following truth table.\nTruth tables for three primary and five derived logical operators. Note the presence of the XOR connective.\nGiven a world model in the KB\n$$m_1 = \\left[ P_{1,2}=FALSE, P_{2,2}=FALSE, P_{3,1}=TRUE \\right]$$\na sentence can be assigned a truth value (FALSE/TRUE) using the semantics above. For example the sentence,\n$$\\neg P_{1,2} \\land (P_{2,2} \\lor P_{3,1}) = TRUE $$\nInference Example Using the operators and their semantics we can now construct an KB as an example for the wumpus world. We will use the following symbols to describe atomic and complex sentences in this world.\n   Symbols Description     $P_{x,y}$ Pit in cel [x,y]   $W_{x,y}$ Wumpus (dead or alive) in cel [x,y]   $B_{x,y}$ Perception of a breeze while in cel [x,y]   $S_{x,y}$ Perception of a stench while in cel [x,y]    Using these symbols we can convert the natural language assertions into logical sentences and populate the KB. The sentences $R_1$ and $R_2/R_3$ are general rules of the wumpus world. $R_4$ and $R_5$ are specific to the specific world instance and moves of the agent.\n   Sentence Description KB     $R_1$ There is no pit in cel [1,1] $\\neg P_{1,1}$   $R_2$ The cell [1,1] is breezy if and only if there is a pit in the neighboring cell. $B_{1,1} ⇔ (P_{1,2} \\lor P_{2,1})$   $R_3$ The cell [2,1] is breezy if and only if there is a pit in the neighboring cell. $B_{2,1} ⇔ (P_{1,1} \\lor P_{2,2} \\lor P_{3,1})$   $R_4$ Agent while in cell [1,1] perceives [Stench, Breeze, Glitter, Bump, Scream]=[None, None, None, None, None] $\\neg B_{1,1}$   $R_5$ Agent while in cell [2,1] perceives [Stench, Breeze, Glitter, Bump, Scream]=[None, Breeze, None, None, None] $B_{2,1}$    As the agent moves, it uses the KB to decide whether a sentence is entailed by the the KB or not. For example can we infer that there is no pit at cell [1,2] ? The sentence of interest is $ \\alpha = \\neg P_{1,2}$ and we need to prove that:\n$$ KB \\models \\alpha $$\nTo answer such question we will use the model checking algorithm outlined in this chapter: enumerate all models and check that $\\alpha$ is true for in every model where the KB is true. We construct the truth table that involves the symbols and sentences present in the KB:\nAs described in the figure caption, 3 models out of the $2^7=128$ models make the KB true and in these rows the $P_{1,2}$ is false.\nAlthough the model checking approach was instructive, there is an issue with its complexity. Notice that if there are $n$ symbols in the KB there will be $2^n$ models, the time complexity is $O(2^n)$.\nThe symbolic representation together with the explosive increase in the number of sentences in the KB as time goes by, cant scale. Another approach to do entailment, potentially more efficient, is theorem proving where we are applying inference rules directly to the sentences of the KB to construct a proof of the desired sentence/query. Even better, we can invest in new representations as described in the PDDL chapter to develop planning approaches that combine search and logic and do not suffer necessarily from the combinatorial explosion problem.\nIf you need to review the BNF expressed grammar for propositional logic (as shown in the syntax above) review part 1 and part 2 video tutorials.  "});index.add({'id':42,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/planning-application/prediction/','title':"Online Prediction",'content':""});index.add({'id':43,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/planning-application/behavior/','title':"Behavioral Planning",'content':""});index.add({'id':44,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/logical-agents/','title':"Logical Agents",'content':"Logical Agents In this chapter we see how agents equipped with the ability to represent internally the state of the environment and reason about the effectiveness of possible actions using propositional logic, can construct plans that are guaranteed to achieve their goals. We start with the KB we developed in wumpus world and enhance it by introducing representations that avoid looking back at the whole percept history to make an inference about the state of the environment currently.\nEnhancing the KB We add a couple of sentences in the KB we had developed,focusing on axioms (facts or rules that evaluate to TRUE). When these axioms involve variables that they never change we call such variables atemporal. This is shown in the next table.\n   Sentence Description KB     $R_1$ There is no pit in cel [1,1] $\\neg P_{1,1}$   $R_2$ The cell [1,1] is breezy if and only if there is a pit in the neighboring cell. $B_{1,1} ⇔ (P_{1,2} \\lor P_{2,1})$   $R_3$ The cell [2,1] is breezy if and only if there is a pit in the neighboring cell. $B_{2,1} ⇔ (P_{1,1} \\lor P_{2,2} \\lor P_{3,1})$   $R_4$ The cell [1,1] is stenchy if and only if there is a wumpus in the neighboring cell. $S_{1,1} ⇔ (W_{1,2} \\lor W_{2,1})$   $R_5$ There is only one wumpus in this world.    $R_{5-1}$ There is at least one wumpus $W_{1,1} \\lor W_{1,2} \\lor \u0026hellip; \\lor W_{4,4}$   $R_{5-2}$ There is at most one wumpus    $R_{5-2-1}$  $\\neg W_{1,1} \\lor \\neg W_{1,2}$   $R_{5-2-2}$  $\\neg W_{1,1} \\lor \\neg W_{1,3}$   \u0026hellip;  \u0026hellip;   $R_{5-2-N}$  $\\neg W_{4,3} \\lor \\neg W_{4,4}$    With respect to percepts and and in fact with everything in the world that is dynamic, we introduce the notion of time ($t$) and index all such variables accordingly. We do so to avoid the situation where we have conflicts in the KB where while in cell [1,2] we store in the KB $\\mathtt{Stench}$ and in another instance e.g. in cell [2,2] we store $\\neg \\mathtt{Stench}$. These type of variables are called some times called fluent but effectively these are the usual mutable state variables. Location as we have seen is a primary variable that is carried by the environment state. We can then define an associated fluent $L_{x,y}^t$ and store the following sentences in the KB.\n   Sentence Description KB     $R_6$ if an agent is in cell [x,y] at $t$ and perceives [Stench, Breeze, Glitter, Bump, Scream]=[None, Breeze, None, None, None] then the cell is a Breeze cell $L_{x,y}^t \\implies (\\mathtt{Breeze}^t ⇔ B_{x,y})$   $R_7$ if an agent is in cell [x,y] at $t$ and perceives [Stench, Breeze, Glitter, Bump, Scream]=[Stench, None, None, None, None] then the cell is a Stench cell $L_{x,y}^t \\implies (\\mathtt{Stench}^t ⇔ S_{x,y})$    Similar to percepts, actions also are taken at specific times $t$. By convention, we first receive a percept at $t$, then take an action during the same time step $t$ and then the environment transitions to another state at $t+1$. To specify the transition model that describes how the world changes we use effect axioms as follows:\n   Sentence Description KB     $R_8$ if an agent is in cell [1,1] at $t=0$ and is $\\mathtt{FacingEast}$ and decides to move $\\mathtt{Forward}$ it will transition to $L_{2,1}$ in the next time step $L_{1,1}^0 \\land \\mathtt{FacingEast}^0 \\land \\mathtt{Forward}^0 \\implies (L_{2,1}^1 \\land \\neg L{1,1}^1)$    There lies a serious issue. There is a combinatorial explosion problem as we have to repeat sentences like the $R_8$ for each of the possible $T$ time steps into the future that include actions. For a single $\\mathtt{Forward}$ action we need to consider $4 x T x n^2$ possibilities, where 4 are the possible agent orientations and $n^2$ is the number cells in the wumpus world! But there are more bad news. We need to also include in the KB was remains unchanged - this is a well known issue called the representation frame problem. It took its name from the video frame where most of the pixels stay unchanged (background) and only a few change (foreground). Imagine the bandwidth that we need to consume to transmit all video frame pixels. This is analogous to the KB - imagine the combinatorial explosion if we have to specify sentences for everything that stays the same at every time instant. As an example we will have to specify that the wumpus is alive at $t+1$ as it was at $t$, that the agent continues to have one arrow when it didn\u0026rsquo;t use it and a myriad other intuitively obvious axioms.\n"});index.add({'id':45,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/classical-planning/','title':"Classical Planning",'content':"Classical Planning Planning Domain Definition Language (PDDL) In propositional logic-based planning we have seen the combinatorial explosion problem that results from the need to include in the reasoning / inference step all possible states and actions combinations over time. To avoid such explosion, with obvious benefits to the planning efficiency, we introduce a language called Planning Domain Definition Language (PDDL) that allows for compressive expressiveness at the action space via action schemas as we will see shortly. PDDL is positioned at the input of the domain independent planner as shown in the figure below.\nPlanning System: A domain-independent solver or planner takes two inputs: 1) the domain model written in a planning language and 2) a problem definition that describes the initial state $i$ and the desired goal state $g$ using the domain model’s terminology; and produces as output a plan, that is, a sequence of actions that takes the agent from the initial state to the goal state. From here\nPDDL expresses the four things we need to plan a sequence of actions. The set of all predicates and action schemas are defined in the domain file ($\\mathtt{domain.pddl}$) as shown next.\n   Domain Syntax Description     Types A description of the possible types of objects in the world. A type can inherit from another type.   Constants The set of constants, which are objects which appear in all problem instances of this domain. The second part of the domain description is a set of predicates. Each predicate is described by a name and a signature, consisting of an ordered list of types.   Predicates Properties of the objects (contained in the problem specification) we are interested in. Each property evaluates to TRUE or FALSE. The domain also describes a set of derived predicates, which are predicates associated with a logical expression. The value of each derived predicate is computed automatically by evaluating the logical expression associated with it.   Actions / Operators Actions are described by action schemas that effectively define the functions needed to do problem-solving search. The schema consist of the name, the signature or the list of all the boolean variables that are ground and functionless, a precondition and effect. The signature is now an ordered list of named parameters, each with a type.    The precondition is a logical formula, whose basic building blocks are the above mentioned predicates, combined using the standard first order logic logical connectives. The predicates can only be parametrized by the operator parameters, the domain constraints, or, if they appear within the scope of a forall or exists statement, by the variable introduced by the quantifier.    The effect is similar, except that it described a partial assignment, rather than a formula, and thus can not contain any disjunctions.    For the famous Blocks world shown below where a robotic arm must reason to stack blocks according what the goal is, we list the corresponding domain PDDL specification.\nBlocks planning problem\nIn natural language, the rules are:\n Blocks are picked up and put down by the arm Blocks can be picked up only if they are clear, i.e., without any block on top The arm can pick up a block only if the arm is empty, i.e., if it is not holding another block, i.e., the arm can be pick up only one block at a time The arm can put down blocks on blocks or on the table  In PDDL these rules are expressed as:\n PDDL Domain Spec. for Blocks-World ...  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73  ;; This is the 4-action blocks world domain which does not refer to a table object and distinguishes actions for moving blocks to-from blocks and moving blocks to-from table (define (domain blocksworld) (:requirements :typing :fluents :negative-preconditions) (:types block) ; we do not need a table type as we use a special ontable predicate (:predicates (on ?a ?b - block) (clear ?a - block) (holding ?a - block) (handempty) (ontable ?x - block) ) (:action pickup ; this action is only for picking from table :parameters (?x - block) :precondition (and (ontable ?x) (handempty) (clear ?x) ) :effect (and (holding ?x) (not (handempty)) (not (clear ?x)) (not (ontable ?x)) ) ) (:action unstack ; only suitable for picking from block :parameters (?x ?y - block) :precondition (and (on ?x ?y) (handempty) (clear ?x) ) :effect (and (holding ?x) (not (handempty)) (not (clear ?x)) (clear ?y) (not (on ?x ?y)) ) ) (:action putdown :parameters (?x - block) :precondition (and (holding ?x) ) :effect (and (ontable ?x) (not (holding ?x)) (handempty) (clear ?x) ) ) (:action stack :parameters (?x ?y - block) :precondition (and (holding ?x) (clear ?y) ) :effect (and (on ?x ?y) (not (holding ?x)) (handempty) (not (clear ?y)) (clear ?x) ) ) )       The objects, the initial state and the goal specifications are defined in the problem file ($\\mathtt{problem.pddl}$) as shown next.\n   Problem Syntax Description     Initial State Each state is represented as conjunction of ground boolean variables. For example, $$\\mathtt{On(Box_1, Table_2) \\land On(Box_2, Table_2)}$$ is a state expression. $\\mathtt{Box_1}$ is distinct than $\\mathtt{Box_2}$. All fluents that are not specified are assumed to be FALSE.   Goal All things we want to be TRUE. The goal is like a precondition - a conjunction of literals that may contain variables.     PDDL Problem Spec. for Blocks-World ...  1 2 3 4 5 6 7 8 9 10  (define (problem BLOCKS-11-0) (:domain BLOCKS) (:objects F A K H G E D I C J B ) (:INIT (CLEAR B) (CLEAR J) (CLEAR C) (ONTABLE I) (ONTABLE D) (ONTABLE E) (ON B G) (ON G H) (ON H K) (ON K A) (ON A F) (ON F I) (ON J D) (ON C E) (HANDEMPTY)) (:goal (AND (ON A J) (ON J D) (ON D B) (ON B H) (ON H K) (ON K I) (ON I F) (ON F E) (ON E G) (ON G C))) )       You need to experiment with this planning tool to make sure you understand what PDDL does and be familiar with the workflow of generating a plan. The blocks word problem is a classic problem employed in the International Planning Competition (IPC) 2000. For non-trivial examples where PDDL is used in e.g. robotics see ROSPlan.\nNow we need to look how the planners solve the PDDL expressed problems. As it turns out many use the Forward (state-space) Search algorithms where despite the factored representation assumed here, we can treat states and actions as atomic and use forward search algorithms with heuristics such as A*.\n"});index.add({'id':46,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/linear-temporal-logic/','title':"Linear Temporal Logic",'content':""});index.add({'id':47,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/search/','title':"Planning with Search",'content':"Planning with Search In classical planning we have seen that symbolic representations enriched with propositional logic in the Knowledge Base allow us to reason and come up with a sequence of actions that the agent needs to execute to reach the goal. This section provides the background that connects the domain-independent planner that solves the planning problem to search. To do so, it is instructive to look initially to problems that are simpler in terms of state and action representation. We effectively zoom out from the factored representation of the environment\u0026rsquo;s state and treat it as atomic i.e. not broken down into its individual variables.\nAtomic state representations of an environment are adequate for a a variety of global planning tasks: one striking use case is path planning There, the scene or environment takes the form of a global map and the goal is to move the embodied agent from a starting state to a goal state. If we assume that the global map takes the form of a grid with a suitable resolution, each grid tile (or cell) represents a different atomic state than any other cell. Similar considerations can be made for other forms of the map e.g. a graph form.\nGiven such state representation, we will use search to find the action sequence that the agent must produce to reach a goal state. Note that in most cases, we are dealing with informed rather than blind search, where we are also given task-specific knowledge (that we use to develop suitable heuristics) to find the solution as we will see shortly.\nForward-Search We will develop the algorithm for the task at hand which is to find the path between a starting state and the goal state in a map. Not just any path but the minimum cost path when the state transition graph is revealed incrementally through a series of actions and associated individual costs (cost function). The task is depicted below.\nA map of a parking lot as determined via postprocessing LIDAR returns. Obstacles colored in yellow are tall obstacles, brown obstacles are curbs, and green obstacles are tree branches that are of no relevance to ground navigation.\nIn practice, maps likes the one above are local both in terms of space (each snapshot is relative to the location of the agent) as well as in terms of time (at the time the agent took these measurements). We can take any map like the one above and form its discrete equivalent such as shown below. We usually call this type metric map and for the purposes of this chapter this is our search area and in it lie all possible feasible solutions, each is a sequence of actions distinguished by path cost. In this example, the least cost path is actually the geographically longer path - the line thickness for each of the two possible solutions in this figure is proportional to its cost.\nAn example search area with action-cost function where left turns are penalized compared to right.This is common penalization in path planning for delivery vehicles.\nThe alternative map representation is topological where the environment is represented as a graph where nodes indicated significant grounded features and edges denote topological relationships (position, orientation, proximity etc.). The sometimes confusing part is that irrespectively of metric or topological representations, the forward search methods we look in this chapter all function on graphs given an initial state $s_I$ and the goal state $s_G$ that is reached after potentially a finite number of actions (if a solution exist). The following pseudo-code is from Steven LaValle\u0026rsquo;s book - Chapter 2\n\\begin{algorithm} \\caption{Forward Search} \\begin{algorithmic} \\INPUT $s_I$, $s_G$ \\STATE Q.Insert($s_I$), mark $s_I$ as explored. \\WHILE{Q not empty} \\STATE $s \\leftarrow Q.GetFirst()$ \\IF{$s \\in S_G$} \\RETURN SUCCESS \\ENDIF \\FOR{$a \\in A(s)$} \\STATE $s' \\leftarrow f(s,a)$ \\IF{$s'$ not visited} \\STATE mark $s'$ as explored \\STATE Q.Insert($s'$) \\ELSE \\STATE Resolve duplicate $s'$ \\ENDIF \\ENDFOR \\ENDWHILE \\end{algorithmic} \\end{algorithm}  The forward search uses two data structures, a priority queue (Q) and a list and proceeds as follows:\n Provided that the starting state is not the goal state, we add it to a priority queue called the frontier (also known as open list in the literature but we avoid using this term as its implemented is a queue). The name frontier is synonymous to unexplored. We expand each state in our frontier, by applying the finite set of actions, generating a new list of states. We use a list that we call the explored set or closed list to remember each node (state) that we expanded. This is the list data structured we mentioned. We then go over each newly generated state and before adding it to the frontier we check whether it has been expanded before and in that case we discard it.  Forward-search approaches The only significant difference between various search algorithms is the specific priority function that implements line 3: $s \\leftarrow Q.GetFirst()$ in other words retrieves a state held in the priority queue for expansion.\n   Search Queue Policy Details     Depth-first search (DFS) LIFO Search frontier is driven by aggressive exploration of the transition model. The algorithm makes deep incursions into the graph and retreats only when it run out of nodes to visit. It does not result in finding the shortest path to the goal.   Breath-first search FIFO Search frontier is expanding uniformly like the propagation of waves when you drop a stone in water. It therefore visit vertices in increasing order of their distance from the starting point.   Dijkstra Cost-to-Come or Past-Cost    A-star Cost-to-Go or Future-Cost     Depth-first search (DFS) In undirected graphs, depth-first search answers the question: What parts of the graph are reachable from a given vertex. It also finds explicit paths to these vertices, summarized in its search tree as shown below.\nDepth-first can\u0026rsquo;t find optimal (shortest) paths. Vertex C is reachable from S by traversing just one edge, while the DFS tree shows a path of length 3. On the right the DFS search tree is shown assuming alphabetical order in breaking up ties. Can you explain the DFS search tree?\nDFS can be run verbatim on directed graphs, taking care to traverse edges only in their prescribed directions.\nBreadth-first search (BFS) In BFS the lifting of the starting state $s$, partitions the graph into layers: $s$ itself (vertex at distance 0), the vertices at distance 1 from it, the vertices at distance 2 from it etc.\nBFS expansion in terms of layers of vertices - each layer at increasing distance from the layer that follows.\nQueue contents during BFS and the BFS search tree assuming alphabetical order. Can you explain the BFS search tree? Is the BFS search tree a shortest-path tree?\nDijkstra\u0026rsquo;s Algorithm Breadth-first search finds shortest paths in any graph whose edges have unit length. Can we adapt it to a more general graph G = (V, E) whose edge lengths $l(e)$ are positive integers? These lengths effectively represent the cost of traversing the edge. fHere is a simple trick for converting G into something BFS can handle: break G’s long edges into unit-length pieces, by introducing “dummy” nodes as shown next.\nTo construct the new graph $G'$ for any edge $e = (s, s^\\prime)$ of $E$, replace it by $l(e)$ edges of length 1, by adding $l(e) − 1$ dummy nodes between nodes $s$ and $s^\\prime$.\nWith the shown transformation, we can now run BFS on $G'$ and the search tree will reveal the shortest path of each goal node from the starting point.\nThe transformation allows us to solve the problem but it did result in an inefficient search where most of the nodes involved are searched but definitely will never be goal nodes. To look for more efficient ways to absorb the edge length $l(e)$ we use the following of cost.\nCost-to-come($s$) or PastCost($s$) vs. Cost-to-Go($s$) or FutureCost($s$). PastCost($s$) is the minimum cost from the start state $s_I$ to state $s$. FutureCost($s$) is the minimum cost from the state $s$ to the goal state $s_G$. The PastCost is used as the prioritization metric of the queue in Dijkstra\u0026rsquo;s algorithm. The addition of the PastCost with an estimate of the FutureCost, the heuristic $h(s)$, i.e. $G(s) =$ PastCost($s$)+$h(s)$, is used as the corresponding metric in the A* algorithm. What would be the ideal metric?\nThe following example is instructive of the execution steps of the algorithm.\nExample of Dijkstra\u0026rsquo;s algorithm execution steps and with $s_I=A$\nThe exact same pseudo-code is executed as before but the priority metric $C(s^\\prime)=C^*(s) + l(e)$ now accounts for costs as they are calculated causing the queue to be reordered accordingly. Here, $C(s^\\prime)$ represents the best cost-to-come that is known so far, but we do not write $ C^*$ because it is not yet known whether $ s^\\prime$ was reached optimally. Due to this, some work is required in line 12: $\\texttt{Resolve duplicate}$ $s^\\prime$. If $s^\\prime$ already exists in $ {Q}$, then it is possible that the newly discovered path to $s^\\prime$ is more efficient. If so, then the cost-to-come value $ {C}(s^\\prime)$ must be lowered for $s^\\prime$, and $ {Q}$ must be reordered accordingly. When does $ {C}(s)$ finally become $ C^*(x)$ for some state $s$? Once $s$ is removed from $ {Q}$ using $ {Q}.GetFirst()$, the state becomes dead, and it is known that $ x$ cannot be reached with a lower cost.\nUsing the demo link below, we can construct a wall-world where we have almost enclosed the starting state. We will comment on this result after treating the A* algorithm.\nDijkstra\u0026rsquo;s algorithm demo\nA* Algorithm Dijkstra\u0026rsquo;s algorithm is very much related to the Uniform Cost Search algorithm and in fact logically they are equivalent as the algorithm explores uniformly all nodes that have the same PastCost. In the Astar algorithm, we start using the fact that we know the end state and therefore attempt to find methods that bias the exploration towards it.\nAs mentioned in the cost definition figure above, A* uses both $C^*(s)$ and an estimate of the optimal Cost-to-go or FutureCost $G^*(s)$ because obviously to know exactly $G^*(s)$ is equivalent to solving the original search problem. Therefore the metric for prioritizing the Q queue is:\n$$C^*(s) + h(s)$$\nIf $h(s)$ is an underestimate of the $G^*(s)$ the Astar algorithm is guaranteed to fine optimal plans.\nFor an example of a heuristic consider this problem:\nA simple example showcasing a modified to what is described above priority metric. What we use is a modification of the edge cost $l\u0026rsquo;(e)=l(e) + [h(s^\\prime)-h(s)]$. Is there a difference?\nIn this example all $l(e)=1$ and the heuristic is a penalty from how much a transition to another node (an action) takes us away from the end state (adopted from CS221).\nUsing the interactive demo page below, repeating the same example wall-world, we can clearly see the substantial difference in search speed and the beamforming effect as soon as the wave (frontier) evaluates nodes where the heuristic (the Manhattan distance from the goal node) becomes dominant. Notice the difference with the UCS / Dijkstra\u0026rsquo;s algorithm in the number of nodes that needed to be evaluated.\n$A^$ algorithm demo\nForward Search Algorithm Implementation Interactive Demo This demo is instructive of the various search algorithms we covered here. You can introduce using your mouse obstacles in the canvas and see how the various search methods behave.\n A* Implementation A stand-alone A* planner in python is shown next. Its instructive to go through the code to understand how it works.\n A* Planner ...  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264  import math import matplotlib.pyplot as plt show_animation = True class AStarPlanner: def __init__(self, ox, oy, reso, rr): \u0026#34;\u0026#34;\u0026#34;Initialize grid map for a star planningox: x position list of Obstacles [m]oy: y position list of Obstacles [m]reso: grid resolution [m]rr: robot radius[m]\u0026#34;\u0026#34;\u0026#34; self.reso = reso self.rr = rr self.calc_obstacle_map(ox, oy) self.motion = self.get_motion_model() class Node: def __init__(self, x, y, cost, pind): self.x = x # index of grid self.y = y # index of grid self.cost = cost self.pind = pind def __str__(self): return str(self.x) + \u0026#34;,\u0026#34; + str(self.y) + \u0026#34;,\u0026#34; + str(self.cost) + \u0026#34;,\u0026#34; + str(self.pind) def planning(self, sx, sy, gx, gy): \u0026#34;\u0026#34;\u0026#34;A star path searchinput:sx: start x position [m]sy: start y position [m]gx: goal x position [m]gx: goal x position [m]output:rx: x position list of the final pathry: y position list of the final path\u0026#34;\u0026#34;\u0026#34; nstart = self.Node(self.calc_xyindex(sx, self.minx), self.calc_xyindex(sy, self.miny), 0.0, -1) ngoal = self.Node(self.calc_xyindex(gx, self.minx), self.calc_xyindex(gy, self.miny), 0.0, -1) open_set, closed_set = dict(), dict() # populate the frontier (open set) with the starting node open_set[self.calc_grid_index(nstart)] = nstart while 1: if len(open_set) == 0: print(\u0026#34;Open set is empty..\u0026#34;) break c_id = min( open_set, key=lambda o: open_set[o].cost + self.calc_heuristic(ngoal, open_set[o])) current = open_set[c_id] # show graph if show_animation: # pragma: no cover plt.plot(self.calc_grid_position(current.x, self.minx), self.calc_grid_position(current.y, self.miny), \u0026#34;xc\u0026#34;) # for stopping simulation with the esc key. plt.gcf().canvas.mpl_connect(\u0026#39;key_release_event\u0026#39;, lambda event: [exit(0) if event.key == \u0026#39;escape\u0026#39; else None]) if len(closed_set.keys()) % 10 == 0: plt.pause(0.001) if current.x == ngoal.x and current.y == ngoal.y: print(\u0026#34;Find goal\u0026#34;) ngoal.pind = current.pind ngoal.cost = current.cost break # Remove the item from the open set del open_set[c_id] # Add it to the closed set closed_set[c_id] = current # expand_grid search grid based on motion model for i, _ in enumerate(self.motion): node = self.Node(current.x + self.motion[i][0], current.y + self.motion[i][1], current.cost + self.motion[i][2], c_id) n_id = self.calc_grid_index(node) # If the node is not safe, do nothing if not self.verify_node(node): continue if n_id in closed_set: continue if n_id not in open_set: open_set[n_id] = node # discovered a new node else: if open_set[n_id].cost \u0026gt; node.cost: # This path is the best until now. record it open_set[n_id] = node rx, ry = self.calc_final_path(ngoal, closed_set) return rx, ry def calc_final_path(self, ngoal, closedset): # generate final course rx, ry = [self.calc_grid_position(ngoal.x, self.minx)], [ self.calc_grid_position(ngoal.y, self.miny)] pind = ngoal.pind while pind != -1: n = closedset[pind] rx.append(self.calc_grid_position(n.x, self.minx)) ry.append(self.calc_grid_position(n.y, self.miny)) pind = n.pind return rx, ry @staticmethod def calc_heuristic(n1, n2): w = 1.0 # weight of heuristic d = w * math.hypot(n1.x - n2.x, n1.y - n2.y) return d def calc_grid_position(self, index, minp): \u0026#34;\u0026#34;\u0026#34;calc grid position:param index::param minp::return:\u0026#34;\u0026#34;\u0026#34; pos = index * self.reso + minp return pos def calc_xyindex(self, position, min_pos): return round((position - min_pos) / self.reso) def calc_grid_index(self, node): return (node.y - self.miny) * self.xwidth + (node.x - self.minx) def verify_node(self, node): px = self.calc_grid_position(node.x, self.minx) py = self.calc_grid_position(node.y, self.miny) if px \u0026lt; self.minx: return False elif py \u0026lt; self.miny: return False elif px \u0026gt;= self.maxx: return False elif py \u0026gt;= self.maxy: return False # collision check if self.obmap[node.x][node.y]: return False return True def calc_obstacle_map(self, ox, oy): # map limits self.minx = round(min(ox)) self.miny = round(min(oy)) self.maxx = round(max(ox)) self.maxy = round(max(oy)) print(\u0026#34;minx:\u0026#34;, self.minx) print(\u0026#34;miny:\u0026#34;, self.miny) print(\u0026#34;maxx:\u0026#34;, self.maxx) print(\u0026#34;maxy:\u0026#34;, self.maxy) self.xwidth = round((self.maxx - self.minx) / self.reso) self.ywidth = round((self.maxy - self.miny) / self.reso) print(\u0026#34;xwidth:\u0026#34;, self.xwidth) print(\u0026#34;ywidth:\u0026#34;, self.ywidth) # obstacle map generation self.obmap = [[False for i in range(self.ywidth)] for i in range(self.xwidth)] for ix in range(self.xwidth): x = self.calc_grid_position(ix, self.minx) for iy in range(self.ywidth): y = self.calc_grid_position(iy, self.miny) for iox, ioy in zip(ox, oy): d = math.hypot(iox - x, ioy - y) if d \u0026lt;= self.rr: self.obmap[ix][iy] = True break @staticmethod def get_motion_model(): # dx, dy, cost motion = [[1, 0, 1], [0, 1, 1], [-1, 0, 1], [0, -1, 1], [-1, -1, math.sqrt(2)], [-1, 1, math.sqrt(2)], [1, -1, math.sqrt(2)], [1, 1, math.sqrt(2)]] return motion def main(): print(__file__ + \u0026#34;start!!\u0026#34;) # start and goal position sx = 10.0 # [m] sy = 10.0 # [m] gx = 50.0 # [m] gy = 50.0 # [m] grid_size = 2.0 # [m] robot_radius = 1.0 # [m] # set obstable positions ox, oy = [], [] for i in range(-10, 60): ox.append(i) oy.append(-10.0) for i in range(-10, 60): ox.append(60.0) oy.append(i) for i in range(-10, 61): ox.append(i) oy.append(60.0) for i in range(-10, 61): ox.append(-10.0) oy.append(i) for i in range(-10, 40): ox.append(20.0) oy.append(i) for i in range(0, 40): ox.append(40.0) oy.append(60.0 - i) if show_animation: # pragma: no cover plt.plot(ox, oy, \u0026#34;.k\u0026#34;) plt.plot(sx, sy, \u0026#34;og\u0026#34;) plt.plot(gx, gy, \u0026#34;xb\u0026#34;) plt.grid(True) plt.axis(\u0026#34;equal\u0026#34;) a_star = AStarPlanner(ox, oy, grid_size, robot_radius) rx, ry = a_star.planning(sx, sy, gx, gy) if show_animation: # pragma: no cover plt.plot(rx, ry, \u0026#34;-r\u0026#34;) plt.show() if __name__ == \u0026#39;__main__\u0026#39;: main()       Executing the code above results in the animation:\nAnimation of the A* algorithm - from here\nThis is excellent overview on how the principles of shortest path algorithms are applied in everyday applications such as Google maps directions. Practical implementation considerations are discussed for multi-modal route finding capabilities where the agent needs to find optimal routes while traversing multiple modes of transportation.\nAlthough the treatment above is self-contained, if you are missing some algorithmic background, afraid not. There is a free and excellent book to help you with the background behind this chapter. In that book Chapters 3 and 4 are the relevant ones.  "});index.add({'id':48,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/planning-application/trajectory/','title':"Trajectory Generation",'content':""});index.add({'id':49,'href':'/cs-gy-6613-spring-2020/docs/lectures/planning/planning-application/','title':"Autonomous Agent Planning Application",'content':""});index.add({'id':50,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/csp/','title':"Constraint Satisfaction Programming",'content':""});index.add({'id':51,'href':'/cs-gy-6613-spring-2020/docs/lectures/mdp/','title':"Lecture 8 - Markov Decision Processes",'content':"Markov Decision Processes We started looking at different agent behavior architectures starting from the planning agents where the model of the environment is known and with no interaction with it the agent improves its policy, using this model as well as problem solving and logical reasoning skills.\nWe now look at agents that can plan by interacting with the environment still knowing the model (model-based) of the environment such as its dynamics and rewards. The planning problem as will see, it will be described via a set of four equations called Bellman expectation and Bellman optimality equations that connect the values (utility) with each state or action with the policy. These equations can be solved by Dynamic Programming algorithms to produce the optimal policy (strategy) that the agent must adopt.\nComputationally we will go through approaches that solve the MDP as efficiently as possible - namely, the value and policy iteration algorithms.\nSolving MDP Problems\n Apart from the notes here that are largely based on David Silver\u0026rsquo;s (Deep Mind) course material and video lectures, the curious mind will be find additional resources:\n in the Richard Sutton\u0026rsquo;s book - David Silver\u0026rsquo;s slides and video lectures are based on this book. The code in Python of the book is here in the suggested book written by Google researchers as well as on OpenAI\u0026rsquo;s website. The chapters we covered is 1-4. You may also want to watch Andrew Ng\u0026rsquo;s, 2018 version of his ML class that includes MDP and RL lectures.    Many of the algorithms presented here like policy and value iteration have been developed in this git repo that you should download and run while reading the notes.\n "});index.add({'id':52,'href':'/cs-gy-6613-spring-2020/docs/lectures/mdp/mdp-intro/','title':"Introduction to MDP",'content':"Introduction to MDP The MDP Agent - Environment Interface We start by reviewing the agent-environment interface with this evolved notation and provide additional definitions that will help in grasping the concepts behind DRL. We treat MDP analytically effectively deriving the four Bellman equations\nAgent-Environment Interface\nThe following table summarizes the notation and contains useful definitions that we will use to describe required concepts later.\n   Symbol Description     $A_t$ agent action at time step $t$, $a \\in \\mathcal{A}$ the finite set of actions   $S_t$ environment state at time step $t$, $s \\in \\mathcal{S}$ the finite set of states   $R_{t+1}$ reward sent by the environment after taking action $A_t$   $t$ time step index associated with each tuple ($S_t, A_t, R_{t+1}$) called the *experience*.   $T$ maximum time step beyond which the interaction terminates   episode the time horizon from $t=0$ to $T-1$   $\\tau$ trajectory - the sequence of experiences over an episode   $G_t$ return - the total discounted rewards from time step $t$ - it will be qualified shortly.   $\\gamma$ the discount factor $\\gamma \\in [0,1]$    As the figure above indicates, the agent perceives fully the environment state $S_t$ (fully observed) via a bank of sensors. In other words the agent knows which state the environment is perfectly.\nThe agent function, called policy $\\pi$, produces an action either deterministically $a=\\pi(s)$ or stochastically where the function produces a probability of actions conditioned in the current state:\n$$\\pi(a|s)=p(A_t=a|S_t=s)$$\nThe policy is assumed to be stationary i.e. not change with time step $t$ and it will depend only on the state $S_t$ i.e. $A_t \\sim \\pi(.|S_t), \\forall t \u0026gt; 0$\nThis will have two effects:\nThe first is that the action itself will change the environment state to some other state. This can be represented via the environment state transition probabilistic model that generically can be written as:\n$$s_{t+1} = p( s_{t+1} | (s_0, a_0), \u0026hellip;, (s_t, a_t) )$$\nUnder the assumption that the next state only depends on the current state and action\n$$s_{t+1} = p( s_{t+1} | (s_0, a_0), \u0026hellip;, (s_t, a_t) ) = p( s_{t+1} | s_t, a_t)$$\nwe define a Markov Decision Process as the 5-tuple $\\mathcal M = \u0026lt;\\mathcal S, \\mathcal P, \\mathcal R, \\mathcal A, \\gamma\u0026gt;$ that produces a sequence of experiences $(S_1, A_1, R_2), (S_2, A_2, R_3), \u0026hellip;$. Together with the policy $\\pi$, the state transition probability matrix $\\mathcal P$ is defined as\n$$\\mathcal P^a_{ss^\\prime} = p[S_{t+1}=s^\\prime | S_t=s, A_t=a ]$$\nwhere $s^\\prime$ simply translates in English to the successor state whatever the new state is.\nCan you determine the state transition matrix for the 4x3 Gridworld in MDP slides? What each row of this matrix represents?  Note that Markov processes are sometimes erroneously called memoryless but in any MDP above we can incorporate memory aka dependence in more than one state over time by cleverly defining the state $S_t$ as a container of a number of states. For example, $S_t = \\left[ S_t=s, S_{t-1} = s^\\prime \\right]$ can still define an Markov transition using $S$ states. The transition model $p(S_t | S_{t-1}) = p(s_t, s_{t-1} | s_{t-1}, s_{t-2}) = p(s_t|s_{t-1}, s_{t-2})$ is called the 2nd order Markov chain.\nThe second effect from the action, is that it will cause the environment to send the agent a signal called (instantaneous reward $R_{t+1}$. Please note that in the literature the reward is also denoted as $R_{t}$ - this is a convention issue rather than something fundamental. The justification of the index $t+1$ is that the environment will take one step to respond to what it receives from the agent.\nThe reward function tells us if we are in state $s$, what reward $R_{t+1}$ in expectation we get when taking an action $a$. It is given by,\n$$\\mathcal{R}^a_s = \\mathop{\\mathbb{E}}[ R_{t+1} | S_t=s, A_t=a]$$\nand it will be used later on during the development of value iteration.\nEach reward and state transition will trigger a new iteration and the interaction will terminate at some point either because the environment terminated after reaching a maximum time step ($T$) or reaching the goal.\nValue Functions To define the value function formally, consider first the return defined as the total discounted reward at time step $t$.\n$$G_t = R_{t+1} + \\gamma R_{t+2} + \u0026hellip; = \\sum_{k=0}^∞\\gamma^k R_{t+1+k}$$\nNotice the two indices needed for its definition - one is the time step $t$ that manifests where we are in the trajectory and the second index $k$ is used to index future rewards up to infinity - this is the case of infinite horizon problems. If the discount factor $\\gamma \u0026lt; 1$ and there the rewards are bounded to $|R| \u0026lt; R_{max}$ then the above sum is _finite_.\n$$ \\sum_{k=0}^∞\\gamma^k R_{t+1+k} \u0026lt; \\sum_{k=0}^∞\\gamma^k R_{max} = \\frac{R_{max}}{1-\\gamma}$$\nThe return is itself a random variable - for each trajectory defined by sampling the policy (strategy) of the agent we get a different return. For the Gridworld of the MDP slides:\n$$\\tau_1: S_0=s_{11}, S_1 = s_{12}, \u0026hellip; S_T=s_{43} \\rightarrow G^{\\tau_1}_0 = 5.6$$ $$\\tau_2: S_0=s_{11}, S_1=s_{21}, \u0026hellip; , S_T=s_{43} \\rightarrow G^{\\tau_2}_0 = 6.9$$ $$ … $$\nPlease note that the actual values are different - these are sample numbers to make the point that the return depends on the specific trajectory.\nThe state-value function $v_\\pi(s)$ provides a notion of the long-term value of state $s$. It is equivalent to the _utility_ we have seen in the MDP slides. It is defined as the _expected_ return starting at state $s$ and following policy $\\pi(a|s)$,\n$$v_\\pi(s) = \\mathop{\\mathbb{E}_\\pi}(G_t | S_t=s)$$\nThe expectation is obviously due to the fact that $G_t$ are random variables since the sequence of states of each trajectory is dictated by the stochastic policy. As an example, assuming that there are just two trajectories whose returns were calculated above, the value function of state $s_{11}$ will be\n$$v_\\pi(s_{11}) = \\frac{1}{2}(G^{\\tau_1}_0 + G^{\\tau_2}_0)$$\nOne corner case is interesting - if we make $\\gamma=0$ then $v_\\pi(s)$ becomes the average of instantaneous rewards we can get from that state.\nWe also define the action-value function $q_\\pi(s,a)$ as the expected return starting from the state $s$, taking action $a$ and following policy $\\pi(a|s)$.\n$$q_\\pi(s,a) = \\mathop{\\mathbb{E}_\\pi} (G_t | S_t=s, A_t=a)$$\nThis is an important quantity as it helps us decide the action we need to take while in state $s$.\nComputing the value functions given a policy In this section we describe how to calculate the value functions. As you can imagine this means replacing the expectations with summations over quantities such as states and actions while, at the same time, making the required computations as efficient as possible.\nLets start with the state-value function that can be written as,\n$$v(s) = \\mathop{\\mathbb{E}} \\left[G_t | S_t=s\\right] = \\mathop{\\mathbb{E}} \\left[ \\sum_{k=0}^∞\\gamma^k R_{t+1+k} | S_t=s\\right]$$ $$ = \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+ \u0026hellip; | S_t=s \\right]$$ $$ = \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3}+ \u0026hellip;) | S_t=s \\right]$$ $$ = \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma v(S_{t+1}=s^\\prime) | S_t=s \\right]$$\nNOTE: All above expectations are with respect to policy $\\pi$.\nThis is perhaps one of the most important recursions in control theory - it is known as the Bellman expectation equation repeated below:\n$$v_\\pi(s) = \\mathop{\\mathbb{E}_\\pi} \\left[ R_{t+1} + \\gamma ~ v_\\pi(S_{t+1}=s^\\prime) | S_t=s \\right]$$\nThe parts of the value function above are (1) the immediate reward, (2) the discounted value of the successor state $\\gamma v(S_{t+1}=s^\\prime)$. Similarly to the state-value function we can decompose the action-value function as,\n$$q_\\pi(s,a) = \\mathop{\\mathbb{E}_\\pi} \\left[ R_{t+1} + \\gamma ~ q_\\pi(S_{t+1}=s^\\prime, A_{t+1}) | S_t=s, A_t=a \\right]$$\nWe now face the problem that we need to compute these two value functions and we start by considering what is happening at each time step. At each time step while in state $S_t=s$ we have a number of actions we can choose, the probabilities of which depend on the policy $\\pi(a|s)$. What value we can reap from each action is given to us by $q_\\pi(s,a)$. This is depicted below.\nActions can be taken from that state $s$ according to the policy $\\pi$. Actions are represented in this simple tree with action nodes (solid circles) while state nodes are represented by empty circles.\nTranslating what we have just described in equation form, allows us to write the state-value equation as,\n$$v(s) = \\sum_{a \\in \\mathcal A} \\pi(a|s) q_\\pi(s,a)$$\nThis sum is easily understood if you move backwards from the action nodes of the tree to the state node. Each edge weighs with $\\pi(a|s)$ the corresponding action-value. This backwards calculation is referred to a a backup. We can now reason fairly similarly about the action-value function that can be written by taking the expectation,\n$$q_\\pi(s,a) = \\mathop{\\mathbb{E}_\\pi} \\left[ R_{t+1} | S_t=s, A_t= a \\right] + \\gamma ~ \\mathop{\\mathbb{E}_\\pi} \\left[ v_\\pi(S_{t+1}=s^\\prime) | S_t=s, A_t= a \\right]$$\nThe first expectation is the reward function $\\mathcal{R}^a_s$ by definition. The second expectation can be written in matrix form by considering that at each time step if we are to take an action $A_t=a$, the environment can transition to a number of successor states $S_{t+1}=s'$ and signal a reward $R_{t+1}$ as shown in the next figure.\n*Successor states that can be reached from state $s$ if the agent selects action $a$. $R_{t+1}=r$ we denote the instantaneous reward for each of the possibilities.*\nIf you recall the agent in the Gridworld, has 80% probability to achieve its intention and make the environment to change the desired state and 20% to make the environment change to not desired states justifying the multiplicity of states given an action in the figure above.\nWhat successor states we will transition to depends on the transition model $P^a_{ss^\\prime} = p[S_{t+1}=s^\\prime | S_t=s, A_t=a ]$ . What value we can reap from each successor state is given by $v_\\pi(s^\\prime)$. The expectation can then be evaluated as a summation over all possible states $\\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v(s^\\prime)$. In conclusion, the action-value function can be written as\n$$q_\\pi(s,a) = \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_\\pi(s^\\prime)$$\nSubstituting the $v_\\pi(s^\\prime)$ is represented by the following tree that considers the action-value function over a look ahead step.\nTree that represents the action-value function after a one-step look ahead.\n$$q_\\pi(s,a) = \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} \\sum_{a^\\prime \\in \\mathcal A} \\pi(a^\\prime|s^\\prime) q_\\pi(s^\\prime,a^\\prime)$$  Now that we have a computable $q_\\pi(s,a)$ value function we can go back and substitute it into the equation of the state-value function. Again we can representing this substitution by the tree structure below.\nTree that represents the state-value function after a one-step look ahead.\nWith the substitution we can write the state-value function as,\n$$v_\\pi(s) = \\sum_{a \\in \\mathcal A} \\pi(a|s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_\\pi(s^\\prime) \\right)$$  As we will see in a separate chapter, this equation is going to be used to iteratively calculate the converged value function of each state given an MDP and a policy. The equation is referred to as the Bellman expectation backup - it took its name from the previously shown tree like structure where we use state value functions from the leaf modes $s^\\prime$ to the root node.\nSolving the MDP Now that we can calculate the value functions efficiently via the Bellman expectation recursions, we can now solve the MDP which requires maximize either of the two functions over all possible policies. The optimal state-value function and optimal action-value function are given by definition,\n$$v_*(s) = \\max_\\pi v_\\pi(s)$$ $$q_*(s,a) = \\max_\\pi q_\\pi(s,a)$$\nIf we can calculate $q_*(s,a)$ we have found the best possible action in each state of the environment. In other words we can now obtain the _optimal policy_ by maximizing over $q_*(s,a)$ - mathematically this can be expressed as,\n$$\\pi_*(a|s) = \\begin{cases}1 \u0026amp; \\text{if }\\ a = \\argmax_{a \\in \\mathcal A} q_*(s,a), \\\\ 0 \u0026amp; \\text{otherwise}\\end{cases}$$\nSo the problem now becomes how to calculate the optimal value functions. We return to the tree structures that helped us understand the interdependencies between the two and this time we look at the optimal equivalents.\n*Actions can be taken from that state $s$ according to the policy $\\pi_*$*\nFollowing similar reasoning as in the Bellman expectation equation where we calculated the value of state $s$ as an average of the values that can be claimed by taking all possible actions, now we simply replace the average with the max.\n$$v_*(s) = \\max_a q_*(s,a)$$\n*Successor states that can be reached from state $s$ if the agent selects action $a$. $R_{t+1}=r$ we denote the instantaneous reward for each of the possibilities.*\nSimilarly, we can express $q_*(s,a)$ as a function of $v_*(s)$ by looking at the corresponding tree above.\n$$q_*(s,a) = \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_*(s^\\prime)$$\nNotice that there is no $\\max$ is this expression as we have no control on the successor state - that is something the environment controls. So all we can do is average.\nNow we can similarly attempt to create a recursion that will lead to the Bellman optimality equations that effectively solve the MDP, by expanding the trees above.\nTree that represents the optimal state-value function after a two-step look ahead.\nTree that represents the optimal action-value function after a two-step look ahead.\n$$v_*(s) = \\max_a \\left( \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_*(s^\\prime) \\right)$$\n$$q_*(s,a) = \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} \\max_{a^\\prime} q_*(s^\\prime,a^\\prime)$$\n These equations due to the $\\max$ operator are non-linear and can be solved to obtain the MDP solution aka $q_*(s,a)$ iteratively via a number of methods: policy iteration, value iteration, Q-learning, SARSA. We will see some of these methods in detail in later chapters. The key advantage in the Bellman optimality equations is efficiency:\n They recursively decompose the problem into two sub-problems: the subproblem of the next step and the optimal value function in all subsequent steps of the trajectory. They cache the optimal value functions to the sub-problems and by doing so we can reuse them as needed.  "});index.add({'id':53,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/pomdp/','title':"Partially Observed MDP",'content':""});index.add({'id':54,'href':'/cs-gy-6613-spring-2020/docs/lectures/drl/prediction/','title':"Model-free Prediction",'content':"Model-free Prediction In this chapter we find optimal policy solutions when the MDP is unknown and we need to learn its underlying value functions - also known as the model free prediction problem. The main idea here is to learn value functions via sampling. These methods are in fact also applicable when the MDP is known but its models are simply too large to use the approaches outlined in the MDP chapter. The two sampling approaches we will cover here are (incremental) Monte-Carlo (MC) and Temporal Difference (TD).\nMonte-Carlo (MC) Learning The the MC learning approach for every state at time $t$ we sample one complete trajectory as shown below.\nBackup tree with value iteration based on the MC approach. MC samples a complete trajectory to the goal node T shown with red.\nThere is some rationale of doing so, if we recall that the state-value function that was defined in the introductory MDP section i.e. the expected return.\n$$v_\\pi(s) = \\mathop{\\mathbb{E}_\\pi}(G_t | S_t=s)$$\ncan be approximated by using the sample mean return over a sample episode / trajectory:\n$$G_t(\\tau) = \\sum_{k=0}^{T-1}\\gamma^k R_{t+1+k}$$\nThe value function is therefore approximated in Monte-Carlo, by the sample mean of the returns over multiple episodes / trajectories. In other words, to update each element of the state value function\n For each time step $t$ that state $S_t$ is visited in an episode  Increment a counter $N(S_t)$ of visitations Calculate the total return $S(S_t) = S(S_t) + G_t$   At the end of multiple episodes, the value is estimated as $V(S_t) = S(S_t) / N(S_t)$  As $N(S_t) \\rightarrow ∞$ the estimate will converge to $V(S_t) \\rightarrow v_\\pi(s)$. **Notice that we started using capital letters for the _estimates_ of the value functions.**\nBut we can also do the following trick, called incremental mean approximation:\n$$ \\mu_k = \\frac{1}{k} \\sum_{j=1}^k x_j = \\frac{1}{k} \\left( x_k + \\sum_{j=1}^{k-1} x_j \\right)$$ $$ = \\frac{1}{k} \\left(x_k + (k-1) \\mu_{k-1}) \\right) = \\mu_{k-1} + \\frac{1}{k} ( x_k - \\mu_{k-1} )$$\nUsing the incremental sample mean, we can approximate the value function after each episode if for each state $S_t$ with return $G_t$, $$ N(S_t) = N(S_t) +1 $$ $$ V(S_t) = V(S_t) + \\alpha \\left( G_t - V(S_t) \\right)$$\nwhere $\\alpha = \\frac{1}{N(S_t)}$ can be interpreted as the forgetting factor.\n $\\alpha$ can also be any number $\u0026lt; 1$ to get into a more flexible sample mean - the running mean that will increase the robustness of this approach in non-stationary environments.\nTemporal Difference (TD) Approximations Backup tree for value iteration with the TD approach. TD samples a single step ahead as shown with red.\nInstead of getting an estimated value function at the end of multiple episodes, we can use the incremental mean approximation to update the value function after each step.\nGoing back to the example of crossing the room optimally, we take one step towards the goal and the we bootstrap the value function of the state we were in from an estimated return for the remaining trajectory. We repeat this as we go along effectively adjusting the value estimate of the starting state from the true returns we have experienced up to now, gradually grounding the whole estimate as we approach the goal.\nTwo value approximation methods: MC (left), TD (right) as converging in their predictions of the value of each of the states in the x-axis. The example is from a hypothetical commute from office back home. In MC you have to wait until the episode ended (reach the goal) to update the value function at each state of the trajectory. In contrast, TD updates the value function at each state based on the estimates of the total travel time. The goal state is \u0026ldquo;arrive home\u0026rdquo;, while the reward function is time.\nAs you can notice in the figure above the solid arrows in the MC case, adjust the predicted value of each state to the actual return while in the TD case the value prediction happens every step in the way. We call TD for this reason an online learning scheme. Another characteristic of TD is that it does not depend on reaching the goal, it continuously learns. MC does depend on the goal and therefore is episodic. This is important in many mission critical applications eg self-driving cars where you dont wait to \u0026ldquo;crash\u0026rdquo; to apply corrections to your state value based on what you experienced.\nMathematically, instead of using the true return, $G_t$, something that it is possible in the MC as we are trully experiencing the world along a trajectory, TD uses a (biased) estimated return called the TD target: $ R_{t+1} + \\gamma V(S_{t+1})$ approximating the value function as:\n$$ V(S_t) = V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right)$$  The difference below is called the TD approximation error,\n$$\\delta_t = R_{t+1} + \\gamma (V(S_{t+1}) - V(S_t))$$\nThe TD($\\lambda$) The TD approach of the previous section, can be extended to multiple steps. Instead of a single look ahead step we can take multiple successive look ahead steps (n), we will call this TD(n) for now, and at the end of the n-th step, we use the value function at that state to backup and get the value function at the state where we started. Effectively after n-steps our return will be:\n$$G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \u0026hellip; + \\gamma^{n-1}R_{t+n} + \\gamma_n V(S_n)$$\nand the TD(n) learning equation becomes\n$$ V(S_t) = V(S_t) + \\alpha \\left( G^{(n)}_t - V(S_t) \\right) $$\nWe now define the so called $\\lambda$-return that combines all n-step return $G_t^{(n)}$ via the weighting function shown below as,\n$$G_t^{(n)} = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}$$\n$\\lambda$ weighting function for TD($\\lambda$)\nthe TD(n) learning equation becomes\n$$ V(S_t) = V(S_t) + \\alpha \\left( G^\\lambda_t - V(S_t) \\right) $$  When $\\lambda=0$ we get TD(0) learning, while when $\\lambda=1$ we get learning that is roughly equivalent to MC. It is instructive to see the difference between MC and TD approaches.\nTD vs MC approaches to $V(s)$ function estimation. TD is converging much faster but it has larger bias compared to MC.\n"});index.add({'id':55,'href':'/cs-gy-6613-spring-2020/docs/lectures/mdp/policy-iteration/','title':"Policy Iteration",'content':"Policy Iteration In this chapter we develop the so called planning problem (which is RL without learning) where we are dealing with a known MDP. This means that we know the transition and reward functions/models of the environment and we are after the optimal policy solutions.\nDynamic Programming and Policy Iteration In the MDP chapter we have derived the Bellman expectation backup equations which allowed us to efficiently compute the value function.\nWe have seen also that the Bellman optimality equations are non linear and need to be solved using iterative approaches - their solution will result in the optimal value function $v_*$ and the optimal policy $\\pi_*$. Since the Bellman equations allow us to decompose recursively the problem into sub-problems, they in fact implement a general and exact approach called _dynamic programming_ which assumes full knowledge of the MDP.\nIn the policy iteration, given the policy $\\pi$, we iterate two distinct steps as shown below:\nPolicy iteration in solving the MDP - in each iteration we execute two steps, policy evaluation and policy improvement\n  In the evaluation (also called the prediction) step we estimate the state value function $v_\\pi ~ \\forall s \\in \\mathcal S$.\n  In the improvement (also called the control) step we apply the greedy heuristic and elect a new policy based on the evaluation of the previous step.\n  This is shown next,\nPolicy and state value convergence to optimality in policy iteration. Up arrows are the evaluation steps while down arrows are the improvement steps.\nIt can be shown that the policy iteration will converge to the optimal value function $v_*(s)$ and policy $\\pi_*$.\nPolicy Evaluation Step The policy $\\pi$ is evaluated when we have produced the state-value function $v_\\pi(s)$ for all states. In other words when we know the expected discounted returns that each state can offer us. To do so we apply the Bellman expectation backup equations repeatedly in an iterative fashion.\nWe start at $k=0$ by initializing all state-value function (a vactor) to $v_0(s)=0$. In each iteration $k+1$ we start with the state value function of the previous iteration $v_k(s)$ and apply the Bellman expectation backup as prescribed by the one step lookahead tree below that is decorated relative to what we have seen with the iteration information. This is called the synchronous backup formulation as we are updating all the elements of the value function vector at the same time.\nTree representation of the state-value function with one step look ahead across iterations.\nThe Bellman expectation backup is given by,\n$$v_{k+1}(s) = \\sum_{a \\in \\mathcal A} \\pi(a|s) \\left( \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_k(s^\\prime) \\right)$$\nand in vector form,\n$$\\mathbf{v}^{k+1} = \\mathbf{\\mathcal R}^\\pi + \\gamma \\mathbf{\\mathcal P}^\\pi \\mathbf{v}^k$$\nThe following source code is instructive and standalone. It executes the policy evaluation for the Gridworld environment from the many that are part of the Gym RL python library.\n Policy Evaluation Python Code ...  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  # this code is from https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919 import numpy as np import gym.spaces from gridworld import GridworldEnv env = GridworldEnv() def policy_eval(policy, env, discount_factor=1.0, epsilon=0.00001): \u0026#34;\u0026#34;\u0026#34;Evaluate a policy given an environment and a full description of the environment\u0026#39;s dynamics.Args:policy: [S, A] shaped matrix representing the policy.env: OpenAI env. env.P represents the transition probabilities of the environment.env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).env.nS is a number of states in the environment. env.nA is a number of actions in the environment.theta: We stop evaluation once our value function change is less than theta for all states.discount_factor: Gamma discount factor.Returns:Vector of length env.nS representing the value function.\u0026#34;\u0026#34;\u0026#34; # Start with a random (all 0) value function V_old = np.zeros(env.nS) while True: #new value function V_new = np.zeros(env.nS) #stopping condition delta = 0 #loop over state space for s in range(env.nS): #To accumelate bellman expectation eqn v_fn = 0 #get probability distribution over actions action_probs = policy[s] #loop over possible actions for a in range(env.nA): #get transitions [(prob, next_state, reward, done)] = env.P[s][a] #apply bellman expectatoin eqn v_fn += action_probs[a] * (reward + discount_factor * V_old[next_state]) #get the biggest difference over state space delta = max(delta, abs(v_fn - V_old[s])) #update state-value V_new[s] = v_fn #the new value function V_old = V_new #if true value function if(delta \u0026lt; epsilon): break return np.array(V_old) random_policy = np.ones([env.nS, env.nA]) / env.nA v = policy_eval(random_policy, env) expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0]) np.testing.assert_array_almost_equal(v, expected_v, decimal=2) print(v) print(expected_v)       Policy Improvement Step In the policy iteration step we are given the value function and simply apply the greedy heuristic to it.\n$$\\pi^\\prime = \\mathtt{greedy}(v_\\pi)$$\nIt can be shown that this heuristic results into a policy that is better than the one the prediction step started ($\\pi^\\prime \\ge \\pi$) and this extends into multiple iterations. We can therefore converge into an optimal policy - the interested reader can follow this lecture for a justification.\n Policy Iteration Python Code ...  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88  import numpy as np import gym.spaces from gridworld import GridworldEnv env = GridworldEnv() def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0): \u0026#34;\u0026#34;\u0026#34;Policy Improvement Algorithm. Iteratively evaluates and improves a policyuntil an optimal policy is found.Args:env: The OpenAI envrionment.policy_eval_fn: Policy Evaluation function that takes 3 arguments:policy, env, discount_factor.discount_factor: gamma discount factor.Returns:A tuple (policy, V). policy is the optimal policy, a matrix of shape [S, A] where each state scontains a valid probability distribution over actions.V is the value function for the optimal policy.\u0026#34;\u0026#34;\u0026#34; def one_step_lookahead(s, value_fn): actions = np.zeros(env.nA) for a in range(env.nA): [(prob, next_state, reward, done)] = env.P[s][a] actions[a] = prob * (reward + discount_factor * value_fn[next_state]) return actions # Start with a random policy policy = np.ones([env.nS, env.nA]) / env.nA actions_values = np.zeros(env.nA) while True: #evaluate the current policy value_fn = policy_eval_fn(policy, env) policy_stable = True #loop over state space for s in range(env.nS): #perform one step lookahead actions_values = one_step_lookahead(s, value_fn) #maximize over possible actions  best_action = np.argmax(actions_values) #best action on current policy chosen_action = np.argmax(policy[s]) #if Bellman optimality equation not satisifed if(best_action != chosen_action): policy_stable = False #the new policy after acting greedily w.r.t value function policy[s] = np.eye(env.nA)[best_action] #if Bellman optimality eqn is satisfied if(policy_stable): return policy, value_fn policy, v = policy_improvement(env) print(\u0026#34;Policy Probability Distribution:\u0026#34;) print(policy) print(\u0026#34;\u0026#34;) print(\u0026#34;Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\u0026#34;) print(np.reshape(np.argmax(policy, axis=1), env.shape)) print(\u0026#34;\u0026#34;) print(\u0026#34;Value Function:\u0026#34;) print(v) print(\u0026#34;\u0026#34;) print(\u0026#34;Reshaped Grid Value Function:\u0026#34;) print(v.reshape(env.shape)) print(\u0026#34;\u0026#34;)       We already have seen that in the Gridworld environment, we may not need to reach the optimal state value function $v_*(s)$ for an optimal policy to result, as shown in the next figure where the value function for the $k=3$ iteration results the same policy as the policy from a far more accurate value function (large k). The gridworld below is characterized by:\n Not discounted episodic MDP (γ = 1) Non terminal states 1, \u0026hellip;, 14 One terminal state (shown twice as shaded squares) Actions leading out of the grid leave state unchanged Reward is −1 until the terminal state is reached Agent follows uniform random policy $\\pi(north|.) = \\pi(south|.) = \\pi(east|.) = \\pi(west | .) = 0.25$  Convergence to optimal policy via separate prediction and policy improvement iterations\nWe can therefore stop early and taking the argument to the limit, do the policy improvement step in each iteration.\nIn summary, we have seen that policy iteration solves the known MDPs. In the next section we remove the known MDP assumption and deal with the first Reinforcement Learning (RL) algorithm.\n A more graphical way to understand how policy iteration functions is through this python code that depicts a more elaborate gridworld. You can type python policy_iteration.py to debug and step through the code after installing the dependencies.\n "});index.add({'id':56,'href':'/cs-gy-6613-spring-2020/docs/lectures/drl/control/','title':"Model-free Control",'content':"Model-free Control In this section we outline methods that can result in optimal policies when the MDP is unknown and we need to learn its underlying functions / models - also known as the model free control problem. Learning in this chapter, follows the on-policy approach where the agent learns these models \u0026ldquo;on the job\u0026rdquo;, so to speak.\nIn the model-free prediction section we have seen that it is in fact possible to get to estimate the state value function without any MDP model dependencies. However, when we try to do do greedy policy improvement\n$$\\pi^\\prime(s) = \\argmax_{a \\in \\mathcal A} (\\mathcal R_s^a + \\mathcal P_{ss^\\prime}^a V(s^\\prime))$$\nwe do have dependencies on knowing the dynamics of the MDP. So the obvious question is - have we done all this discussion in vain? It turns out that we did not. All it takes is to apply prediction to the state-action value function $Q(s,a)$ and then apply the greedy policy improvement step\n$$\\pi^\\prime = \\argmax_{a \\in \\mathcal A} Q(s,a)$$\nThis is shown next:\nGeneralized policy iteration using action-value function\nThe following tables summarize the relationship between TD backup and DP (full backup). The entry for SARSA will be evident after you go through it.\nBackup Trees for DP vs TD\nUpdate equation comparison between DP vs TD\n"});index.add({'id':57,'href':'/cs-gy-6613-spring-2020/docs/lectures/drl/reinforce/','title':"REINFORCE",'content':"The REINFORCE Algorithm Given that RL can be posed as an MDP, in this section we continue with a policy-based algorithm that learns the policy directly by optimizing the objective function and can then map the states to actions. The algorithm we treat here, called REINFORCE, is important although more modern algorithms do perform better.\nIt took its name from the fact that during training actions that resulted in good outcomes should become more probable—these actions are positively reinforced. Conversely, actions which resulted in bad outcomes should become less probable. If learning is successful, over the course of many iterations, action probabilities produced by the policy, shift to a distribution that results in good performance in an environment. Action probabilities are changed by following the policy gradient, therefore REINFORCE is known as a policy gradient algorithm.\nThe algorithm needs three components:\n   Component Description     Parametrized policy $\\pi_\\theta (a|s)$ The key idea of the algorithm is to learn a good policy, and this means doing function approximation. Neural networks are powerful and flexible function approximators, so we can represent a policy using a deep neural network (DNN) consisting of learnable parameters $\\mathbf \\theta$. This is often referred to as a policy network $\\pi_θ$. We say that the policy is parametrized by $\\theta$. Each specific set of values of the parameters of the policy network represents a particular policy. To see why, consider $θ1 ≠ θ2$. For any given state $s$, different policy networks may output different sets of action probabilities, that is, $π_{θ1}(a   The objective to be maximized $J(\\pi_\\theta)$1 At this point is nothing else other than the expected discounted return over policy, just like in MDP.   Policy Gradient A method for updating the policy parameters $\\theta$. The policy gradient algorithm searches for a local maximum in $J(\\pi_\\theta)$: $\\max_\\theta J(\\pi_\\theta)$. This is the common gradient ascent algorithm that we met in a similar form in neural network. $$\\theta ← \\theta + \\alpha \\nabla_\\theta J(\\pi_\\theta)$$ where $\\alpha$ is the learning rate.    Out of the three components the most complicated one is the policy gradient that can be shown to be given by the differentiable quantity:\n$$ \\nabla_\\theta J(\\pi_\\theta)= \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta (a|s) v_\\pi (s) \\right ]$$\nWe understand that this expression came out of nowhere but the interested reader can find its detailed derivation in the chapter 2 of the DRL book. We can approximate the value at state $s$ with the return over many sample trajectories $\\tau$ that are sampled from the policy network.\n$$ \\nabla_\\theta J(\\pi_\\theta)= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ G_t \\nabla_\\theta \\log \\pi_\\theta (a|s) \\right ]$$\nwhere $G_t$ is the return - a quantity we have seen earlier albeit now the return is limited by the length of each trajectory,\n$$G_t(\\tau) = \\sum_{k=0}^{T-1}\\gamma^k R_{t+1+k}$$\nThe $\\gamma$ is usually a hyper-parameter that we need to optimize usually iterating over many values in [0.01,\u0026hellip;,0.99] and selecting the one with the best results.\nWe also have an expectation in the gradient expression that we need to address. The expectation $\\mathbb E_{\\tau \\sim \\pi_\\theta}$ we need to take is approximated with a summation over _each_ trajectory. This is commonly called Monte-Carlo approximation. Effectively, we are generating the right hand side as in line 8 in the code below, by sampling a trajectory (line 4) and estimating its return (line 7) in a completely model-free fashion i.e. without assuming any knowledge of the transition and reward functions. This is implemented next:\n1: Initialize learning rate $\\alpha$\n2: Initialize weights θ of a policy network $π_θ$\n3: for episode = 0, . . . , MAX_EPISODE do\n4: Sample a trajectory using the policy network $\\pi_\\theta$, $τ = s_0, a_0, r_0, . . . , s_T, a_T, r_T$\n5: Set $∇_θ J(π_θ) = 0$\n6: for t = 0, . . . , T-1 do\n7: Calculate $G_t(τ)$\n8: $\\nabla_\\theta J(\\pi_\\theta) = \\nabla_\\theta J(\\pi_\\theta) + G_t (τ) \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t) $\n9: end for\n10: $θ = θ + α ∇_θ J(π_θ)$\n11: end for\nIt is important that a trajectory is discarded after each parameter update—it cannot be reused. This is because REINFORCE is an on-policy algorithm. Intuitively an on-policy algorithm \u0026ldquo;learns on the job\u0026rdquo;. This is evidently seen in line 10 where the parameter update equation uses the policy gradient that itself (line 8) directly depends on action probabilities $π_θ(a_t | s_t)$ generated by the current policy $π_θ$ only and not some past policy $π_{θ′}$. Correspondingly, the return $G_t(τ)$ where $τ ~ π_θ$ must also be generated from $π_θ$, otherwise the action probabilities will be adjusted based on returns that the policy wouldn’t have generated.\nPolicy Network One of the key ingredients that DRL introduces is the policy network that is approximated with a DNN eg. a fully connected neural network with a number of hidden layers that is hyper-parameter (e.g. 2 RELU).\n1: Given a policy network net, a Categorical (multinomial) distribution class, and a state\n2: Compute the output pdparams = net(state)\n3: Construct an instance of an action probability distribution pd = Categorical(logits=pdparams)\n4: Use pd to sample an action, action = pd.sample()\n5: Use pd and action to compute the action log probability, log_prob = pd.log_prob(action)\nOther discrete distributions can be used and many actual libraries parametrize continuous distributions such as Gaussians.\nApplying the REINFORCE algorithm It is now instructive to see an stand-alone example in python for the so called CartPole-v0 2\n REINFORCE Python Code ...  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80  1 from torch.distributions import Categorical 2 import gym 3 import numpy as np 4 import torch 5 import torch.nn as nn 6 import torch.optim as optim 7 8 gamma = 0.99 9 10 class Pi(nn.Module): 11 def __init__(self, in_dim, out_dim): 12 super(Pi, self).__init__() 13 layers = [ 14 nn.Linear(in_dim, 64), 15 nn.ReLU(), 16 nn.Linear(64, out_dim), 17 ] 18 self.model = nn.Sequential(*layers) 19 self.onpolicy_reset() 20 self.train() # set training mode 21 22 def onpolicy_reset(self): 23 self.log_probs = [] 24 self.rewards = [] 25 26 def forward(self, x): 27 pdparam = self.model(x) 28 return pdparam 29 30 def act(self, state): 31 x = torch.from_numpy(state.astype(np.float32)) # to tensor 32 pdparam = self.forward(x) # forward pass 33 pd = Categorical(logits=pdparam) # probability distribution 34 action = pd.sample() # pi(a|s) in action via pd 35 log_prob = pd.log_prob(action) # log_prob of pi(a|s) 36 self.log_probs.append(log_prob) # store for training 37 return action.item() 38 39 def train(pi, optimizer): 40 # Inner gradient-ascent loop of REINFORCE algorithm 41 T = len(pi.rewards) 42 rets = np.empty(T, dtype=np.float32) # the returns 43 future_ret = 0.0 44 # compute the returns efficiently 45 for t in reversed(range(T)): 46 future_ret = pi.rewards[t] + gamma * future_ret 47 rets[t] = future_ret 48 rets = torch.tensor(rets) 49 log_probs = torch.stack(pi.log_probs) 50 loss = - log_probs * rets # gradient term; Negative for maximizing 51 loss = torch.sum(loss) 52 optimizer.zero_grad() 53 loss.backward() # backpropagate, compute gradients 54 optimizer.step() # gradient-ascent, update the weights 55 return loss 56 57 def main(): 58 env = gym.make(\u0026#39;CartPole-v0\u0026#39;) 59 in_dim = env.observation_space.shape[0] # 4 60 out_dim = env.action_space.n # 2 61 pi = Pi(in_dim, out_dim) # policy pi_theta for REINFORCE 62 optimizer = optim.Adam(pi.parameters(), lr=0.01) 63 for epi in range(300): 64 state = env.reset() 65 for t in range(200): # cartpole max timestep is 200 66 action = pi.act(state) 67 state, reward, done, _ = env.step(action) 68 pi.rewards.append(reward) 69 env.render() 70 if done: 71 break 72 loss = train(pi, optimizer) # train per episode 73 total_reward = sum(pi.rewards) 74 solved = total_reward \u0026gt; 195.0 75 pi.onpolicy_reset() # onpolicy: clear memory after training 76 print(f\u0026#39;Episode {epi}, loss: {loss}, \\ 77 total_reward: {total_reward}, solved: {solved}\u0026#39;) 78 79 if __name__ == \u0026#39;__main__\u0026#39;: 80 main()       The REINFORCE algorithm presented here can generally be applied to continuous and discreet problems but it has been shown to possess high variance and sample-inefficiency. Several improvements have been proposed and the interested reader can refer to section 2.5.1 of the suggested book.\n  Notation wise, since we need to have a bit more flexibility in RL problems, we will use the symbol $J(\\pi_\\theta)$ as the objective function. \u0026#x21a9;\u0026#xfe0e;\n Please note that SLM-Lab, the library that accompanies the suggested in the syllabus book, is a mature library and probably a good example of how to develop ML/RL libraries in python. You will learn a lot by reviewing the implementations under the agents/algorithms directory to get a feel of how RL problems are abstracted . \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':58,'href':'/cs-gy-6613-spring-2020/docs/lectures/mdp/value-iteration/','title':"Value Iteration",'content':"Value Iteration In this chapter we will look at an approach called value iteration for the control problem with a known MDP. This is similar to the policy-based that we presume the reader has gone through.\nSummary of Value Iteration\nDynamic Programming and Value Iteration The basic principle behind value-iteration is the principle that underlines dynamic programming and is called the principle of optimality as applied to policies. According to this principle an optimal policy can be divided into two components.\n An optimal first action $a_*$. An optimal policy from the successor state $s^\\prime$.  More formally, a policy $\\pi(a|s)$ achieves the optimal value from state $s$, $v_\\pi(s) = v_*(s)$ iff for any state $s^\\prime$ reachable from $s$, $v_\\pi(s^\\prime)=v_*(s)$.\nEffectively this principle allows us to decompose the problem into two sub-problems with one of them being straightforward to determine and use the Bellman optimality equation that provides the one step backup induction at each iteration.\n$$v_*(s) = \\max_a \\left( \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_*(s^\\prime) \\right)$$\nAs an example if I want to move optimally towards a location in the room, I can make a optimal first step and at that point I can follow the optimal policy, that I was magically given, towards the desired final location. That optimal first step, think about making it by walking backwards from the goal. We start at the end of the problem where we know the final rewards and work backwards to all the states that correct to it in our look-ahead tree.\nOne step look-ahead tree representation of value iteration algorithm\nThe \u0026ldquo;start from the end\u0026rdquo; intuition behind the equation is usually applied with no consideration as to if we are at the end or not. We just do the backup inductive step for each state. In value iteration for synchronous backups, we start at $k=0$ from the value function $v_0(s)=0.0$ and at each iteration $k+1$ for all states $s \\in \\mathcal{S}$ we update the $v_{k+1}(s)$ from $v_k(s)$. As the iterations progress, the value function will converge to $v_*$.\nThe equation of value iteration is taken straight out of the Bellman optimality equation.\n$$v_{k+1}(s) = \\max_a \\left( \\mathcal R_s^a + \\gamma \\sum_{s^\\prime \\in \\mathcal S} \\mathcal{P}^a_{ss^\\prime} v_k(s^\\prime) \\right) $$\nwhich can be written in a vector form as,\n$$\\mathbf v_{k+1} = \\max_a \\left( \\mathcal R^a + \\gamma \\mathcal P^a \\mathbf v_k \\right) $$\nNotice that we are not building an explicit policy at every iteration and also perhaps importantly, the intermediate value functions may not correspond to a feasible policy. Before going into a more elaborate example, we can go back to the same simple world we have looked at in the policy iteration section and focus only on the state-value calculation using the formula above.\nState values for an MDP with random policy (0.25 prob of taking any of the four available actions), $\\gamma=1$, that rewards the agent with -1 at each transition except towards the goal states that are in the top left and bottom right corners\nWe return to the tree representation of the value iteration with DP - this will be useful when we compare the DP with other value iteration approaches.\n$$V(S_t) = \\mathbb E_\\pi \\left[R_{t+1} + \\gamma V(S_{t+1}) \\right]$$\nBackup tree with value iteration based on the DP approach - Notice that we do one step look ahead but we do not sample as we do in the other value iteration approaches.\nDP Value-iteration example In example world shown below (from here)\nGridworld to showcase the state-value calculation in Python code below. The states are numbered sequentially from top right.\nwe can calculate the state-value function its the vector form - the function in this world maps the state space to the 11th dim real vector space $v(s): \\mathcal S \\rightarrow \\mathbb R^{11}$ aka the value function is a vector of size 11.\n$$\\mathbf v_{k+1} = \\max_a \\left( \\mathcal R^a + \\gamma \\mathcal P^a \\mathbf v_k \\right) $$\n Grid world value iteration ↕  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  # Look at the backup tree above and the vector form Bellman equation to understand this code.  # Have them side by side while you are reading.  # Each of the 11 rows of the \u0026#34;matrix\u0026#34; P[s][a] has 4 tuples - one for each of the allowed actions. Each tuple / action is written in the format (probability, s\u0026#39;) and is associated with the 3 possible next states that the agent may end up despite its intention to go to the desired state. The states are numbered sequentially from top left to bottom right.  P = { 0: {0: [(0.9,0),(0.1,1),(0,4)], 1: [(0.8,1),(0.1,4),(0.1,0)], 2: [(0.8,4),(0.1,1),(0.1,0)], 3: [(0.9,0),(0.1,4)]}, 1: {0: [(0.8,1),(0.1,2),(0.1,0)], 1: [(0.8,2),(0.2,1)], 2: [(0.8,1),(0.1,0),(0.1,2)], 3: [(0.8,0),(0.2,1)]}, 2: {0: [(0.8,2),(0.1,3),(0.1,1)], 1: [(0.8,3),(0.1,5),(0.1,2)], 2: [(0.8,5),(0.1,1),(0.1,3)], 3: [(0.8,1),(0.1,2),(0.1,5)]}, 3: {0: [(0.9,3),(0.1,2)], 1: [(0.9,3),(0.1,6)], 2: [(0.8,6),(0.1,2),(0.1,3)], 3: [(0.8,2),(0.1,3),(0.1,6)]}, 4: {0: [(0.8,0),(0.2,4)], 1: [(0.8,4),(0.1,7),(0.1,0)], 2: [(0.8,7),(0.2,4)], 3: [(0.8,4),(0.1,0),(0.1,7)]}, 5: {0: [(0.8,2),(0.1,6),(0.1,5)], 1: [(0.8,6),(0.1,9),(0.1,2)], 2: [(0.8,9),(0.1,5),(0.1,6)], 3: [(0.8,5),(0.1,2),(0.1,9)]}, 6: {0: [(0.8,3),(0.1,6),(0.1,5)], 1: [(0.8,6),(0.1,10),(0.1,3)], 2: [(0.8,10),(0.1,5),(0.1,6)], 3: [(0.8,5),(0.1,3),(0.1,10)]}, 7: {0: [(0.8,4),(0.1,8),(0.1,7)], 1: [(0.8,8),(0.1,7),(0.1,4)], 2: [(0.9,7),(0.1,8)], 3: [(0.9,7),(0.1,4)]}, 8: {0: [(0.8,8),(0.1,9),(0.1,7)], 1: [(0.8,9),(0.2,8)], 2: [(0.8,8),(0.1,7),(0.1,9)], 3: [(0.8,7),(0.2,8)]}, 9: {0: [(0.8,5),(0.1,10),(0.1,8)], 1: [(0.8,9),(0.1,9),(0.1,5)], 2: [(0.8,9),(0.1,8),(0.1,10)], 3: [(0.8,8),(0.1,5),(0.1,9)]}, 10: {0: [(0.8,6),(0.1,10),(0.1,9)], 1: [(0.9,10),(0.1,6)], 2: [(0.9,10),(0.1,9)], 3: [(0.8,9),(0.1,6),(0.1,10)]} } R = [0, 0, 0, 1, 0, 0, -100, 0, 0, 0, 0] gamma = 0.9 States = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Actions = [0, 1, 2, 3] # [north, east, south, west] v = [0]*11 # value iteration for i in range(100): for s in States: # trans[1] = s\u0026#39; # trans[0] = P_ss\u0026#39; q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0]) q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1]) q_2 = sum(trans[0]*v[trans[1]] for trans in P[s][2]) q_3 = sum(trans[0]*v[trans[1]] for trans in P[s][3]) v[s] = R[s] + gamma*max(q_0, q_1, q_2, q_3) print(v) # [5.46991289990088, 6.313016781079707, 7.189835364530538, 8.668832766371658, 4.8028486314273, 3.346646443535637, -96.67286272722137, 4.161433444369266, 3.6539401768050603, 3.2220160316109103, 1.526193402980731] # once v computed, we can calculate the optimal policy  optPolicy = [0]*11 for s in States: optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions]) print(optPolicy) # [1, 1, 1, 0, 0, 3, 3, 0, 3, 3, 2]        A more graphical way to understand how policy iteration functions is through this python code that depicts a more elaborate gridworld. You can type python value_iteration.py to debug and step through the code after installing the dependencies.\n "});index.add({'id':59,'href':'/cs-gy-6613-spring-2020/docs/lectures/drl/sarsa/','title':"SARSA",'content':"The SARSA DRL Algorithm As we discussed in the [model-free control] section, the SARSA algorithm implements a $Q(s,a)$ value-based algorithm. SARSA is one of the best known RL algorithms and is very practical as compared to pure policy-based algorithms. It tends to be more sample efficient - a general trait of many value-based algorithms despite the algorithmic hybridization that is usually applicable today. Its name is attributed to the fact that we need to know the State-Action-Reward-State-Action before performing an update. The tree for SARSA is shown below:\nSARSA action-value backup update tree. The name SARSA is written as you read from the top to the bottom of the tree :)\nThere are two concepts that we need to grasp:\n The first is a technique for learning the Q-function via TD-learning that we have seen earlier. The second is a method for evolving the policy using the learned Q-function.  Using the tree and following the value estimate of temporal difference (TD) learning, we can write the value update equation as:\n$$Q(S,A) = Q(S,A) + \\alpha (R + \\gamma Q(S^\\prime, A^\\prime)-Q(S,A))$$\nEffectively the equation above updates the Q function by $\\alpha$ times the direction of the TD-error. What SARSA does is basically the policy iteration diagram we have seen in the control above but with a twist. Instead of trying to evaluate the policy all the way using the DP, or over an episode using MC, SARSA does policy improvement over each time step significantly increasing the iteration rate - this is figuratively shown below:\nSARSA on-policy control\nThe idea is to increase the frequency of the so called $\\epsilon$-Greedy policy improvement step where we select with probability $\\epsilon$ a random action instead of the action that maximizes the $Q(s,a)$ function (greedy). We do so, in order to \u0026ldquo;hit\u0026rdquo; new states that we havent seen before (exploration).\nThe SARSA algorithm is summarized below:\nSARSA algorithm for on-policy control\n$Q(s,a)$ in practice is a table (matrix) stored in memory. Every step that we take an action we flip a \u0026ldquo;bent coin\u0026rdquo; and if \u0026ldquo;heads\u0026rdquo; comes up we take the maximum of the $Q(s,a)$ values and this will be the policy improvement for the subsequent step. If it comes up as \u0026ldquo;tails\u0026rdquo; we just pick a random action and update the policy accordingly.\nSARSA Example Suppose an agent is learning to play the toy environment shown above. This is a essentially a corridor and the agent has to learn to navigate to the end of the corridor to the good terminal state $s_{T2}$, denoted with a star.\n There are five states in total. The agent always starts the game in state $s_1$, denoted $S$, and the game ends if the agent reaches either of the terminal states. $s_{T2}$ is the goal state—the agent receives a reward of 1 if it reaches this state. There are only two actions, $a_{UP}$ and $a_{DOWN}$. The agent receives rewards of 0 in all other states. The agent’s discount rate γ is 0.9. The game is optimally solved by a policy which reaches $s_{T2}$ in the smallest number of steps because an agent values rewards received sooner more than rewards received later in time. In this case, the smallest number of steps an agent can take to optimally solve the environment is 3.  How can we learn the optimal Q function?\nThe diagram is split into five blocks from top to bottom. Each block corresponds to a single episode of experiences in the environment; the first block corresponds to the first episode, the second block the second episode, and so on. Each block contains a number of columns. They are interpreted from left to right as follows:\n  Q-function episode start: The value of the Q-function at the start of the episode. At the beginning of episode 1, all of the values are initialized to 0 since we have no information yet about the function.\n  Episode: The episode number.\n  Time step: Each block contains a variable number of experiences. For example, there are three experiences in episode 2 and seven experiences in episode 4. The time index of each experience within the block is indicated by the time step.\n  Action: The action the agent took at each time step.\n  $(s, a, r, s′)$: The agent’s experience at each time step. This consists of the current state s, the action the agent took a, the reward received r, and the next state the environment transitioned into s′.\n  $r + γQ*(s′, a)$: The target value (i.e., the right-hand side of the equation) to use in the Bellman update:\n $$Q*(s, a) = r + γQ*(s^\\prime, a^\\prime)$$.    Q-function episode end: The value of Q-function at the end of the episode. The Bellman update has been applied for each experience of the episode in time step order. This means that the Bellman update was applied first for the experience corresponding to time step 1, then time step 2, and so on. The table shows the final result after all of the Bellman updates have been applied for the episode.\n  TD Q-function learning\nObviously this is a trivial example to show in detail the calculations that are being done at every episode and time step. For a more elaborate gridworld, the python code that follows shows how SARSA would work in the environment below.\nSARSA Gridworld\n Python SARSA Gridworld Envrironment ↕  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144  # code is from https://github.com/rlcode/reinforcement-learning/tree/master/1-grid-world import time import numpy as np import tkinter as tk from PIL import ImageTk, Image np.random.seed(1) PhotoImage = ImageTk.PhotoImage UNIT = 100 # pixels HEIGHT = 5 # grid height WIDTH = 5 # grid width class Env(tk.Tk): def __init__(self): super(Env, self).__init__() self.action_space = [\u0026#39;u\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;r\u0026#39;] self.n_actions = len(self.action_space) self.title(\u0026#39;SARSA\u0026#39;) self.geometry(\u0026#39;{0}x{1}\u0026#39;.format(HEIGHT * UNIT, HEIGHT * UNIT)) self.shapes = self.load_images() self.canvas = self._build_canvas() self.texts = [] def _build_canvas(self): canvas = tk.Canvas(self, bg=\u0026#39;white\u0026#39;, height=HEIGHT * UNIT, width=WIDTH * UNIT) # create grids for c in range(0, WIDTH * UNIT, UNIT): # 0~400 by 80 x0, y0, x1, y1 = c, 0, c, HEIGHT * UNIT canvas.create_line(x0, y0, x1, y1) for r in range(0, HEIGHT * UNIT, UNIT): # 0~400 by 80 x0, y0, x1, y1 = 0, r, HEIGHT * UNIT, r canvas.create_line(x0, y0, x1, y1) # add img to canvas self.rectangle = canvas.create_image(50, 50, image=self.shapes[0]) self.triangle1 = canvas.create_image(250, 150, image=self.shapes[1]) self.triangle2 = canvas.create_image(150, 250, image=self.shapes[1]) self.circle = canvas.create_image(250, 250, image=self.shapes[2]) # pack all canvas.pack() return canvas def load_images(self): rectangle = PhotoImage( Image.open(\u0026#34;../img/rectangle.png\u0026#34;).resize((65, 65))) triangle = PhotoImage( Image.open(\u0026#34;../img/triangle.png\u0026#34;).resize((65, 65))) circle = PhotoImage( Image.open(\u0026#34;../img/circle.png\u0026#34;).resize((65, 65))) return rectangle, triangle, circle def text_value(self, row, col, contents, action, font=\u0026#39;Helvetica\u0026#39;, size=10, style=\u0026#39;normal\u0026#39;, anchor=\u0026#34;nw\u0026#34;): if action == 0: origin_x, origin_y = 7, 42 elif action == 1: origin_x, origin_y = 85, 42 elif action == 2: origin_x, origin_y = 42, 5 else: origin_x, origin_y = 42, 77 x, y = origin_y + (UNIT * col), origin_x + (UNIT * row) font = (font, str(size), style) text = self.canvas.create_text(x, y, fill=\u0026#34;black\u0026#34;, text=contents, font=font, anchor=anchor) return self.texts.append(text) def print_value_all(self, q_table): for i in self.texts: self.canvas.delete(i) self.texts.clear() for x in range(HEIGHT): for y in range(WIDTH): for action in range(0, 4): state = [x, y] if str(state) in q_table.keys(): temp = q_table[str(state)][action] self.text_value(y, x, round(temp, 2), action) def coords_to_state(self, coords): x = int((coords[0] - 50) / 100) y = int((coords[1] - 50) / 100) return [x, y] def reset(self): self.update() time.sleep(0.5) x, y = self.canvas.coords(self.rectangle) self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y) self.render() # return observation return self.coords_to_state(self.canvas.coords(self.rectangle)) def step(self, action): state = self.canvas.coords(self.rectangle) base_action = np.array([0, 0]) self.render() if action == 0: # up if state[1] \u0026gt; UNIT: base_action[1] -= UNIT elif action == 1: # down if state[1] \u0026lt; (HEIGHT - 1) * UNIT: base_action[1] += UNIT elif action == 2: # left if state[0] \u0026gt; UNIT: base_action[0] -= UNIT elif action == 3: # right if state[0] \u0026lt; (WIDTH - 1) * UNIT: base_action[0] += UNIT # move agent self.canvas.move(self.rectangle, base_action[0], base_action[1]) # move rectangle to top level of canvas self.canvas.tag_raise(self.rectangle) next_state = self.canvas.coords(self.rectangle) # reward function if next_state == self.canvas.coords(self.circle): reward = 100 done = True elif next_state in [self.canvas.coords(self.triangle1), self.canvas.coords(self.triangle2)]: reward = -100 done = True else: reward = 0 done = False next_state = self.coords_to_state(next_state) return next_state, reward, done def render(self): time.sleep(0.03) self.update()        Python SARSA code for agent ↕  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78  import numpy as np import random from collections import defaultdict from environment import Env # SARSA agent learns every time step from the sample \u0026lt;s, a, r, s\u0026#39;, a\u0026#39;\u0026gt; class SARSAgent: def __init__(self, actions): self.actions = actions self.learning_rate = 0.01 self.discount_factor = 0.9 self.epsilon = 0.1 self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0]) # with sample \u0026lt;s, a, r, s\u0026#39;, a\u0026#39;\u0026gt;, learns new q function def learn(self, state, action, reward, next_state, next_action): current_q = self.q_table[state][action] next_state_q = self.q_table[next_state][next_action] new_q = (current_q + self.learning_rate * (reward + self.discount_factor * next_state_q - current_q)) self.q_table[state][action] = new_q # get action for the state according to the q function table # agent pick action of epsilon-greedy policy def get_action(self, state): if np.random.rand() \u0026lt; self.epsilon: # take random action action = np.random.choice(self.actions) else: # take action according to the q function table state_action = self.q_table[state] action = self.arg_max(state_action) return action @staticmethod def arg_max(state_action): max_index_list = [] max_value = state_action[0] for index, value in enumerate(state_action): if value \u0026gt; max_value: max_index_list.clear() max_value = value max_index_list.append(index) elif value == max_value: max_index_list.append(index) return random.choice(max_index_list) if __name__ == \u0026#34;__main__\u0026#34;: env = Env() agent = SARSAgent(actions=list(range(env.n_actions))) for episode in range(1000): # reset environment and initialize state state = env.reset() # get action of state from agent action = agent.get_action(str(state)) while True: env.render() # take action and proceed one step in the environment next_state, reward, done = env.step(action) next_action = agent.get_action(str(next_state)) # with sample \u0026lt;s,a,r,s\u0026#39;,a\u0026#39;\u0026gt;, agent learns new q function agent.learn(str(state), action, reward, str(next_state), next_action) state = next_state action = next_action # print q function of all states at screen env.print_value_all(agent.q_table) # if episode ends, then break if done: break       "});index.add({'id':60,'href':'/cs-gy-6613-spring-2020/docs/lectures/drl/','title':"Lecture 9 - Reinforcement Learning",'content':"Reinforcement Learning Different Approaches to solve known and unknown MDPs\nWe started looking at different agent behavior architectures starting from the planning agents where the model of the environment is known and with no interaction with it the agent improves its policy, using this model as well as problem solving and logical reasoning skills.\nWe then looked at agents that can plan by interacting with the environment still knowing the model - this was covered in the MDP chapter. We have seen that DP uses full width backups as every successor state and action is considered and evaluated using the known transition (environment dynamics) and reward functions. This can be dealt with for moderate size problems but even a single backup cant be feasible when we have very large state spaces like in the game of Go for example. So we definitely need to develop approaches that allow agents to\n optimally act in very large known MDPs or optimally act when we don\u0026rsquo;t know the MDP functions.  In this chapter we we outline the prediction and control methods that are the basic building blocks behind both problems.\nWe develop agents that can act in an initially unknown environment and learn via their interactions with it, gradually improving their policy. In the reinforcement learning problem setting, agents do not know essential elements of the MDP $\\mathcal M = \u0026lt;\\mathcal S, \\mathcal P, \\mathcal R, \\mathcal A, \\gamma\u0026gt;$ that were assumed as given in the previous section. This includes the transition function, $P^a_{ss^\\prime}$ and the reward function $\\mathcal R_s^a$ that are essential as we have seen previously to estimate the value function and optimize the policy.\nThe only way an agent can get information about these missing functions is through its experiences (states, actions, and rewards) in the environment—that is, the sequences of tuples ($S_t, A_t, R_{t+1}$). Provided that it can _learn_ such functions, RL can be posed as an MDP and many concepts we have already covered in the MDP chapter still apply.\nTo scale to large problems however, we also need to develop approaches that can learn both computation and space (memory) efficiency . We will go through algorithms that use DNNs to provide, in the form of approximations, the needed efficiency boost.\nDRL principle - we will cover it in the SARSA section.\nSuffice to say that exploring DRL algorithms is a very long journey as shown below - we will cover only three key algorithms: REINFORCE, SARSA and DQN that can be used as design patterns for the others. These algorithms were not invented in vacuum though. The reader must appreciate that these algorithms are instantiations of the so called model-free prediction and model-free control approaches to solving either unknown MDP problems (RL) or known MDP problems that are too large to apply the methods outlined in the MDP chapter.\nDRL algorithms - taxonomy and evolution\n Apart from the notes here that are largely based on David Silver\u0026rsquo;s (Deep Mind) course material and video lectures, the curious mind will be find additional resources:\n in the Richard Sutton\u0026rsquo;s book - David Silver\u0026rsquo;s slides and video lectures are based on this book. The code in Python of the book is here in the suggested book written by Google researchers as well as on OpenAI\u0026rsquo;s website. The chapters we covered is 1-4. You may also want to watch Andrew Ng\u0026rsquo;s, 2018 version of his ML class that includes MDP and RL lectures.   "});index.add({'id':61,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/','title':"Lecture 10 - Recurrent Neural Networks (RNN)",'content':""});index.add({'id':62,'href':'/cs-gy-6613-spring-2020/docs/lectures/nlp/','title':"Lecture 11/12 - Natural Language Processing",'content':""});index.add({'id':63,'href':'/cs-gy-6613-spring-2020/docs/lectures/nlp/nlp-intro/','title':"Introduction to NLP",'content':""});index.add({'id':64,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/introduction/','title':"Introduction to Recurrent Neural Networks (RNN)",'content':"Introduction to Recurrent Neural Networks (RNN) Sequences Data streams are everywhere in various applications. For example, weather station sensor data arrive in streams indexed by time, financial trading data and obviously reading comprehension - one can think of many others. We are interested to fit sequenced data with a model and therefore we need a hypothesis set, that is rich enough for sequential tasks. In the following we use $t$ as the index variable without this implying any time semantics.\nDynamical systems are such rich models where the recurrent state evolution can be represented as:\n$$\\mathbf{s}_t = f_t(\\mathbf{s}_{t-1}, \\mathbf{a}_t ; \\bm \\theta_t)$$\nwhere $\\bm s$ is the evolving state, $\\bm a$ is an external action or control and $\\bm \\theta$ is a set of parameters that specify the state evolution model $f$. This innocent looking equation can capture quite a lot of complexity.\n The state space which is the set of states can depend on $t$. The action space similarly can depend on $t$ Finally, the function that maps previous states and actions to a new state can also depend on $t$  So the dynamical system above has indeed offered a very profound modeling flexibility.\nRNN Architecture The RNN architecture is a constrained implementation of the above dynamical system:\n$$\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{x}_t ; \\bm \\theta)$$\nRNNs implement the same function (parametrized by $\\bm \\theta$) across the sequence $1:\\tau$. The state is latent and is denoted with $\\bm h$ to match the notation we used earlier for the DNN hidden layers. There is also no dependency on $t$ of the parameters $\\bm \\theta$ and this means that the network shares parameters across the sequence. We have seen parameter sharing in CNNs as well but if you recall the sharing was over the relatively small span of the filter. But the most striking difference between CNNs and RNNs is in recursion itself.\nRecursive state representation in RNNs\nThe weights $\\bm h$ in CNNs were not a function of previous weights and this means that they cannot remember previous hidden states in the classification or regression task they try to solve. This is perhaps the most distinguishing element of the RNN architecture - its ability to remember via the hidden state who is dimensioned according to the task at hand. There is a way using sliding windows to allow DNNs to remember past inputs as shown in the figure below for an NLP application.\nDNNs can create models from sequential data (such as the language modeling use case shown here). At each step $t$ the network with a sliding window span of $\\tau=3$ that acts as memory, will concatenate the word embeddings and use a hidden layer $\\bm h$ to predict the the next element in the sequence. However, notice that (a) the span is limited and fixed (b) words such as \u0026ldquo;the ground\u0026rdquo; will appear in multiple sliding windows forcing the network to learn two different patterns for this constituent (\u0026ldquo;in the ground\u0026rdquo;, \u0026ldquo;the ground there\u0026rdquo;).\nRNNs have a wide variety of architectures.\nFrom left to right, CNNs, Image Captioning, Sentiment Classification, Machine Translation, Multi-Object Tracking. RNNs are able to work with variable size input and output sequenced data.\nIn this course will suffice to go over just two to understand what they offer in terms of their representational capacity. One significant factor that separates the architectures is the way they perform the hidden state calculation at each $t$. This is shown in the next figure.\nDifferentiating Architectures (a) DNN, (b) Simple RNN, (c) LTSM, (d) GRU\n"});index.add({'id':65,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/simple-rnn/','title':"Simple RNNs and their Backpropagation",'content':"Simple RNN Simple RNN with recurrences between hidden units. This architecture can compute any computable function and therefore is a Universal Turing Machine.\nNotice how the path from input $\\bm x_{t-1}$ affects the label $\\bm y_{t}$ and also the conditional independence between $\\bm y$ given $\\bm x$. Please note that this is not a computational graph rather one way to represent the hidden state transfer between recurrences.\nForward Propagation This network maps the input sequence to a sequence of the same length and implements the following forward pass:\n$$\\bm a_t = \\bm W \\bm h _{t-1} + \\bm U \\bm x_t + \\bm b$$\n$$\\bm h_t = \\tanh(\\bm a_t)$$\n$$\\bm o_t = \\bm V \\bm h_t + \\bm c$$\n$$\\hat \\bm y_t = \\mathtt{softmax}(\\bm o_t)$$\n$$L(\\bm x_1, \\dots , \\bm x_{\\tau}, \\bm y_1, \\dots , \\bm y_{\\tau}) = D_{KL}[\\hat p_{data}(\\bm y | \\bm x) || p_{model}(\\bm y | \\bm x; \\bm w)]$$\n$$= - \\mathbb E_{\\bm y | \\bm x \\sim \\hat{p}_{data}} \\log p_{model}(\\bm y | \\bm x ; \\bm w) = - \\sum_t \\log p_{model}(y_t | \\bm x_1, \\dots, \\bm x_t ; \\bm w)$$\nNotice that RNNs can model very generic distributions $\\log p_{model}(\\bm x, \\bm y ; \\bm w)$. The simple RNN architecture above, effectively models the posterior distribution $\\log p_{model}(\\bm y | \\bm x ; \\bm w)$ and based on a conditional independence assumption it factorizes into $\\sum_t \\log p_{model}(y_t | \\bm x_1, \\dots, \\bm x_t ; \\bm w)$.\nNote that by connecting the $\\bm y_{t-1}$ to $\\bm h_t$ via a matrix e.g. $\\bm R$ we can avoid this simplifying assumption and be able to model an arbitrary distribution $\\log p_{model}(\\bm y | \\bm x ; \\bm w)$. In other words just like in the other DNN architectures, connectivity directly affects the representational capacity of the hypothesis set.\nIn many instances we have problems where it only matters the label $y_\\tau$ at the end of the sequence. Lets say that you are classifying speech or video inside the cabin of a car to detect the psychological state of the driver. The same architecture shown above can also represent such problems - the only difference is the only the $\\bm o_\\tau$, $L_\\tau$ and $y_\\tau$ will be considered.\nLets see an example to understand better the forward propagation equations.\nExample sentence as input to the RNN\nIn the figure above you have a hypothetical document (a sentence) that is broken into what in natural language processing called tokens. Lets say that a token is a word in this case. In the simpler case where we need a classification of the whole document, given that $\\tau=6$, we are going to receive at t=1, the first token $\\bm x_1$ and with an input hidden state $\\bm h_0 = 0$ we will calculate the forward equations for $\\bm h_1$, ignoring the output $\\bm o_1$ and repeat the unrolling when the next input $\\bm x_2$ comes in until we reach the end of sentence token $\\bm x_6$ which in this case will calculate the output $y_6$ and loss\n$$- \\log p_{model} (y_6|\\bm x_1, \\dots , \\bm x_6; \\bm w)$$\nwhere $\\bm w = \\{ \\bm W, \\bm U, \\bm V, \\bm b, \\bm c \\}$.\nWhere it gets interesting though is when we connect RNNs and the language models.\nRNN language model example - training ref\nIn the figure above we see a toy example where the vocabulary is [\u0026lsquo;h\u0026rsquo;,\u0026lsquo;e\u0026rsquo;,\u0026lsquo;l\u0026rsquo;,\u0026lsquo;o\u0026rsquo;]. where the tokens are single letters represented in the input with a one-hot encoded vector. Let us assume that the network is being trained with the sequence \u0026ldquo;hello\u0026rdquo;. The letters will come in one at a time, each letter going through the forward pass that produces at the output the $\\mathbf y_t$ that indicates which letter is expected to arrive next. You can see, since we are just started training, that this network is not predicting correctly - this will improve over time as the model is trained with more sequence permutations form our limited vocabulary. During inference we will use the language model to\nRNN language model example - generate the next token ref\nBack-Propagation Through Time (BPTT) Lets now see how the backward propagation would work.\nUnderstanding RNN memory through BPTT procedure\nBackpropagation is similar to that of feed-forward (FF) networks simply because the unrolled architecture resembles a FF one. But there is an important difference and we explain this using the above computational graph for the unrolled recurrences $t$ and $t-1$. During computation of the variable $\\bm h_t$ we use the value of the variable $\\bm h_{t-1}$ calculated in the previous recurrence. So when we apply the chain rule in the backward phase of BP, for all nodes that involve the such variables with recurrent dependencies, the end result is that _non local_ gradients from previous backpropagation steps ($t$ in the figure) appear. This is effectively why we say that simple RNNs feature _memory_. This is in contrast to the FF network case where during BP only local to each gate gradients where involved as we have seen in the the DNN chapter.\nThe key point to notice in the backpropagation in recurrence $t-1$ is the junction between $\\tanh$ and $\\bm V \\bm h_{t-1}$. This junction brings in the gradient $\\nabla_{\\bm h_{t-1}}L_t$ from the backpropagation of the $\\bm W h_{t-1}$ node in recurrence $t$ and just because its a junction, it is added to the backpropagated gradient from above in the current recurrence $t-1$ i.e.\n$$\\nabla_{\\bm h_{t-1}}L_{t-1} \\leftarrow \\nabla_{\\bm h_{t-1}}L_{t-1} + \\nabla_{\\bm h_{t-1}}L_t $$\nIan Goodfellow\u0026rsquo;s book section 10.2.2 provides the exact equations - please note that you need to know only the intuition behind computational graphs for RNNs. In practice BPTT is truncated to avoid having to do one full forward pass and one full reverse pass through the training dataset of an e.g. language model that is usually very large, to do a single gradient update.\n Minimal RNN language model code from Stanford CS231n ↕  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113  # see here for notation http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture10.pdf \u0026#34;\u0026#34;\u0026#34;Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)BSD License\u0026#34;\u0026#34;\u0026#34; import numpy as np # data I/O data = open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;).read() # should be simple plain text file chars = list(set(data)) data_size, vocab_size = len(data), len(chars) print \u0026#39;data has %dcharacters, %dunique.\u0026#39; % (data_size, vocab_size) char_to_ix = { ch:i for i,ch in enumerate(chars) } ix_to_char = { i:ch for i,ch in enumerate(chars) } # hyperparameters hidden_size = 100 # size of hidden layer of neurons seq_length = 25 # number of steps to unroll the RNN for learning_rate = 1e-1 # model parameters Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output bh = np.zeros((hidden_size, 1)) # hidden bias by = np.zeros((vocab_size, 1)) # output bias def lossFun(inputs, targets, hprev): \u0026#34;\u0026#34;\u0026#34;inputs,targets are both list of integers.hprev is Hx1 array of initial hidden statereturns the loss, gradients on model parameters, and last hidden state\u0026#34;\u0026#34;\u0026#34; xs, hs, ys, ps = {}, {}, {}, {} hs[-1] = np.copy(hprev) loss = 0 # forward pass for t in xrange(len(inputs)): xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation xs[t][inputs[t]] = 1 hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss) # backward pass: compute gradients going backwards dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) dbh, dby = np.zeros_like(bh), np.zeros_like(by) dhnext = np.zeros_like(hs[0]) for t in reversed(xrange(len(inputs))): dy = np.copy(ps[t]) dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here dWhy += np.dot(dy, hs[t].T) dby += dy dh = np.dot(Why.T, dy) + dhnext # backprop into h dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity dbh += dhraw dWxh += np.dot(dhraw, xs[t].T) dWhh += np.dot(dhraw, hs[t-1].T) dhnext = np.dot(Whh.T, dhraw) for dparam in [dWxh, dWhh, dWhy, dbh, dby]: np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1] def sample(h, seed_ix, n): \u0026#34;\u0026#34;\u0026#34;sample a sequence of integers from the model h is memory state, seed_ix is seed letter for first time step\u0026#34;\u0026#34;\u0026#34; x = np.zeros((vocab_size, 1)) x[seed_ix] = 1 ixes = [] for t in xrange(n): h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh) y = np.dot(Why, h) + by p = np.exp(y) / np.sum(np.exp(y)) ix = np.random.choice(range(vocab_size), p=p.ravel()) x = np.zeros((vocab_size, 1)) x[ix] = 1 ixes.append(ix) return ixes n, p = 0, 0 mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0 while True: # prepare inputs (we\u0026#39;re sweeping from left to right in steps seq_length long) if p+seq_length+1 \u0026gt;= len(data) or n == 0: hprev = np.zeros((hidden_size,1)) # reset RNN memory p = 0 # go from start of data inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # sample from the model now and then if n % 100 == 0: sample_ix = sample(hprev, inputs[0], 200) txt = \u0026#39;\u0026#39;.join(ix_to_char[ix] for ix in sample_ix) print \u0026#39;----\\n%s\\n----\u0026#39; % (txt, ) # forward seq_length characters through the net and fetch gradient loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev) smooth_loss = smooth_loss * 0.999 + loss * 0.001 if n % 100 == 0: print \u0026#39;iter %d, loss: %f\u0026#39; % (n, smooth_loss) # print progress # perform parameter update with Adagrad for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]): mem += dparam * dparam param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update p += seq_length # move data pointer n += 1 # iteration counter        Vanishing or exploding gradients In the figure below we have drafted a conceptual version of what is happening with recurrences over time. Its called an infinite impulse response filter for reasons that will be apparent shortly.\nInfinite Impulse Response (IIR) filter with weight $w$\nWith $D$ denoting a unit delay, the recurrence formula for this system is:\n$$h_t = w h_{t-1} + x_t$$\nwhere $w$is a weight (a scalar). Lets consider what happens when an impulse, $x_t = \\delta_t$ is fed at the input of this system with $w=-0.9$.\n$$h_0 = -0.9 h_{-1} + \\delta_0 = 1$$ $$h_1 = -0.9 h_{0} + \\delta_1 = -0.9$$ $$h_2 = -0.9 h_{1} + \\delta_2 = 0.81$$ $$h_3 = -0.9 h_{2} + \\delta_3 = -0.729$$\nWith $w=-0.9$, the h_t (called impulse response) follows a decaying exponential envelope while obviously with $w \u0026gt; 1.0$ it would follow an exponentially increasing envelope. Such recurrences if continue will result in vanishing or exploding responses long after the impulse showed up in the input $t=0$.\nIn a similar fashion, the RNN hidden state recurrence, in the backwards pass of backpropagation that extends from the $t=\\tau$ to $t=1$ can make the gradient, when $\\tau$ is large, either vanish or explode. Instead of a scalar $w$ we have matrices $\\bm W$ involved instead of $h$ we have gradients $\\nabla_{\\bm h_{t}}L_{t}$. This is discussed in this paper.\nUsing this primitive IIR filter as an example, we can see that the weight plays a crucial role in the impulse response. This is further discussed in the LSTM section.\n"});index.add({'id':66,'href':'/cs-gy-6613-spring-2020/docs/lectures/nlp/word2vec/','title':"Word Embeddings",'content':""});index.add({'id':67,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/lstm/','title':"The Long Short-Term Memory (LSTM) Cell Architecture",'content':"The Long Short-Term Memory (LSTM) Cell Architecture In the simple RNN we have seen the problem of exploding or vanishing gradients when the span of back-propagation is large (large $\\tau$). Using the conceptual IIR filter, that ultimately integrates the input signal, we have seen that in order to avoid an exploding or vanishing impulse response, we need to control $w$. This is exactly what is being done in an evolutionary RNN architectures that we will treat in this section called gated RNNs.\nThe way we control $w$ is to have another system produce it for the task at hand. In the best known gated RNN architecture, the IIR recurrence and everything that controls it, is contained in the LSTM cell. The LSTM cell adjusts $w$ depending on the input sequence context and this means that (a) there is an internal memory to the cell, we call this the cell state and (b) $w$ will fluctuate depending on $x$. We employ another hidden unit to learn the context and, based on that, set the right $w$. This unit is called the forget gate: since by making $w$ equal to zero, $h_t$ stops propagating aka it forgets the previous hidden state. We employ a couple of other gates as well: the input gate and the output gate as shown in the diagram below.\nLSTM Cell: The cell is divided into three areas: input (green), cell state (blue) and output (red). The $i$ index (see description below) has been supressed for clarity\nThe cell is divided into three areas: input (green), cell state (blue) and output (red)\nIn each area there is a corresponding gate (filled node) - these are the input gate, forget gate, output gate for the input, cell state and output regions respectively. The gates controls the flow of information that goes through these areas via element-wise multipliers. The two inputs to the cells are the concatenation of $\\bm x_t$ and $\\bm h_{t-1}$ and the cell state from the previous recurrence $\\bm s_{t-1}$.\nThe Cell State Starting at the heart of the LSTM cell, to describe the update we will use two indices: one for the unfolding sequence index $t$ and the other for the cell index $i$. We use the additional index to allow the current cell at step $t$ to use or forget inputs and hidden states from other cells.\n$$s_t(i) = f_t(i) s_{t-1}(i) + g_t(i) \\sigma \\Big( \\bm W^T(i) \\bm h_{t-1}(i) + \\bm U^T(i) \\bm x_t(i) + \\bm b(i) \\Big)$$\nThe parameters $\\theta_{in} = \\{ \\bm W, \\bm U, \\bm b \\}$ are the recurrent weights, input weights and bias at the input of the LSTM cell.\nThe forget gate calculates the forgetting factor,\n$$f_t(i) =\\sigma \\Big( \\bm W_f^T(i) \\bm h_{t-1}(i) + \\bm U_f^T(i) \\bm x_t(i) + \\bm b_f(i) \\Big) $$\nInput The input gate protects the cell state contents from perturbations by irrelevant to the context inputs. Quantitatively, input gate calculates the factor,\n$$g_t(i) =\\sigma \\Big( \\bm W_g^T(i) \\bm h_{t-1}(i) + \\bm U_g^T(i) \\bm x_t(i) + \\bm b_g(i) \\Big) $$\nThe gate with its sigmoid function adjusts the value of each element produced by the input neural network.\nOutput The output gate protects the subsequent cells from perturbations by irrelevant to their context cell state. Quantitatively,\n$$h_t(i) = q_t(i) \\tanh(s_t(i))$$\nwhere $q_t(i)$ is the output factor\n$$q_t(i) =\\sigma \\Big( \\bm W_o^T(i) \\bm h_{t-1}(i) + \\bm U_o^T(i) \\bm x_t(i) + \\bm b_o(i) \\Big) $$\nNotice that if you omit the gates we are back to the simple RNN architecture. You can expect backpropagation to work similarly in LSTM albeit with more complicated expressions. Some more diagraming and annimation dont hurt to understand LSTMs. See 1 and 2.\n"});index.add({'id':68,'href':'/cs-gy-6613-spring-2020/docs/projects/','title':"Projects",'content':"The following projects needs to be delivered by the deadlines.\n Surface Type Classification - Due 2/23/2020 11:59pm Lifelong Learning - Robotic Vision - Due 3/29/2020 11:59pm Semantic Code Search - Due 5/10/2020 11:59pm Going Back to Work - Due 5/10/2020 11:59pm Explainable COVID-19 Pneumonia - Due 5/3/2020 11.59am  "});index.add({'id':69,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/transfer-learning/','title':"Transfer Learning",'content':""});index.add({'id':70,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/','title':"Background - Math for ML",'content':""});index.add({'id':71,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/','title':"Background - ML Frameworks",'content':"The Zillow App The Zillow app is based on the end to end machine learning example in Chapter 2 of Geron\u0026rsquo;s book. We can go through this end to end example during a recitation.\nAlthough the ML project checklist provided in Appendix B of Garon\u0026rsquo;s book is extensive (we will go through this list in the lecture as we go through your first ML application) for now focus on the eight steps as shown below.\nSteps in workflow (from here)\n As discussed the data pipeline is responsible for providing the training datasets if the aim is to train (or retrain) a model. For the purposes of this lecture we assume that we deal with small data aka. data fit in the memory of today\u0026rsquo;s typical workstations/laptops (\u0026lt; 16 GB). Therefore you will be given a URL from where compressed data files can be downloaded. For structured data, these files when uncompressed will be typically CSV. For unstructured they will be in various formats depending on the use case. In most instances, ML frameworks that implement training will require certain transformations to optimize the format for the framework at hand (see TFrecords in tensorflow).\n Appendix B of Garon\u0026rsquo;s book goes into more detail on the steps suggested to be followed in an end to end ML project.\n Key Questions  Is the dataset appropriate for training?   Any unexpected ranges, any range heterogeneity, any clipping? Do we face long-tails? What options do we have to glean the data?\n  What will happen if we remove the following line from the split_train_set function?\n1  shuffled_indices = np.random.permutation(len(data))     "});index.add({'id':72,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/drl-model/vae/','title':"Generative Modeling and Continuous Variational Auto Encoders (VAE)",'content':""});index.add({'id':73,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/agents-slides/','title':"Agents Slides",'content':""});index.add({'id':74,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/ai-pipelines-slides/','title':"Ai Pipelines Slides",'content':""});index.add({'id':75,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/systems-approach-slides/','title':"Systems Approach Slides",'content':""});index.add({'id':76,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/','title':"Drafts",'content':""});index.add({'id':77,'href':'/cs-gy-6613-spring-2020/docs/lectures/learning-problem/learning-problem-slides/','title':"Learning Problem Slides",'content':""});index.add({'id':78,'href':'/cs-gy-6613-spring-2020/docs/lectures/mdp/mdp-slides/','title':"Mdp Slides",'content':"Lecture 8 Gurudutt Hosangadi, Bell Labs, NJ April 6th, 2020  PLAN  Review Sequential Decisions and MDP POMDPs Reinforcement Learning   Part 1/4: REVIEW Note: we begin with review\n Agents An agent is anything that can perceive an environment through sensors and then act on the environment. An agent therefore maps percepts into actions.\nAgent Types $4$ main categories depending on how percepts are mapped to actions:\n Simple reflex agent: selects an action based on current percept Model based reflex agent: uses current percept and a model of the world to take action Goal based agent: uses current perceptive, a model of the world and set of goals it is trying to achieve to take action Utility based agent: uses current perceptive, a model of the world, set of goals it is trying to achieve and utility of outcomes to take action   Example: Mars lander Suppose that the mars lander needs to pick up 1 sample of each different looking rock. There could be obstructions along the way.\nCuriocity in the surface of Mars\nWhat would be the outcome of the different types of agents?\n Mars Lander - continued Simplex reflex agent  if the Lander found a rock in a specific place it needed to collect then it would collect it.   if it found the same rock in a different place it would still pick it up as it doesn\u0026rsquo;t take into account that it already picked it up.  Model based reflex agent  this time the Lander after picking up its first sample, stores this information in its model so when it comes across the second same sample it passes it by.   Mars lander - continued Assume that the Lander comes across an obstruction.\nGoal-based agent  it is not clear which path will be taken by the agent which may take a longer path and still reach the goal of picking up the rock.   Mars lander - continued Utility based agent  will pick the path with the best utility.   Environments Fully versus Partially observable  If it is possible to determine the complete state of the environment at each time point from the percepts then it is fully observable Otherwise it is only partially observable. We have to take into consideration the point of view of the agent since the agent may have limited perception capabilities.  Deterministic versus Stochastic  If the next state of the environment is completely determined by the current state, and the actions of the agent, then the environment is deterministic Otherwise it is non-deterministic.   Environments - continued Episodic versus Sequential:  if the choice of current actions is not dependent on previous actions then the enviroment is episodic. otherwise it is sequential  Car driving  partially observable   stochastic   sequential   Handling uncertainty  Uncertainty arises from stochastic nature of environment or due to partial observability Agents handle this by keeping track of a belief state - a representation of the set of possible states that environment might be in Probability theory allows us to summarize uncertainty as it allows you to characterize the degree of belief in terms of probability of a belief state. Bayes theorem:  $P(A | B) = \\frac{P(B|A)P(A)}{P(B)}$.\n You can think of this as   $P_{posterior} = \\frac{likelihood \\cdot prior}{evidence}$\n Handling uncertainty - example  Suppose you have a robot who is uncertain if a door is open or closed Robot obtains evidence (or measurement) $e$. Then the posterior probability of the door being open given evidence $e$ is given by $P(open/e) = \\frac{P(e/open)P(open)}{P(e)}$ $P(e/open)$ easier to estimate as you can look at the past observations and estimate the probability density function.   Decision Theory  Good decisions must be distinguished from good outcomes. To decide, an agent must have preferences between the possible outcomes Preferences are represented by utility(or usefulness) i.e. an agent will prefer outcomes with higher utility. Probability theory describes what the agent should believe on the basis of evidence. Utility theory describes what the agent wants Decision theory puts the two together to describe what the agent should do i.e $ = $ Utility Theory + Probability Theory   Decision Networks  Bayseian networks are used to represent knowledge in an uncertain domain Decision Networks extends Bayesian networks by incorporating actions and utilities.    Decision Networks - continued  Utility function (denoted U)  Quantifies how we value outcomes   Agent\u0026rsquo;s desirability of a state or outcome is captured by an utility function. In Reinforcement Learning (RL) we come across value function which is an example of a utility function Rational agent will choose an action $a^{*}$ that will maximize the expected utility. Note that we are usually working with estimates of the true expected utility.  This is a one-shot or episodic decision.     PART 2/4: Sequential Decisions and MDP  A Sequential Decision Problem: Maze  Robot is at the \u0026ldquo;START\u0026rdquo; Agent commands robot with actions : UP($\\uparrow$),DOWN($\\downarrow$),LEFT($\\leftarrow$),RIGHT($\\rightarrow$) and robot follows exactly Agent knows where it is i.e. environment is fully observable state: where Robot is for e.g. state $S_{42}$ is if robot is in the square with red oval. reward or punishment received when a state is reached   Maze - continued  Question: is there any uncertainty?  No - since the actions executed are same as action commands issued and the environment is fully observable     Utility of a sequence of states is given by  $$U(s_0,s_1,\u0026hellip;,s_T)=R(s_0)+R(s_1)+\u0026hellip;+R(s_T)$$\n Question: Find sequence of actions from current state to goal (green oval) that maximizes utility?   MAZE with no uncertainty: Utility  What we are looking for is a policy which recommends the action to be taken when in a given state   MAZE with no uncertainty: Policy  Policy: $\\pi(s) = a$ i.e. $\\pi$ function maps state $s$ to action $a$.   Utility $U(s)$ of state $s$ is the sum of discounted rewards of the sequence of states starting at $s$ generated by using the policy $\\pi$ i.e.  $$U^{\\pi}(s) = R(s) + \\gamma R(s_1) + \\gamma^{2} R(s_2) + \u0026hellip;$$\n MAZE with no uncertainty: Policy  Optimal policy $\\pi$ policy that yields the highest expected utility for the sequence of states generated by $\\pi^*$.    For the maze, the optimal policy tells us which action to take so that we are closer to the goal\n $\\pi^*(s_{41})=$ ??? $\\pi^*(s_{32})=$ ??? $\\pi^*(s_{11})=$ ???     Markov Decision Process (MDP)  Imagine that the maze environment is stochastic yet fully observable. Due to uncertainty, an action causes transition from state to another state with some probability. There is no dependence on previous states. We now have a sequential decision problem for a fully observable, stochastic environment with Markovian transition model and additive rewards consisting of:  a set of states $S$. State at time $t$ is $s_t$ actions $A$. Action at time $t$ is $a_t$. transition model describing outcome of each action in each state $P( s_{t+1} | s_t,a_t)$ reward function $r_t=R(s_t)$     MDP Transition Model  Transition Model Graph:  Each node is a state. Each edge is the probability of transition     Utility for MDP  Since we have stochastic environment, we need to take into account the transition probability matrix Utility of a state is the immediate reward of the state plus the expected discounted utility of the next state due to the action taken Bellman\u0026rsquo;s Equations: if we choose an action $a$ then  $U(s) = R(s) + \\gamma \\sum_{s^{'}} P(s^{'}| s,a)U(s^{'})$\n Utility for MDP - continued  Suppose robot is in state $S_{33}$ and the action taken is \u0026ldquo;RIGHT\u0026rdquo;. Also assume $\\gamma = 1$ We want to compute the utility of this state: $$ U(S_{33}) = R(S_{33}) + \\gamma (P(S_{43} | S_{33}, \\rightarrow) U(S_{43}) + P(S_{33} | S_{33}, \\rightarrow) U(S_{33}) + P(S_{32} | S_{33}, \\rightarrow) U(S_{32}))$$ Substituting we get:  $$U(S_{33}) = R(S_{33}) + \\gamma ( (0.8 \\times U(S_{43})) + (0.1 \\times U(S_{33})) + (0.1 \\times U(S_{23})))$$\n Policy for MDP   If we choose action $a$ that maximizes future rewards, $U(s)$ is the maximum we can get over all possible choices of actions and is represented as $U^{*}(s)$.\n  We can write this as $$U^*(s) = R(s) + \\gamma \\underset{a}{ \\max} (\\sum_{s^{'}} P(s^{'}| s,a)U(s\u0026rsquo;))$$\n  The optimal policy (which recommends $a$ that maximizes U) is given by:\n  $$\\pi^{*}(s) = \\underset{a}{\\arg \\max}(\\sum_{s^{'}} P(s^{'}| s,a)U^{*}(s^{'}))$$\n Can the above $2$ be solved directly?  The set of $|S|$ equations for $U^*(s)$ cannot be solved directly because they are non-linear due the presence of \u0026lsquo;max\u0026rsquo; function. The set of $|S|$ equations for $\\pi^*(s)$ cannot be solved directly as it is dependent on unknown $U^*(s)$.     Optimal Policy for MDP  Value Iteration  To solve the non-linear equations for $U^{*}(s)$ we use an iterative approach. Steps:  Initialize estimates for the utilities of states with arbitrary values: $U(s) \\leftarrow 0 \\forall s \\epsilon S$ Next use the iteration step below which is also called Bellman Update:    $$U_{t+1}(s) \\leftarrow R(s) + \\gamma \\underset{a}{ \\max} \\left[ \\sum_{s^{'}} P(s^{'}| s,a) U_t(s^{'}) \\right] \\forall s \\epsilon S$$\nThis step is repeated and updated   Let us apply this to the maze example. Assume that $\\gamma = 1$  Initialize value estimates to $0$\n Value Iteration  Next we want to apply Bellman Update: $$U_{t+1}(s) \\leftarrow R(s) + \\gamma \\max_{a} \\left[\\sum_{s^\\prime} P(s^\\prime | s,a)U_t(s^\\prime) \\right] \\forall s \\epsilon S$$ Since we are taking $\\max$ we only need to consider states whose next states have a positive utility value. For the remaining states, the utility is equal to the immediate reward in the first iteration.   Value Iteration (t=0) $$ U_{t+1}(S_{33}) = R(S_{33}) + \\gamma \\max_a \\left[\\sum_{s^{'}} P(s^{'}| S_{33},a)U(s^{'}) \\right] \\forall s \\in S $$\n$$ U_{t+1}(S_{33}) = -0.04 + \\max_a \\left[ \\sum_{s\u0026rsquo;} P(s'| S_{33},\\uparrow) U_t(s\u0026rsquo;), \\sum_{s\u0026rsquo;} P(s'| S_{33},\\downarrow)U_t(s\u0026rsquo;), \\sum_{s\u0026rsquo;} P(s'| S_{33},\\rightarrow) U_t(s\u0026rsquo;), \\sum_{s\u0026rsquo;} P(s'| S_{33}, \\leftarrow)U_t(s\u0026rsquo;) \\right]$$\n$$U_{t+1}(S_{33}) = -0.04 + \\sum_{s^{'}} P(s^{'}| S_{33},\\rightarrow) U_t(s^\\prime) $$\n$$U_{t+1}(S_{33}) = -0.04 + P(S_{43}|S_{33},\\rightarrow)U(S_{43})+P(S_{33}|S_{33},\\rightarrow)U(S_{33})+P(S_{32}|S_{33},\\rightarrow)U_t(S_{32}) $$\n$$U_{t+1}(S_{33}) = -0.04 + 0.8 \\times 1 + 0.1 \\times 0 + 0.1 \\times 0 = 0.76 $$\n Value Iteration (t=1) (A) Initial utility estimates for iteration 2. (B) States with next state positive utility\n$$U_{t+1}(S_{33}) = -0.04 + P(S_{43}|S_{33},\\rightarrow)U_t(S_{43})+P(S_{33}|S_{33},\\rightarrow)U_t(S_{33}) +P(S_{32}|S_{33},\\rightarrow)U_t(S_{32}) $$\n$$U_{t+1}(S_{33}) = -0.04 + 0.8 \\times 1 + 0.1 \\times 0.76 + 0.1 \\times 0 = 0.836$$\n$$U_{t+1}(S_{23}) = -0.04 + P(S_{33}|S_{23},\\rightarrow)U_t(S_{23})+P(S_{23}|S_{23},\\rightarrow)U_t(S_{23}) = -0.04 + 0.8 \\times 0.76 = 0.568$$\n$$U_{t+1}(S_{32}) = -0.04 + P(S_{33}|S_{32},\\uparrow)U_t(S_{33})+P(S_{42}|S_{32},\\uparrow)U_t(S_{42}) +P(S_{32}|S_{32},\\uparrow)U_t(S_{32})$$ $$U_{t+1}(S_{32}) = -0.04 + 0.8 \\times 0.76 + 0.1 \\times -1 + 0.1 \\times 0= 0.468$$\n Value Iteration (t=2) (A)Initial utility estimates for iteration 3. (B) States with next state positive utility\n Information propagates outward from terminal states and eventually all states have correct value estimates Notice that $S_{32}$ has a lower utility compared to $S_{23}$ due to the red oval state with negative reward next to $S_{32}$   Value Iteration - Convergence  Rate of convergence depends on the maximum reward value and more importantly on the discount factor $\\gamma$. The policy that we get from coarse estimates is close to the optimal policy long before $U$ has converged. This means that after a reasonable number of iterations, we could use: $$\\pi(s) = \\argmax_a \\left[ \\sum_{s^{'}} P(s^{'}| s,a)U_{est}(s^{'}) \\right]$$ Note that this is a form of greedy policy.  Convergence of utility for the maze problem (Norvig chap 17)\n For the maze problem, convergence is reached within 5 to 10 iterations   Policy Iteration   Alternates between two steps:\n Policy evaluation: given a policy, find the utility of states Policy improvement: given the utility estimates so far, find the best policy    The steps are as follows:\n Compute utility/value of the policy $U^{\\pi}$ Update $\\pi$ to be a greedy policy w.r.t. $U^{\\pi}$: $$\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s^\\prime} P(s^\\prime|s,a)U^{\\pi}(s^\\prime)$$ If the policy changed then return to step $1$    Policy improves each step and converges to the optimal policy $\\pi^{*}$\n   Policy Iteration for Grid World  Let us consider another grid world example.    The terminal states are shaded. The reward is $-1$ on all transitions until the terminal states are reached. The non-terminal states are $S_1,S_2,\u0026hellip;,S_{14}$.\n  We begin with random values (or utilities) and random policy $\\pi$\n  Initial values and policy for policy iteration\n Policy Iteration (Step 1)  Find value function based on initial random policy:  $$U(s) = R(s) + \\sum_{s^\\prime} P(s^\\prime| s,a)U(s^\\prime)$$\n$$U(S_{1}) = -1 + \\frac{1}{4}U(S_{1}) + \\frac{1}{4}U(S_{2}) + \\frac{1}{4}U(S_{5}) = -1$$ $$\\ldots$$ $$ U(S_{9}) = -1 + \\frac{1}{4}U(S_{8}) + \\frac{1}{4}U(S_{5}) + \\frac{1}{4}U(S_{13}) + \\frac{1}{4}U(S_{10}) = -1$$ $$ \\ldots $$\n The result is as shown below:   Policy Iteration (step 2)  Next we compute the policy:  $$ \\pi(S_{4}) = \\underset{a}{max}[\\frac{1}{4}U(S_{term})| \\uparrow, \\frac{1}{4}U(S_5)| \\rightarrow, \\frac{1}{4}U(S_4)| \\leftarrow, \\frac{1}{4}U(S_8), \\downarrow] = \\uparrow $$\n$$\\pi(S_{6}) = \\underset{a}{max}[\\frac{1}{4}U(S_2)| \\uparrow, \\frac{1}{4}U(S_7)| \\rightarrow, \\frac{1}{4}U(S_5)| \\leftarrow, \\frac{1}{4}U(S_{10}), \\downarrow ] = \\mathtt{random\\ policy} $$\n The result is shown below for $k=2$.   PART 3/4: POMDPs  Partially Observable MDPs  We considered \u0026ldquo;uncertainty\u0026rdquo; in the action outcome previously. Now, the environment is partially observable. We now deal with a belief state which is the agent\u0026rsquo;s current belief about the state that it is in.   POMDP Parameters  The MDP parameters we listed previously continue to hold for POMDP:  a set of states $S$. State at time $t$ is $s_t$ actions $A$. Action at time $t$ is $a_t$. transition model describing outcome of each action in each state $P( s_{t+1} | s_t, a_t)$ reward function $r_t=R(s_t)$   Additional POMDP parameters:  initial belief of state $s$: $b(s)=P(s)$ if $b(s)$ was the previous belief state, agent does an action $a$ then perceives evidence $e$ then the new belief state is given by: $$b^\\prime(s^\\prime) = P(s^\\prime | e,a,b)$$ observation probability: $P(e|s^{'},a)$   The belief state $b$ also satisfies the Markov property   POMDP versus other models source\n POMDP Example  We want to find the optimal policy\u0026hellip;i.e. what is the best action the person should take?   POMDP Example - Transition Probabilities  The \u0026ldquo;Listen\u0026rdquo; action does not change the tiger location     $P(s^{'}$| $s, Listen)$ TL TR     TL 1.0 0   TR 0 1.0     The \u0026ldquo;open-left\u0026rdquo; or \u0026ldquo;open-right\u0026rdquo; action resets the problem in which case the tiger can be on the left or right with equal probability     $P(s^{'}$| $s, open-right)$ TL TR     TL 0.5 0   TR 0 0.5       $P(s^{'}$ | $s, open-left)$ TL TR     TL 0.5 0   TR 0 0.5     POMDP Example - Observation Probabilities  Only the \u0026ldquo;Listen\u0026rdquo; action is informative     $P(e$ | $s, Listen)$ TL TR     TL 0.85 0.15   TR 0.15 0.85     Any observation without the \u0026ldquo;listen\u0026rdquo; action is uninformative     $P(e$ | $s, open-right)$ TL TR     TL 0.5 0   TR 0 0.5       $P(e$ | $s, open-left)$ TL TR     TL 0.5 0   TR 0 0.5     POMDP Example - Immediate Rewards  \u0026ldquo;Listen\u0026rdquo; action results in a small penalty     $R(s)$ | $Listen$      TL -1   TR -1     Opening the wrong door results in large penalty     $R(s)$ | $open-left$      TL -100   TR +10       $R(s)$ | $open-right$      TL +10   TR -100    Belief State Space  b(left) versus b(right)   POMDP as a Belief-state MDP  Solving a POMDP on a physical state space reduces to solving an MDP on the corresponding belief-state space The resulting MDP has a high dimensional continuous(typically in real world problems) belief state space which makes it more difficult to solve Approach to solving this:  Each policy is a plan conditioned on belief $b$ Each conditional plan is a hyperplane Optimal policy then is the conditional plan with the highest expected utility The optimal action depends only on the agents\u0026rsquo;s current belief state. That is, the optimal policy $\\pi^{*}(b)$ maps from belief states to actions. The decision cycle in this case would comprise of the following $3$ steps:  Given the current belief state, execute the action $a=\\pi^{*}(b)$ Receive percept $e$ Set the current belief state to $b^{'}(s^{'})$ given by $$b^{'}(s^{'}) = \\alpha P(e|s^{'}) \\sum_{s} P(s^{'}|s,a)b(s)$$       Solving POMDP  The value iteration approach for POMDP looks something like this:  $$U_{t+1}(b) \\leftarrow \\max_{a}[ \\sum_s b(s)R(s,a) +\\gamma \\sum_eP(e|b,a)U(\\tau(e,b,a)]$$\nwhere $\\tau(e,b,a)$ is the transition function for the belief state.\n This is in general very hard to solve as it is a continuous space MDP Instead one resorts to exploiting special properties in terms of  Policy Tree Piecewise linear and convex property of the value function     Solving the tiger problem - 1-step horizon  Suppose that you were told the $b(left) = \\rho = 0.5$ i.e. tiger could be either on the left or right with equal probability. You are told that you have only 1 chance to take an action, what would that be and why?  The Tiger Problem\n Solving the tiger problem - 1-step horizon  Determine expected utility for each possible action for different belief distributions     action expected utility for $\\rho=0.5$ expected utility for $\\rho=0.4$     LEFT $0.5 \\times -100 + 0.5 \\times 10 = -45$ $0.4 \\times -100 + 0.6 \\times 10 = -36$   RIGHT $-45$ $0.6 \\times -100 + 0.4 \\times 10 = -56$   LISTEN $-1$ $-1$     For the above cases, we would pick \u0026ldquo;listen\u0026rdquo; as it has the highest expected utility How low should $\\rho$ go so that the utility of picking \u0026ldquo;left\u0026rdquo; is better than picking \u0026ldquo;listen\u0026rdquo;  Find $x \\ni \\rho \\times -100 + (1-\\rho) \\times 10 \\lt -1$ Solving we get $\\rho \\lt 0.1$. This means that if that if $0 \\lt b(left) \\lt 0.1$ then choose \u0026ldquo;left. This range is called the belief interval for which we would select \u0026ldquo;left\u0026rdquo;. Based on the above analysis, the optimal 1 step policy is as below     Solving the tiger problem - t-step horrizon  The value function of POMDPs can be represented as max of linear segments   How about if you were given $2$ chances? i.e. $t=2$ and $b(left)=0.5$.  It turns out that the optimal policy for the first step is to always \u0026ldquo;listen\u0026rdquo;. The reason is that if you opened the door on the first step  the tiger would be randomly placed behind one of the doors and the agent\u0026rsquo;s belief state would be reset to $(0.5, 0.5)$. The agent would be left with no information about the tiger\u0026rsquo;s location and with one action remaining.       Solving the tiger problem - t-step horrizon  PART 4/4: Reinforcement Learning (RL) NOTE: Reinforcement Learning and Deep RL (DRL) are covered in Lecture 9.\n Introduction  Reinforcement learning is concerned with solving sequential decision problems. Many real world problems fall into this category e.g. playing video games, driving, robotic control etc. Reinforcement learning might be considered to encompass all of AI: an agent is placed in an environment and must learn to behave therein The goal of reinforcement learning is to use observed rewards to learn the optimal (or close to optimal) policy for the environment. So far we have been looking at solving sequential decision making problems but we have assumed a complete model of the environment and the reward function Can we learn directly from experiences in the world?  Must receive feedback for good/bad experiences Called rewards or reinforcement One assumption that we make is that the reward input is known i.e. we know that a particular sensory input corresponds to reward     Learning  An agent is learning if it improves its performance on future tasks after making observations about the world. What is learned:  mapping from state to action utility information indicating desirability of states action-value information about the desirability of actions goals that describe states whose achievement maximize agent\u0026rsquo;s utility   Feeback types used that determine three types of learning  observes patterns in input without explicit feedback - unsupervised learning reinforcements i.e. rewards or punishments - reinforcement learning observes example input-output pairs and learns the mapping functions - supervised learning     Utility, Value, Action-Value, Action-Utility, Q functions  Utility function: $U^{\\pi}(s)$ provide expected utility or return or reward of a state by executing a given policy $\\pi$ Value function: $V^{\\pi}=U^{\\pi}(s)$ Action-utility function: $U^{\\pi}(s,a)$ gives the expected utility by taking an action $a$ while in state $s$ Action-value function: $V^{\\pi}(s,a)=U^{\\pi}(s,a)$ Q-function: $Q^{\\pi}(s,a)=V^{\\pi}(s,a)=U^{\\pi}(s,a)$ You get the value function by taking the expection of the action-value function over the set of actions i.e. $V^{\\pi}(s) = \\underset{a}{E}[Q(s,a)]$   Reinforcement Learning  Reinforcement learning is learning what to do i.e. how to map situations to actions so as to maximize a numerical reward signal. The learner must discover which actions yield the most reward by trying them. These two characteristics: trial-and-error search and delayed reward are the two most important distinguishing features of reinforcement learning. There is a feedback control loop where agent and environment exchange signals while the agent tries to maximize the rewards or objective. Signal exchanged at any time $t$ is $(s_t,a_t,r_t)$ which correspond to the state, action and reward at time $t$. This tuple is called an experience.   Reinforcement Learning as MDP  Reinforcement learning can be formulated as an MDP with the following parameters:  transition function ( $ P(s_{t+1} | s_t,a_t) $ ) captures how the enviroment transitions from one state to the next and is formulated as MDP reward function ( $ R(s_t,a_t,a_{t+1}) $ ) set of actions $ A $ set of states $ S $   One important assumption in the above formulation is that the agent does not have access to the transition or reward function Functions to be learning in RL  policy $\\pi$ value function $V^{\\pi}$ or action value function $Q^{\\pi}(s,a)$ environment model $P(s^{'} | s,a)$     Deep RL  use neural networks as function approximators to learn the functions Policy-based methods  learn policy $\\pi$ to maximize objective PROS  general class of optimization methods any type of actions: discrete, continous or a mix guaranteeed to converge(locally) for e.g. via Poilcy Gradient Algorithm   CONS  high variance sample inefficient       Deep RL - Value Based Methods  agent learns either $V^{\\pi}(s)$ or $Q^{\\pi}$ uses the learnt function(s) to generate the policy for e.g. greedy policy generally $Q^{\\pi}(s,a)$ is preferred as agent can select the action when in a given state to maximize the objective PROS:  more sample efficient than policy based algorithms   CONS  no guarantee of convergence to optimal policy most methods are for discrete action spaces though recently QT-OPT has been proposed which can handle continuous spaces     Deep RL - Model Based Methods  agent learns a model of the environment dynamics using this model, agent can imagine or predict what will happen if a set of actions are taken for few time steps without actually changing the environment Based on these predictions, agent can figure out the best actions PROS:  gives agent foresight tend to require fewer samples   CONS:  learning the model can be difficult as typically real world environments can have a large state and action space predictions depend on the accuracy of the model     Taxonomy of RL algorithms  Off policy versus on policy  On policy  e.g. SARSA agent learns on the policy i.e. training data generated from the current poicy is used in other words agent is learning the value of the policy that is being followed after the agent is trained, data is discarded and the iterated policy is used. sample inefficent due to the discarding of data but memory efficient   Off policy  e.g. Q-learning any data collected can be used for training the value of a policy that is different from the one being followed is being learnt more memory may be required to store data     Deep Learning  Neural networks learn functions $f_{\\theta}(x)$ which map input $x$ to output $y$. The weights of the neural network are represented by $\\theta$.  if $y=f(x)$ then learnt $f_{\\theta}(x)$ is the estimate for $f(x)$ Loss function $L(f_{\\theta}(x),y)$ captures the difference between the target $y$ and the predicted network output $f_{\\theta}(x)$. This loss function needs to be minimized. generally we have training data samples which are independent and identically distributed (iid)   Changing the weights will corresponding to different mapping functions. Increasing number of nodes in a layer or layers allows learning of more complex functions In reinforcement learning:  neither $x$ or $y$ are known in advance instead these values are obtained through agent interactions with environment - where it observes states and rewards reward functions are the main source of feedback and the rewards are quite sparse since current state and actions that an agent takes affect the future states, the iid assumption between samples for neural network training no longer holds and this affects the rate of convergence.     Case Study - World Models  Goal is to learn from pixels and control a car on a racing track  source: https://worldmodels.github.io/   Case Study - World Models - continued  Model based reinforcement learning vision + memory + control  source: https://worldmodels.github.io/   Case Study - World Models - continued  Case Study - World Models - continued \n THANK YOU!!!! "});index.add({'id':79,'href':'/cs-gy-6613-spring-2020/categories/','title':"Categories",'content':""});index.add({'id':80,'href':'/cs-gy-6613-spring-2020/','title':"CS-GY-6613 Artificial Intelligence - Spring 2020",'content':"Welcome to CS-GY-6613 ! Logistics Time/location: Brooklyn Campus, Mon 6.00 PM - 8.30 PM at RGSH 315.\nCommunication: We will use Slack for all communications: announcements and questions related to lectures, assignments, and projects. All registered students with NYU email addresses can click here to join - link expires after 30 days.\nInstructor Pantelis Monogioudis, Ph.D (Bell Labs) Head of Applied ML Research Group Murray Hill, NJ\nTeaching Assistants TA\u0026rsquo;s contact will be announced as soon as I receive confirmation from HR that were hired. I am targeting two TAs for this class.\nWhat is this course about This course is all about the algorithms and methods that will enable agents that exhibit forms of intelligence and autonomy.\nGrading  Final (30%) Midterm (30%) Projects (40%)  "});index.add({'id':81,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/decision-trees/','title':"Decision Trees",'content':"Decision Trees Continuing our path into non-parametric methods, the decision tree is one of the most popular ML algorithms. Its popularity stems also from yet another attribute that is becoming very important in the application of ML/AI in mission critical industries such as health: its ability to offer interpretable results and be visualized easily.\n Note: The material below is due to (a) \u0026ldquo;ML with Random Forests and Decision Trees\u0026rdquo; by Scott Hartshorn, (b) Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning by Criminisi et.a.l.\n Introduction A Decision Tree is simply a step by step process to go through to decide a category something belongs to - in the case of classification. They are non-parametric models because they dont use a predetermined set of parameters as in parametric models - rather the tree fits the data very closely and often overfits using as many parameters are required during training.\nFor example, let’s say that you had a basket of fruit in front of you, and you were trying to teach someone who had never seen these types of fruit before how to tell them apart. How could you do it? The answer is shown pictorially below.\nA decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value).\nIf your decision tree is good, you can now pick up an unknown piece of fruit and follow the flow chart to classify it. If your decision tree is bad, you can go down the wrong path and put something in the wrong category. For instance, if you didn’t know yellow apples existed when you built your decision tree, you might have assumed that all yellow fruit are either bananas or plantains.\nIn what follows, we focus on a dataset with $m=88$ and 4 labels: Apples, Oranges, Bananas, Grapefruit. Each example has multiple features: color, width and length.\n   Fruit Colors     Apples Red, Green, or Yellow   Oranges Orange   Bananas Yellow or Green   Grapefruit  Orange or Yellow    Fruit Dataset\nIf we are to draw separation lines on feature space of length ($x_1$) and width ($x_2$) without using an ML algorithm but by hand, we probably would come up with the picture below.\nDraw by hand partition\nNow, lets try to solve the same problem using an algorithm bearing in mind that many real-life data sets might have dozens or hundreds of different features. CART algorithm One of the most popular algorithms that implement decision trees is the Classification and Regression Tree (CART) algorithm.\n NOTE: scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\n At its heart, the algorithm implements a recursive binary partitioning of the input feature space. The feature space in the example above is $\\mathbf{x} = (x_1, x_2, x_3)^T$ denoting length, width and color. Given a training dataset as usual $D={(\\mathbf x_i, y_i}$ for $i={1, \\dots m}$, we need to come up with an close to optimal partitioning for the generalization error.\nWe start at a root node that corresponds to the whole feature space (no partition) and design a test that is in its simplest form a conditional statement against a feature (a comparison if you like). Depending on the binary outcome of the test (either the input examples will satisfy the condition or not) we produce the corresponding child nodes each inheriting a subset of the input population and we repeat the exercise. The recursion stops when we reach the so called leaf nodes e.g. when the remaining examples in these nodes cannot be split further. We will come back at this terminal / leaf nodes later. An example tree and corresponding partition is shown in the two figures below.\nExample tree\nPartition for the example tree\nThe test specification consists of the variables $\\theta_i$ that are are also called thresholds as well as the specific feature $x_k$ that is being selected for the test.\n NOTE: Mind you that its not the example $\\mathbf x$ that is part of the test spec - its the feature.\n Final decision tree for the fruit classification problem\nLets see the three recursions of the algorithm as shown below.\nFirst split\nSecond split\nThird split\nThis brings up the question of how we select the test spec parameters $x_k$ and $\\theta_k$ to minimize a certain metric that is dependent on the type of the problem we deal with - classification or regression.\nSelecting the feature $x_k$ to split To gauge which feature we will choose split requires a review of certain probabilistic concepts namely the concept of entropy. We can develop on top of entropy the concept of information gain that is pictorially explained using an example as shown below\nInformation gain for two possible splits\nThe input dataset in this example has uniform distribution over classes - we have exactly the same number of points in each class. If we split the data horizontally (select feature $x_1$) this produces two sets of data. Each set is associated with a lower entropy (higher information, peakier class histograms) that is defined as usual\n$$H(D) = − \\sum_{c \\in C} p( c ) \\log(p( c ))$$\nThe entropy drops after any resonable split as we exclude labels from the original set and end up with more homogenous sets of labels. The gain of information achieved by splitting the data into two parts (1 / 2 or left / right) is computed as\n$$IG = H(D) − L(x_k, \\theta_k)$$\n$$ = H(D) - \\sum_{j \\in {1,2}} \\frac{|D^j|}{|D|} H(D^j)$$\nwhere $|.|$ is the cardinality operator i.e. the number of elements in the corresponding set and $L$ is the loss function that the algorithm is searching for its minima by trying $(x_k, \\theta_k)$ pairs. Apart from entropy we can also use another measure of impurity of the labels of the corresponding set - the Gini impurity measure. You are expected to understand the Entropy measure as it is used in other algorithms that we cover in this course. In programming, if you use sklearn, you need to explicitly change the default Gini measure to entropy.\nInference Training will result into the heuristically optimal decision tree. Inference is then straightforward as the input data will trasverse the tree and find itself into a leaf node. We can also estimate the probability that an instance belongs to a particular class k or $p(c_k|\\mathbf x)$. First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node.\nApplicability When we use decision trees for regression, the entropy is replaced with the usual MSE. We also have the possibility of parametric clustering in an unsupervised learning setting as shown below where the Gaussian distribution is being used to fit the data before and after the split. The Information Gain concept is generic enough to be applied in both discrete, continuous, supervised and unsupervised problems.\n"});index.add({'id':82,'href':'/cs-gy-6613-spring-2020/docs/','title':"Docs",'content':""});index.add({'id':83,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/simulation/','title':"Embodied AI Simulation",'content':""});index.add({'id':84,'href':'/cs-gy-6613-spring-2020/docs/projects/pneumonia/','title':"Explainable COVID-19 Pneumonia (OPTIONAL)",'content':"Explainable COVID-19 Pneumonia Opacities in the lungs caused by pneumonia\nThis project is due May 3 at 11:59pm\nIn spring of 2020, the spread of COVID-19 caused hundreds of thousands of deaths world wide due to the severe pneumonia in combination of immune system reactions to it. Your job is to develop an AI system that detects pneumonia. Doctors are reluctant to accept black box algorithms such as your deep learning based method - as an AI engineer you need to listen to them and try to satisfy their needs, they are your customer after all. They tell you that your automated diagnostic system that processes the imaging they give you, must be explainable.\nThey give you the COVID X-ray / CT Imaging dataset and:\n First you find this this implementation of the method called Local Interpretable Model-Agnostic Explanations (i.e. LIME). You also read this article and you get your hands dirty and replicate the results in your colab notebook with GPU enabled kernel(40%). A fellow AI engineer, tells you about another method called SHAP that stands for SHapley Additive exPlanations and she mentions that Shapley was a Nobel prize winner so it must be important. You then find out that Google is using it and wrote a readable white paper about it and your excitement grows. Your manager sees you on the corridor and mentions that your work is needed soon. You are keen to impress her and start writing your 2-3 page summary of the SHAP approach as can be applied to explaining deep learning classifiers such as the ResNet network used in (1). (40%) After your presentation, your manager is clearly impressed with the depth of the SHAP approach and asks for some results for explaining the COVID-19 diagnoses via it. You notice that the extremely popular SHAP Github repo already has an example with VGG16 network applied to ImageNet. You think it wont be too difficult to plugin the model you trained in (1) and explain it. (20%)  "});index.add({'id':85,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/tensorflow-introduction/','title':"Introduction to Tensorflow",'content':"This by no means is a tutorial introduction but rather a set of slides that we can use to describe the principle of computational graphs. This can be skipped and consulted later in the course.\nWe will cover slides #1 - #28 as shown below. The slides are from CS 20: Tensorflow for Deep Learning Research and despite the title are appropriate for Tensorflow beginners. Slides beyond #28 will be selectively consulted when we go over gradients and backpropagation.\n"});index.add({'id':86,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/knowledge-representations/','title':"Knowledge Representations",'content':""});index.add({'id':87,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/','title':"Linear Algebra for Machine Learning",'content':"Linear Algebra for Machine Learning The corresponding chapter of Ian Goodfellow\u0026rsquo;s Deep Learning is essentially the background you need.\n Another resource is the book with the funny title \u0026ldquo;No Bullshit Guide to Linear Algebra\u0026rdquo; by Ivan Savov.\nKey Points We can now summarize the points to pay attention to, for ML applications. In the following we assume a data matrix $A$ with $m$ rows and $n$ columns. We also assume that the matrix is such that it has $r$ independent rows or columns, called the matrix rank.\nProjections Its important to understand this basic operator and its geometric interpretation as it is met in problems like Ordinary Least Squares but also all over ML and other fields such as compressed sensing. In the following we assume that the reader is familiar with the concept of vector spaces and subspaces.\nLet $S$ be a vector subspace of $\\R^n$. For example in $\\R^3$, $S$ are the lines and planes going through the origin. The projection operator onto $S$ implements a linear transformation: $\\Pi_S: \\R^3 →S$. We will stick to $\\R^3$ to maintain the ability to plot the operations involved. We also define the orthogonal subspace,\n$$S^\\perp ≡ \\{ \\bm w \\in \\R^3 | \\bm w ^T \\bm s = 0, ∀ \\bm s \\in S \\} $$\nThe transformation $\\Pi_S$ projects onto space $S$ in the sense that when you apply this operator, every vector $\\bm u$ in any other space results in the subspace $S$. In our example above,\n$$\\Pi_S(\\bm u) \\in S, \\forall \\bm u \\in \\R^3$$\nThis means that any components of the vector $\\bm u$ that belonged to $S^\\perp$ are gone when applying the projection operator. Effectively, the original space is decomposed into\n$$ \\R^3 = S \\oplus S^\\perp $$\nNow we can treat projections onto specific subspaces such as lines and planes passing through the origin.\nFor a line defined by a direction vector $\\bm u$\n$$l = \\{ (x,y,z) \\in \\R^3 | (x,y,z) = \\bm 0 + t \\bm u \\} $$\nwe can define the projection onto the line\nProjection of $\\bm u$ onto the line $l$\nThe space $S^\\perp ≡ l^\\perp$ is a plane since it consists of all the vectors that are perpendicular to the line. What is shown in the figure as a dashed line is simply the projection of $\\bm u$ on the $l^\\perp$ subspace,\n$$l^\\perp = \\{ (x,y,z) \\in \\R^3 | \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}^T \\bm v = 0\\} $$\nThe orthogonal space of a line with direction vector $\\bm v$ is a plane with a normal vector $\\bm v$. So when we project the $\\bm v$ on the line we get two components one is lying on the line and is the $\\Pi_l \\bm u$ and the other is the vector $\\bm w$ = $\\Pi_{l^\\perp} \\bm u = \\bm u - \\bm v = \\bm u - \\Pi_{\\bm v} \\bm u $. The vector $\\bm w$ is what remains when we remove the projected on $\\bm v$ part from the $\\bm u$.\nThe Four Fundamental Subspaces The fundamental theorem of Linear Algebra specifies the effect of the multiplication operation of the matrix and a vector ($A\\mathbf{x}$). The matrix gives raise to 4 subspaces:\n The column space of $A$, denoted by $\\mathcal{R}(A)$, with dimension $r$. The nullspace of $A$, denoted by $\\mathcal{N}(A)$, with dimension $n-r$. The row space of $A$ which is the column space of $A^T$, with dimension $r$ The left nullspace of $A$, which is the nullspace of $A^T$, denoted by $\\mathcal{N}(A^T)$, with dimension $m-r$.  The real action that the matrix performs is to transform its row space to its column space.\nThe type of matrices that are common in ML are those that the number of rows $m$ representing observations is much larger than the number of columns $n$ that represent features. We will call these matrices \u0026ldquo;tall\u0026rdquo; for obvious reasons. Let us consider one trivial but instructive example of the smallest possible \u0026ldquo;tall\u0026rdquo; matrix:\n$$\\begin{bmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\\\ a_{31} \u0026amp; a_{32} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 5 \u0026amp; 4 \\\\ 2 \u0026amp; 4 \\end{bmatrix}$$\nIn ML we are usually concerned with the problem of learning the weights $x_1, x_2$ that will combine the features and result into the given target variables $\\mathbf{b}$. The notation here is different and we have adopted the notation of many linear algebra textbooks.\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 5 \u0026amp; 4 \\\\ 2 \u0026amp; 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$$\nTo make more explicit the combination of features we can write,\n$$ x_1 \\begin{bmatrix} 1 \\\\ 5 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 0 \\\\ 4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$$\nSince $m=3 \u0026gt; n=2$, we have more equations than unknowns we in general we have no solutions - a system with $m \u0026gt; n$ will be solvable only for certain right hand sides $\\mathbf{b}$. Those are all the vectors $\\mathbf{b}$ that lie in the column space of $A$.\nIn this example, as shown in the picture $\\mathbf{b}$ must lie in the plane spanned by the two columns of $A$. The plane is a subspace of $\\mathbb{R}^m=\\mathbb{R}^3$ in this case.\nNow instead of looking at what properties $\\mathbf{b}$ must have for the system to have a solution, lets look at the dual problem i.e. what weights $\\mathbf{x}$ can attain those $\\mathbf{b}$. The right-hand side $\\mathbf{b}=0$ always allows the solution $\\mathbf{x}=0$ The solutions to $A \\mathbf{x} = \\mathbf{0}$ form a vector space - the nullspace $\\mathcal{N}(A)$. The nullspace is also called the kernel of matrix $A$ and the its dimension $n-r$ is called the nullity.\n$\\mathcal{N}(A)$ is a subspace of $\\mathbb{R}^n=\\mathbb{R}^2$ in this case. For our specific example,\n$$ x_1 \\begin{bmatrix} 1 \\\\ 5 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 0 \\\\ 4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\nthe only solution that can satisfy this set of homogenous equations is: $\\mathbf{x}=\\mathbf{0}$ and this means that the null space contains only the zero vector and this\nTwo vectors are independent when their linear combination cannot be zero, unless both $x_1$ and $x_2$ are zero. The columns of $A$ are therefore linearly independent and they span the column space. They have therefore all the properties needed for them to constitute a set called the basis for that space and we have two basis vectors (the rank is $r=2$ in this case). The dimension of the column space is in fact the same as the dimension of the row space ($r$) and the mapping from row space to column space is in fact invertible. Every vector $\\mathbf{b}$ comes from one and only one vector $\\mathbf{x}$ of the row space ($\\mathbf{x}_r$). And this vector can be found by the inverse operation - noting that only the inverse $A^{-1}$ is the operation that moves the vector correctly from the column space to the row space. The inverse exists only if $r=m=n$ - this is important as in most ML problems we are dealing with \u0026ldquo;tall\u0026rdquo; matrices with the number of equations much larger than the number of unknowns which makes the system inconsistent (or degenerate).\nProjection onto the column space\nGeometrically you can think about the basis vectors as the axes of the space. However, if the axes are not orthogonal, calculations will tend to be complicated not to mention that we usually attribute to each vector of the basis to have length one (1.0).\nEigenvalues and Eigenvectors The following video gives an intuitive explanation of eigenvalues and eigenvectors and its included here due to its visualizations that it offers. The video must be viewed in conjunction with Strang\u0026rsquo;s introduction\n During the lecture we will go through an example from how your brain processes the sensory input generated by the voice of the lecturer(unless you are already asleep by that time) to combine optimally the sound from both your ears.\nA geometric interpretation of the eigenvectors and eigenvalues is given in the following figure:\n"});index.add({'id':88,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/optimization/','title':"Optimization and Stochastic Gradient Descent",'content':"Optimization and Stochastic Gradient Descent In this lecture we will go over concepts from Ian Goodfellow\u0026rsquo;s chapter 4 below. Stochastic gradient descent is treated also in section 5.9.\n "});index.add({'id':89,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/probability/','title':"Probability and Information Theory Basics",'content':"Book Chapters From Ian Goodfellow\u0026rsquo;s book:\n We will go through the main points during the lecture and treat also MacKay\u0026rsquo;s book (Chapter 2) that is also instructive and a much better in introducing probability concepts. If you are a visual learner, the visual information theory blog post is also a good starting point.\nKey Concepts to understand Probability The pictures below are from MacKays book and despite their conceptual simplicity they hide many questions that we will go over the lecture.\nProbability distributions Probability distribution over the letters of the English alphabet (letter 27 symbolizes space) as measured by reading the Linux FAQ document.\nJoint probability distributions Joint probability $P(x,y)$ distribution over the 27x27 possible bigrams $xy$ found in this document: https://www.tldp.org/FAQ/pdf/Linux-FAQ.pdf\nWhat is the marginal probability $P(x)$ ?\nConditional probability distribution\nConditional probability distribution over the 27x27 possible bigrams $xy$ found in this document: https://www.tldp.org/FAQ/pdf/Linux-FAQ.pdf\nAre $x$ and $y$ independent ?\nProbability Rules If H is the hypothesis governing the probabilities distributions,\nProduct or chain rule:\nThis is obtained from the definition of conditional probability:\n$P(x,y|H) = P(x | y,H)P(y | H) = P(y | x,H)P(x |H)$\nSum rule:\nThis is obtaining by rewriting of the marginal probability denition: $P(x |H) = \\sum_y P(x,y |H) = \\sum_y P(x | y,H)P(y |H)$\nKey probability distributions Multi-variate Gaussian distribution $$f_{\\mathbf X}(x_1,\\ldots,x_k) = \\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right)}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma|}}$$ where where \u0026lt;${\\mathbf x}$ is a real \u0026lsquo;n\u0026rsquo;-dimensional column vector and $|\\boldsymbol\\Sigma|\\equiv \\operatorname{det}\\boldsymbol\\Sigma$ is the determinant of $\\boldsymbol\\Sigma$.\nApart from the definition, you need to connect the geometric interpretation of the bivariate Gaussian distribution to the eigendecomposition in the linear algebra lecture as shown in the Figure 2.7 of Bishop:\nSuch geometric interpretations will be very useful when we study dimensionality reduction via Principal Component Analysis (PCA).\nProbabilistic Modeling   The whole purpose of probabilistic modeling is to introduce uncertainty into our problem statement. There are three types of uncertainties:\n Inherent stochasticity - e.g. impact of wind in self-driving car control systems at moderate to high speed. Incomplete observability - e.g. sensor imperfections causing loss of sensing information Incomplete modeling - e.g. models and algorithms that are not implementable to an analog world and need to be discretized.    Probabilities can be used in two ways.\n Probabilities can describe frequencies of outcomes in random experiments Probabilities can also be used, more generally, to describe degrees of belief in propositions that do not involve random variables. This more general use of probability to quantify beliefs is known as the Bayesian viewpoint. It is also known as the subjective interpretation of probability, since the probabilities depend on assumptions.    The Bayesian theorem is the cornerstone of probabilistic modeling. If $\\mathbf{\\theta}$ denotes the unknown parameters, $D$ denotes the dataset and $\\mathcal{H}$ denotes the hypothesis space - the model we have seen in the learning problem chapter.\n  $$ P(\\mathbf{\\theta} | D, \\mathcal{H}) = \\frac{P( D | \\mathbf{\\theta}, \\mathcal{H}) P(\\mathbf{\\theta} | \\mathcal{H}) }{ P(D|\\mathcal{H})} $$\nThe Bayesian framework allows the introduction of priors from a wide variety of sources (experts, other data, past posteriors, etc.) For example,a medical patient is exhibiting symptoms x, y and z. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor (the expert) has beliefs about which disease, but a second doctor may have slightly different beliefs.\n NOTE: The Probabilistic Programming \u0026amp; Bayesian Methods for Hackers book is one of the best resources out there containing practical python examples. In addition they have been recoded recently to work in Tensorflow Probability an industrial-strength framework that can bring together Deep Learning and domain-specific probabilistic modeling. The book cant match the rigorousness of Bishop\u0026rsquo;s book but it offers a good treatment on problems and use cases and should be considered complimentary.\n Information-theoretic definitions Entropy An outcome $x_t$ carries information that is a function of the probability of this outcome $P(x_t)$ by,\n$I(x_t) = \\ln \\frac{1}{P(x_t)} = - \\ln P(x_t)$\nThis can be intuitively understood when you compare two outcomes. For example, consider someone is producing the result of the vehicular traffic outside of Holland tunnel on Monday morning. The information that the results is \u0026ldquo;low\u0026rdquo; carries much more information when the result is \u0026ldquo;high\u0026rdquo; since most people expect that there will be horrendous traffic outside of Holland tunnel on Monday mornings. When we want to represent the amount of uncertainty over a distribution (i.e. the traffic in Holland tunnel over all times) we can take the expectation over all possible outcomes i.e.\n$H(P) = - \\mathbb{E} \\ln P(x)$\nand we call this quantity the entropy of the probability distribution $P(x)$. When $x$ is continuous the entropy is known as differential entropy. Continuing the alphabetical example, we can determine the entropy over the distribution of letters in the sample text we met before as,\nThis is 4.1 bits (as the $\\log$ is taken with base 2). This represents the average number of bits required to transmit each letter of this text to a hypothetical receiver. Note that we used the information carried by each \u0026ldquo;outcome\u0026rdquo; (the letter) that our source produced. If the source was binary, we can plot the entropy of such source over the probability p that the outcome is a 1 as shown below,\nThe plot simply was produced by taking the definition of entropy and applying to the binary case,\n$H(p) = - [p \\ln p - (1-p) \\ln(1-p)]$\nAs you can see the maximum entropy is when the outcome is most unpredictable i.e. when a 1 can show up with uniform probability (in this case equal probability to a 0).\nRelative entropy or KL divergence In the ML problem statement, it is evident that the job of the learning algorithm is to come up with a final hypothesis that is close to the unknown target function. In other occasions, we need to approximate a distribution by sampling from another easier to model distribution. As in ML we work with probabilities, we need to have a metric that compares two probability distributions ${P(x),Q(x)}$ in terms of their \u0026ldquo;distance\u0026rdquo; from each other (the quotes will be explained shortly). This is given by the quantity known as relative entropy or KL divergence.\n$KL(P||Q)= \\mathbb{E}[\\ln P(x) - \\ln Q(x)]$\nIf the two distributions are identical, $KL=0$ - in general however $KL(P||Q) \\ge 0$. One key element to understand is that $KL$ is not a true distance metric as its assymetric. Ensure that you understand fully the following figure and caption.\nVery close to the relative entropy is probably one of the most used information theoretic concepts in ML: the cross-entropy. We will motivate cross entropy via a diagram shown below,\nBackground for logistic regression If $\\sigma$ is a probability of an event, then the ratio $\\frac{\\sigma}{1-\\sigma}$ is the corresponding odds, the ratio of the event occurring divided by not occurring. For example, if a race horse runs 100 races and wins 25 times and loses the other 75 times, the probability of winning is 25/100 = 0.25 or 25%, but the odds of the horse winning are 25/75 = 0.333 or 1 win to 3 loses. In the binary classification case, the log odds is given by\n$$ \\mathtt{logit}(\\sigma) = \\alpha = \\ln \\frac{\\sigma}{1-\\sigma} = \\ln \\frac{p(\\mathcal{C}_1|\\mathbf{x})}{p(\\mathcal{C}_2|\\mathbf{x})}$$\nWhat is used in ML though is the logistic function of any number $\\alpha$ that is given by the inverse logit:\n$$\\mathtt{logistic}(\\alpha) = \\sigma(\\alpha) = \\mathtt{logit}^{-1}(\\alpha) = \\frac{1}{1 + \\exp(-\\alpha)} = \\frac{\\exp(\\alpha)}{ \\exp(\\alpha) + 1}$$\nand is plotted below. It maps its argument to the \u0026ldquo;probability\u0026rdquo; space [0,1].\nLogistic sigmoid (red)\nThe sigmoid function satisfies the following symmetry:\n$$\\sigma(-\\alpha) = 1 - \\sigma(\\alpha)$$\nIn addition it offers very convenient derivatives and has been used extensively in deep neural networks (for many architectures has been superceded by RELU). The derivative can be obtained as follows:\nConsider $$ f(x)=\\dfrac{1}{\\sigma(x)} = 1+e^{-x} . $$ Then, on the one hand, the chain rule gives $$ f\u0026rsquo;(x) = \\frac{d}{dx} \\biggl( \\frac{1}{\\sigma(x)} \\biggr) = -\\frac{\\sigma\u0026rsquo;(x)}{\\sigma(x)^2} , $$ and on the other hand, $$ f\u0026rsquo;(x) = \\frac{d}{dx} \\bigl( 1+e^{-x} \\bigr) = -e^{-x} = 1-f(x) = 1 - \\frac{1}{\\sigma(x)} = \\frac{\\sigma(x)-1}{\\sigma(x)} $$\nEquating the two expressions we finally obtain,\n$$\\sigma\u0026rsquo;(x) = \\sigma(x)(1-\\sigma(x))$$\n"});index.add({'id':90,'href':'/cs-gy-6613-spring-2020/docs/projects/imu-classification/','title':"Project 1 - Surface Type Classification",'content':"Surface Type Classification   Your first project description is published in https://www.kaggle.com/c/career-con-2019/overview\n  You must submit your assignment with the results by 11:59pm 2/23/2020. The submission will be done by sharing the github/kaggle notebook with the TA.\n  "});index.add({'id':91,'href':'/cs-gy-6613-spring-2020/docs/projects/continuous-learning/','title':"Project 2 - Continual Learning for Robotic Perception",'content':"Continual Learning (CL) for Robotic Perception This project is due March 29 at 11:59pm\nIntroduction One of the greatest goals of AI is building an artificial continual learning agent which can construct a sophisticated understanding of the external world from its own experience through the adaptive, goal-oriented and incremental development of ever more complex skills and knowledge.\nContinual learning (CL) is essential in robotics where high dimensional data streams need to be constantly processed and where naïve continual learning strategies have been shown to suffer from catastrophic forgetting also known as catastrophic interference. Take deep learning architectures for example: they excel at a number of classification and regression tasks by using a large dataset, however the dataset must be present during the training phase and the whole network must be retrained every time there the underlying distribution $p_{data}$ changes. The existing approaches of _transfer learning_ that use weights of a different dataset to set the weights for the new dataset are inadequate, since they only help on reducing the training time for new dataset, rather than preserve the pre-existing ability to achieve good classification performance on the original dataset classes. Aside from supervised learning, CL accommodate a wider range of tasks such as unsupervised and reinforcement learning.\nOur brains exhibits plasticity that allows us on one hand to integrate new knowledge and on the other hand to do so without overcompensating for new knowledge causing interference with the consolidated knowledge - this tradeoff is called plasticity-stability dilemma. Hebbian plasticity, termed after Hebb\u0026rsquo;s rule (1949), basically postulate the tendency of strengthening the connection between pre-synaptic and post-synaptic neurons when the activations $x$ of the former affect the activations $y$ of the later. In its simplest form, Hebb\u0026rsquo;s rule states that a synaptic strength $w$ changes as: $\\Delta w = \\eta × x \\times y$ where $\\eta$ is the learning rate. Hebb\u0026rsquo;s rule can lead to instability and homeostatic mechanisms are used, represented by a modulatory feedback control signal $m$ that regulates the unstable dynamics of Hebb\u0026rsquo;s rule:\n$$ \\Delta w = m \\times \\eta × x \\times y$$\nWe can draw the block diagram of such dynamical system model:\nHebbian-Homeostatic Plasticity Model for synaptic updates.\nWhile the synaptic updates under the plasticity rule are essential to avoid catastrophic forgetting at the neural circuit level, we need a system level mechanism to carry out two complementary tasks, also known as Complementary Learning Systems (CLS) theory: statistical learning with the ability to generalize across experiences and retain what it learned for the long term and episodic learning that learns quickly novel information and retains episodic event memories (memorization). Statistical learning is implemented by the neocortical functional area of the grain and is a slower process as it extracts statistical structures of perceptive signals to be able to generalize to novel examples / situations. Episodic leanring, implemented in the hypocampus, is much faster as its goal is to learn specific events, retain them and play them back to the neocortex for information integration purposes. Both subsystems use the controlled Hebbian plasticity mechanism outlined earlier.\nHypocampus and Neocortex complementary learning system\nThe aforementioned widely accepted as explanatory functions of the brain learning system, guided computational models and architectures of continuous learning in AI. For the deep learning architectures, three are the main threads:\n Imposing weight constraints (Regularization) Allocating additional neural resources to capture the new knowledge (dynamic architectures) CLS-based approaches.  In this project, you are free to select a method from any of these three non-exclusive categories, as described in detail. This is a very active area of AI research.\nDatasets and Tasks You are given two dataset options for this project as shown in the table below. The CORe50 option is more difficult than the MNIST option. Grading will happen relative to teams that selected the same option. The CORe50 option may require an AWS/Azure/Google cloud or NYU GPU compute resource to run. MNIST should be able to run in Colab/Kaggle with standard free accounts. If you have access to compute, you will learn more from selecting the CORe50 option as this is closest to a real life dataset than the rotated MNIST which is not even testing for new classes - rather it tests CL on rotated version of classes it has seen before.\n   CORe50 Option Rotated MNIST Option     You will use this dataset and evaluate your method for New Class (NC) scenario. Use the dataset provided here based on this paper   Object recognition (classification). Object recognition (classification).    "});index.add({'id':92,'href':'/cs-gy-6613-spring-2020/docs/projects/semantic-code-search/','title':"Project 3 - Semantic Code Search",'content':"Semantic Code Search This project is due May 10 at 11:59pm\nImagine a world where software is automatically generated and stitched together to create complex distributed / reactive systems that can solve the problem at hand. In this course you got to appreciate the systems approach to AI and this project will allow you to understand one very important facet of automatic AI system synthesis: a search engine for code. It can be used initially by human programmers and later by robots to find pieces of code that are relevant to the target function.\nSemantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.\nIn this project you will use the CodeSearchNet Corpus and participate in the corresponding challenge.. In this project you will focus only on Python language and the associated dataset contains about 0.5 million pairs of function-documentation pairs and about another 1.1 million functions without an associated documentation. You are required to submit your Normalized Discounted Cumulative Gain (NDCG) score for only the human annotated examples.\nSteps to complete this project:\n  Follow the setup instructions.\n  Implement and explain a baseline model e.g Self-Attention and try to improve its performance over the Elastic Search baseline. You are free to try to develop your own model.\n  Follow the instructions in the competition to submit your score to the leaderboard and submit your github / colab notebooks and writeups.\n  Despite the fact that the project is not mandating the invention of new methods, the project is challenging - teams each with a maximum of three students are allowed.\n"});index.add({'id':93,'href':'/cs-gy-6613-spring-2020/docs/projects/going-back-to-work/','title':"Project 4 - Going Back to Work",'content':"Going Back to Work In the spring of 2020 the world has experienced an unprecedented in modern history event - a pandemic that killed tens of thousands of people and resulted in a global economic recession; tens of millions lost their jobs. You are called to help rebuild the economy and develop an system that can protect people from infection while still allow them to go back to work.\nOne of the ways that this can be achieved is by maintaining a certain density of people in buildings. To do that, we need to engineer a reservation (booking) system where each user uses a messaging app to reserve a time slot (period) that he or she is allowed in the building, by entering the destination address and desired arrival time and length of stay.\nYour AI-powered decision system responds giving the requestor a decision. Each decision is either an accept, manifested with a QR code with the address and allowed arrival and departure times in text or a downright reject. The departure time is needed since the system must preserve building density and people must leave the building to make \u0026ldquo;space\u0026rdquo; for others to enter. The security personnel or receptionists supervise a programmable scanner or manually scan the QR code to allow entrance and exit (pretty similar to the scanners used to allow entry of an aircraft at the gate). The system also allows for group bookings, made by an assistant for example, to enable face to face meetings. In this case the decision system accepts or rejects for the whole group.\nTo give you a practical perspective, the system can be entirely powered by well known chat / messaging apps and your cloud-native \u0026amp; cloud-hosted application - the decision system. The system also complements contact-tracing systems that are powered by mobile OS and are under active development by Apple and Google.\nTo simplify the problem,\n The departure times cannot change dynamically. This means that if the actual demand is lower than the predicted demand when the scheduling decision took place, the departure time stays fixed. Employees / customers can extend their departure by sending another request via the messaging app but this is an independent reservation and is not accounted for the original decision policy. Although this can be very useful for stores and co-working spaces, there is no attempt to flatten the demand curve by distributing accepts among geographically separated facilities of the same legal entity. There is no semantic consideration in the decision making algorithm. It is assumed that theaters and other sports venues control the reservations and seating to maintain social distancing. The same is assumed for office buildings - the system is not designed (although it can be extended via video surveillance and other localization technologies) to consider per office floor occupancies. There are no fairness considerations. One can go ahead and book multiple time slots by sending messages one after the other to the system. This is an important limitation but can be easily addressed in practice and will unnecessarily complicate the policy given the limited time to complete the project. There are no risk considerations in the decision making process. A user that requests a reservation that lives far away and travels via public transportation has equal chances to a user that lives next to the requested address.  This system can be powered by cellular network based localization datasets but in the absence of such data that only operators possess, we will use the the dataset that contains all NYC taxi rides from 2009 to 2019. The dataset includes pickups and drop offs and we will focus for this problem on drop offs only and only in the borough of Manhattan (if you face computational load hardships you can focus on the midtown Manhattan area only).\nThere are two steps to do:\n Preprocessing step. In this step you generate the demand model before the intervention by the decision making process. Think about it as the pre-pandemic demand for the address. You need to take the (lat, long) drop off data and using a geocoder convert them to addresses. See also here for some preprocessing ideas and here for code that does preprocessing. Below is a sample result of the preprocessing mapping the drop off data into addresses.  Optimal Decision Policy. You will use the demand model from (1) as input to simulate the requested reservations at a location. Obviously the optimal policy will involve a closed loop feedback where the actual arrivals into each building are streamed in almost real time to the web service such as from video surveillance cameras. You are free to make assumptions in determining the policy such as knowing the actual capacity of each of the addresses / neighborhoods you are demonstrating. As an example, the decision policy can be a scheduling policy. The scheduler at any instant in time determines whether to accept or reject incoming reservations requests.  The project is flexible in the sense that there is not a strict metric other than a demonstration that the policy flattened the demand curve at a location (such as the GS building above). I hope you enjoy doing it not only for its technical depth but also because it serves a real and timely need with worldwide impact.\n"});index.add({'id':94,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/random-forests/','title':"Random Forests",'content':"Random Forests Random forests is a popular and very strong ML algorithm that belongs to what is called ensemble learning methods. As the name implies they use many classification tree learners to improve on their generalization ability. Each of the trees is called the weak learner - when grouped together they form the strong learner.\nA key aspect of decision forests is the fact that its component trees are all randomly different from one another. This leads to de-correlation between the individual tree predictions and, in turn, to improved generalization. Forest randomness also helps achieve high robustness with respect to noisy data. Randomness is injected into the trees during the training phase. Two of the most popular ways of doing so are:\n Random training data set sampling (when sampling is performed with replacement, this is called bagging), Randomized node optimization.  These two techniques are not mutually exclusive and could be used together.\nIn the sklearn library, the Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity. In addition, we can make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds - also known as Extra-Trees.\nInference Example of three trees receiving the instance $\\mathbf x$ (shown as $\\mathbf v$ in the figure)\nDuring testing the same unlabelled test input data $\\mathbf x$ is pushed through each component tree. At each internal node a test is applied and the data point sent to the appropriate child. The process is repeated until a leaf is reached. At the leaf the stored posterior $p_t(c|\\mathbf x)$ is read off. The forest class posterior $p(c|\\mathbf x)$ is simply the average of all tree (T) posteriors.\n$p(c|\\mathbf x) = \\frac{1}{T} \\sum_{t=1}^T p_t(c|\\mathbf x) $\nImpact of random forest parameters Given a training dataset with two classes (a), different training trees produce different partitions and thus different leaf predictors. The colour of tree nodes and edges indicates the class probability of training points going through them (b) In testing, increasing the forest size $T$ produces smoother class posteriors.\n"});index.add({'id':95,'href':'/cs-gy-6613-spring-2020/docs/resources/','title':"Resources",'content':"Resources This is a collection of study guides.\n"});index.add({'id':96,'href':'/cs-gy-6613-spring-2020/docs/lectures/drafts/language-grounding/','title':"Robotic Language Grounding",'content':""});index.add({'id':97,'href':'/cs-gy-6613-spring-2020/tags/','title':"Tags",'content':""});index.add({'id':98,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/netflix/','title':"The Netflix Prize and Singular Value Decomposition",'content':"Introduction The following are based on the winning submission paper as well as their subsequent publication.\nThe Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users or the films being identified except by numbers assigned for the contest.\nThe competition was held by Netflix, an online DVD-rental and video streaming service, and was open to anyone who is neither connected with Netflix (current and former employees, agents, close relatives of Netflix employees, etc.) nor a resident of certain blocked countries (such as Cuba or North Korea).[1] On September 21, 2009, the grand prize of US $ 1,000,000 was given to the BellKor\u0026rsquo;s Pragmatic Chaos team which bested Netflix\u0026rsquo;s own algorithm for predicting ratings by 10.06%\nThis competition is instructive since:\n Collaborative filtering models try to capture the interactions between users and items that produce the different rating values. However, many of the observed rating values are due to effects associated with either users or items, independently of their interaction. A prime example is that typical CF data exhibit large user and item biases – i.e., systematic tendencies for some users to give higher ratings than others, and for some items to receive higher ratings than others. Observing the posted improvements in RMSE over time, the competition has become of little business value to Netflix after a while. This means that it was unlikely that any minute improvement to RMSE (e.g. 0.1%) will translate to additional revenue. The 10% improvement goal was a lucky number after all. Netflix had no clue as to if this was the right number when they defined the terms. Any small deviation from this number, would have made the competition either too easy or impossibly difficult.  Problem statement You are given the following dataset structure (will be explained in class) shown below,\nAssuming that the rating matrix A is an m x n matrix with m users (500K) and n items (17K movies), this matrix is extremely sparse - it has only 100 million ratings, the remaining 8.4 billion ratings are missing (about 99% of the possible ratings are missing, because a user typically rates only a small portion of the movies).\nGiven the very large data matrix it was only expected that competitors attempted to work out some form of dimensionality reductions and as it turns out this was the basis for the winning algorithm. If you recall the premise of SVD from linear algebra, it is a decomposition that can work with rectangular matrices and can result into a decomposition of the following kind:\n$$A = U \\Sigma V^†$$\nwhere $\\mathbf{U}$ is a $m \\times m$ real unitary matrix, $\\mathbf{\\Sigma}$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $\\mathbf{V}$ is a $n \\times n$ real unitary matrix. Remember that a unitary matrix $U$ is a matrix that its conjugate transpose $U^†$ is also its inverse - its the complex analog of the orthogonal matrix and we have by definition $UU^†=I$.\nThe columns of $U$ are eigenvectors of $AA^T$, and the columns of $V$ are eigenvectors of $A^TA$. The $r$ singular values on the diagonal of $\\Sigma$ are the square roots of the nonzero eigenvalues of both $AA^T$ and $A^TA$.\nIt is interesting to attribute the columns of these matrices with the four fundamental subspaces:\n The column space of $A$ is spanned by the first $r$ columns of $U$. The left nullspace of $A$ are the last $m-r$ columns of $U$. The row space of $A$ are the first $r$ columns of $V$. The nullspace of $A$ are the last $n-r$ columns of V.  We can write the SVD as,\n$$A = U \\sqrt{\\Sigma} \\sqrt{\\Sigma} V^†$$\nand given the span of the subspaces above we can now intuitively think what the terms $\\mathbf{p}_u = U \\sqrt{\\Sigma}$ and $\\mathbf{q}_i = \\sqrt{\\Sigma} V^†$ represent.\nSVD decomposition reveals latent features weighted by the underlying singular values of the data matrix\nThe first term represents the column space aka. it provides a representation of all inter-user factors (also called latent features, latent means hidden). The second term represents the row space aka. it provides a representation of all inter-movie factors. Which brings us to the major point of what the $\\sqrt{\\Sigma}$ is doing to both terms. It represents the significance of those factors and therefore we can very easily use the singular values it contains to \u0026ldquo;compress\u0026rdquo; our representation by selecting the $k$ largest singular values and ignoring the rest.\nGiven these vectors of factors we can now use them to predict the rating:\n*Prediction of a rating is the product between user latent features and movie latent features matrices.\nFor an intuitive description of the SVD solution see here.\n"});index.add({'id':99,'href':'/cs-gy-6613-spring-2020/docs/resources/mdp-study-guide/','title':"What you need to know on MDP \u0026 RL",'content':"What you need to know on MDP \u0026amp; RL You first need to read Section 17.1-17.3 of the AIMA book\n You need to be able to determine the transition function of environments like in Fig 17.1. You need to be able to explain why the policy behaves with the reward function if you are given an environment like in Fig 17.2. You need to be able to classify an problem if its infinite vs. finite horizon and what is the role of $\\gamma$. You need to know the intuition behind Bellman equations (both expectation and optimality pairs).  You need to be able to draw a tree that represents the problem you are trying to solve   You need to be able to explain DP vs MC vs TD. You need to be able to calculate policy and value iterations.  Look at exercises like 17.1, 17.2 and 17.8.\nYou need to read chapter 21.1-21.5\nYou need to know how the REINFORCE and SARSA would function in a hypothetical small world.  "});index.add({'id':100,'href':'/cs-gy-6613-spring-2020/docs/projects/env/','title':"Your Programming Environment",'content':"Your Programming Environment Starting Jupyter in Google Colab The runtime performance will greatly improve for some projects using the free GPU resources provided by Google Colab. In this course we will make use of these facilities - the good news is that you have an account in Google Colab as most of you have a google account. If not go ahead and create one to be able to login into Google colab. You will need Google Colab for all your projects so that you can demonstrate that your results can be replicated. In addition Colab has many features that come handy.\nI heavily borrowed from Geron\u0026rsquo;s book for the following.\nSetup Anaconda Python When using Anaconda, you need to create an isolated Python environment dedicated to this course. This is recommended as it makes it possible to have a different environment for each project, with potentially different libraries and library versions:\n$ conda create -n cs6613 python=3.6 anaconda $ conda activate cs6613  This creates a fresh Python 3.6 environment called cs6613 (you can change the name if you want to), and it activates it. This environment contains all the scientific libraries that come with Anaconda. This includes all the libraries we will need (NumPy, Matplotlib, Pandas, Jupyter and a few others), except for TensorFlow, so let\u0026rsquo;s install it:\n$ conda install -n cs6613 -c conda-forge tensorflow  This installs the latest version of TensorFlow available for Anaconda (which is usually not the latest TensorFlow version) in the cs6613 environment (fetching it from the conda-forge repository). If you chose not to create an cs6613 environment, then just remove the -n cs6613 option.\nNext, you can optionally install Jupyter extensions. These are useful to have nice tables of contents in the notebooks, but they are not required.\n$ conda install -n cs6613 -c conda-forge jupyter_contrib_nbextensions  Kaggle Assuming you have activated the cs6613 conda environment, follow the directions here to install the Kaggle command line interface (CLI). You will need Kaggle for all your projects. You guessed it right - all the projects in this course are in fact Kaggle competitions. Not only you will get to compete (your ranking relative to others does not matter per se), but as you improve your knowledge over time you can revisit these competitions and see how your score improves.\nYou are all set! Next, jump to the Starting Jupyter section.\nStarting Jupyter locally If you want to use the Jupyter extensions (optional, they are mainly useful to have nice tables of contents), you first need to install them:\n$ jupyter contrib nbextension install --user  Then you can activate an extension, such as the Table of Contents (2) extension:\n$ jupyter nbextension enable toc2/main  Okay! You can now start Jupyter, simply type:\n$ jupyter notebook  This should open up your browser, and you should see Jupyter\u0026rsquo;s tree view, with the contents of the current directory. If your browser does not open automatically, visit localhost:8888. Click on index.ipynb to get started!\nNote: you can also visit http://localhost:8888/nbextensions to activate and configure Jupyter extensions.\nGit / Github Git is the defacto standard when it comes to code version control. Learning basic git commands takes less than half an hour. However, to install git and understand the principle behind git, please go over Chapters 1 and 2 of the ProGit book.\nAs we have discussed in class you need to be able to publish your work in Github so you need to create a Github account. Then you will use the git client for your operating system to interact with github and iterate on your projects. You may be using Kaggle or Colab hosted notebooks but the underlying technology that powers such web-frontends when it comes to committing the code and seeing version numbers in your screen is git.\nIn addition, almost no data science project starts in vacuum - there is almost always software that will be inherited to be refined.\n"});})();