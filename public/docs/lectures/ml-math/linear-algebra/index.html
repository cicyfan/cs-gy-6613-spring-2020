<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Linear Algebra for Machine Learning"><meta property="og:title" content="Linear Algebra for Machine Learning" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/" />

<title>Linear Algebra for Machine Learning | CS-GY-6613 Spring 2020</title>
<link rel="icon" href="/cs-gy-6613-spring-2020/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/cs-gy-6613-spring-2020/book.min.b7fe34b8a4c4da05b2690b4aacfa71ddce0668cc6da3abb67dd3946844125444.css" integrity="sha256-t/40uKTE2gWyaQtKrPpx3c4GaMxto6u2fdOUaEQSVEQ=">


<script defer src="/cs-gy-6613-spring-2020/en.search.min.b8904ca7346a07b61d9e2ea16c1c9cef5270e25d8f2be2382d83e341e72529e6.js" integrity="sha256-uJBMpzRqB7Ydni6hbByc71Jw4l2PK&#43;I4LYPjQeclKeY="></script>

<link rel="alternate" type="application/rss+xml" href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/index.xml" title="CS-GY-6613 Spring 2020" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/cs-gy-6613-spring-2020"><img src="/cs-gy-6613-spring-2020/tandon_long_black.png" alt="Logo" /><span>CS-GY-6613 Spring 2020</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





  

  
  





 
  
    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/syllabus/" >
      Syllabus
  </a>

</li>
      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/" >
      Week 1 - Introduction to AI
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/" >
      Course Introduction
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/" >
      A systems approach to AI
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/" >
      The Way of Working in AI
  </a>


    

    




  
  <ul>
    
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Intelligent Agents and Representations</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/" >
      Projects
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/imu-classification/" >
      Project 1 - Surface Type Classification
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/continuous-learning/" >
      Project 2 - Continual Learning for Robotic Perception
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/projects/env/" >
      Your Programming Environment
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Background - Math for ML</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/"  class="active">
      Linear Algebra for Machine Learning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/optimization/optimization/" >
      Optimization and Stochastic Gradient Descent
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/probability/probability/" >
      Probability and Information Theory Basics
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/netflix/netflix/" >
      The Netflix Prize and Singular Value Decomposition
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Background - ML Frameworks</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/development-workflow/" >
      Development Workflow
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/tensorflow-introduction/" >
      Introduction to Tensorflow
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/numpy-pandas/" >
      Numerical Python (Numpy/Scipy and Pandas)
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/zillow-app/" >
      The Zillow App
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  











</nav>


<script>
(function() {
  var menu = document.querySelector("aside.book-menu nav");
  addEventListener("beforeunload", function(event) {
    localStorage.setItem("menu.scrollTop", menu.scrollTop);
  });
  menu.scrollTop = localStorage.getItem("menu.scrollTop");
})();
</script>

 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/cs-gy-6613-spring-2020/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Linear Algebra for Machine Learning</strong>

  <label for="toc-control">
    <img src="/cs-gy-6613-spring-2020/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
<ul>
<li><a href="#linear-algebra-for-machine-learning">Linear Algebra for Machine Learning</a>
<ul>
<li><a href="#key-points">Key Points</a>
<ul>
<li><a href="#the-four-fundamental-subspaces">The Four Fundamental Subspaces</a></li>
</ul></li>
</ul></li>
<li><a href="#end-bmatrix">\end{bmatrix}</a></li>
<li><a href="#end-bmatrix-1">\end{bmatrix}</a></li>
<li><a href="#end-bmatrix-2">\end{bmatrix}</a></li>
<li><a href="#end-bmatrix-3">\end{bmatrix}</a>
<ul>
<li>
<ul>
<li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
</ul></li>
</ul></li>
</ul>
</nav>


    </aside>
  
 
      </header>

      
<article class="markdown">

<h1 id="linear-algebra-for-machine-learning">Linear Algebra for Machine Learning</h1>

<p>The corresponding chapter of Ian Goodfellow&rsquo;s Deep Learning is essentially the background you need.</p>

<iframe src="https://www.deeplearningbook.org/contents/linear_algebra.html" width="800" height="1200"></iframe>

<h2 id="key-points">Key Points</h2>

<p>We can now summarize the points to pay attention to for ML applications.  In the following we assume a data matrix $A$ with $m$ rows and $n$ columns. We also assume that the matrix is such that it has $r$ independent rows, called <em>the matrix rank</em>.</p>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>



<span class="katex">
  \( x^2 \)
</span>


<h3 id="the-four-fundamental-subspaces">The Four Fundamental Subspaces</h3>

<p><img src="images/four-fundamental-spaces-linear-alg.png" alt="Four fundamental spaces" /></p>

<p>The <em>fundamental theorem of Linear Algebra</em> specifies the effect of the multiplication operation of the matrix and a vector ($A\mathbf{x}$). The matrix gives raise to 4 subspaces:</p>

<ol>
<li><strong>The column space of $A$</strong>, denoted by $\mathcal{R}(A)$, with dimension $r$.</li>
<li><strong>The nullspace of $A$</strong>, denoted by $\mathcal{N}(A)$, with dimension $n-r$.</li>
<li><strong>The row space of $A$</strong> which is the column space of $A^T$, with dimension $r$</li>
<li><strong>The left nullspace of $A$</strong>, which is the nullspace of $A^T$, denoted by $\mathcal{N}(A^T)$, with dimension $m-r$.</li>
</ol>

<p>The real action that the matrix perform is to <strong>transform</strong> its row space to its column space.</p>

<p>The type of matrices that are common in ML are those that the number of rows $m$ representing observations is much larger than the number of columns $n$ that represent features. We will call these matrices &ldquo;tall&rdquo; for obvious reasons. Let us consider one trivial but instructive example of the smallest possible &ldquo;tall&rdquo; matrix:</p>

<p>$$\begin{bmatrix}
    a<em>{11}       &amp; a</em>{12} <br />
    a<em>{21}       &amp; a</em>{22} <br />
    a<em>{31}       &amp; a</em>{32}</p>

<h1 id="end-bmatrix">\end{bmatrix}</h1>

<p>\begin{bmatrix}
    1       &amp; 0 <br />
    5       &amp; 4 <br />
    2       &amp; 4
\end{bmatrix}$$</p>

<p>In ML we are usually concerned with the problem of learning the weights $x_1, x_2$ that will combine the features and result into the given target variables $\mathbf{b}$. The notation here is different and we have adopted the notation of many linear algebra textbooks.</p>

<p>$$
\begin{bmatrix}
    1       &amp; 0 <br />
    5       &amp; 4 <br />
    2       &amp; 4
\end{bmatrix}
\begin{bmatrix}
    x_1 <br />
    x_2</p>

<h1 id="end-bmatrix-1">\end{bmatrix}</h1>

<p>\begin{bmatrix}
    b_1 <br />
    b_2 \
    b_3
\end{bmatrix}
$$
To make more explicit the combination of features we can write,</p>

<p>$$
x_1 \begin{bmatrix}
    1 <br />
    5 \
    2
\end{bmatrix} + x_2 \begin{bmatrix}
    0 <br />
    4 \
    4</p>

<h1 id="end-bmatrix-2">\end{bmatrix}</h1>

<p>\begin{bmatrix}
    b_1 <br />
    b_2 \
    b_3
\end{bmatrix}
$$</p>

<p>Since $m=3 &gt; n=2$, we have more equations than unknowns we in general we have no solutions - a system with $m &gt; n$ will be solvable only for certain right hand sides $\mathbf{b}$. Those are all the vectors $\mathbf{b}$ that lie in the column space of $A$.</p>

<p><img src="images/column-space.png" alt="column-space" /></p>

<p>In this example, as shown in the picture $\mathbf{b}$ must lie in the plane spanned by the two columns of $A$. The plane is a subspace of $\mathbb{R}^m=\mathbb{R}^3$ in this case.</p>

<p>Now instead of looking at what properties $\mathbf{b}$ must have for the system to have a solution, lets look at the <em>dual</em> problem i.e. what weights $\mathbf{x}$ can attain those $\mathbf{b}$. The right-hand side $\mathbf{b}=0$ always allows the solution $\mathbf{x}=0$
The solutions to $A \mathbf{x} = \mathbf{0}$ form a vector space - <strong>the nullspace</strong> $\mathcal{N}(A)$. The nullspace is also called the <em>kernel</em> of matrix $A$ and the its dimension $n-r$ is called the nullity.</p>

<p>$\mathcal{N}(A)$ is a subspace of $\mathbb{R}^n=\mathbb{R}^2$ in this case. For our specific example,</p>

<p>$$
x_1 \begin{bmatrix}
    1 <br />
    5 <br />
    2
\end{bmatrix} + x_2 \begin{bmatrix}
    0 <br />
    4 <br />
    4</p>

<h1 id="end-bmatrix-3">\end{bmatrix}</h1>

<p>\begin{bmatrix}
    0 <br />
    0 <br />
    0
\end{bmatrix}
$$</p>

<p>the only solution that can satisfy this set of homogenous equations is: $\mathbf{x}=\mathbf{0}$ and this means that the null space contains only the zero vector and this</p>

<p>Two vectors are independent when their linear combination cannot be zero, unless both $x_1$ and $x_2$ are zero.  The columns of $A$ are therefore linearly independent and they span the column space. They have therefore all the properties needed for them to constitute a set called the <em>basis</em> for that space and we have two basis vectors (the rank is $r=2$ in this case). The dimension of the column space is in fact the same as the dimension of the row space ($r$) and the mapping from row space to column space is in fact invertible. Every vector $\mathbf{b}$ comes from one and only one vector $\mathbf{x}$ of the row space ($\mathbf{x}_r$). And this vector can be found by the inverse operation - noting that only the inverse $A^{-1}$ is the operation that moves the vector correctly from the column space to the row space. The inverse exists only if $r=m=n$ - this is important as in most ML problems we are dealing with &ldquo;tall&rdquo; matrices with the number of equations much larger than the number of unknowns which makes the system <em>inconsistent</em> (or <em>degenerate</em>).</p>

<p><img src="images/projection-column-space.png" alt="projection-column-space" />
<em>Projection onto the column space</em></p>

<p>Geometrically you can think about the basis vectors as the axes of the space. However, if the axes are not orthogonal, calculations will tend to be complicated not to mention that we usually attribute to each vector of the basis to have length one (1.0).</p>

<h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3>

<p>The following video gives an intuitive explanation of eigenvalues and eigenvectors and its included here due to its visualizations that it offers.  The video must be viewed in conjunction with <a href="http://math.mit.edu/~gs/linearalgebra/linearalgebra5_6-1.pdf">Strang&rsquo;s introduction</a></p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/PFDu9oVAE-g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>During the lecture we will go through an example from how your brain processes the sensory input generated by the voice of the lecturer(unless you are already asleep by that time) to combine <em>optimally</em> the sound from both your ears.</p>

<p>A geometric interpretation of the eigenvectors and eigenvalues is given in the following figure:</p>

<p><img src="images/eigenvectors.png" alt="eigenvectors" /></p>
</article>
 

      <footer class="book-footer">
        
  <div class="flex justify-between">





  <div>
    <a class="flex align-center" href="https://github.com/pantelis/cs-gy-6613-spring-2020/edit/master/cs-gy-6613-spring-2020/content/docs/lectures/ml-math/linear-algebra/_index.md" target="_blank" rel="noopener">
      <img src="/cs-gy-6613-spring-2020/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>Edit this page</span>
    </a>
  </div>

</div>

 
        
  
 
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
<ul>
<li><a href="#linear-algebra-for-machine-learning">Linear Algebra for Machine Learning</a>
<ul>
<li><a href="#key-points">Key Points</a>
<ul>
<li><a href="#the-four-fundamental-subspaces">The Four Fundamental Subspaces</a></li>
</ul></li>
</ul></li>
<li><a href="#end-bmatrix">\end{bmatrix}</a></li>
<li><a href="#end-bmatrix-1">\end{bmatrix}</a></li>
<li><a href="#end-bmatrix-2">\end{bmatrix}</a></li>
<li><a href="#end-bmatrix-3">\end{bmatrix}</a>
<ul>
<li>
<ul>
<li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
</ul></li>
</ul></li>
</ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>












