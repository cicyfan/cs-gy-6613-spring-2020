'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/course-introduction/','title':"Course Introduction",'content':"AI Evolution according to DARPA If engineering difficulty has a pinnacle today this must be in AI domains that combines ML, optimal control and planning. autonomous cars and humanoids from Boston Dynamics fit the bill.\nInitially there were rules.\n In the 1980s knowledge-base systems that hard-coded knowledge about the world in formal languages.  IF this happens, THEN do that.   They failed to get significant traction as the number of rules that are needed to model the real world exploded. However, they are still in use today in vertical modeling domains e.g. fault management. For example Rule Based Engines are used today in many complex systems that manage mission critical infrastructures e.g. ONAP.  The introduction of advanced AI methods few years ago, created a situation we can explain with the following analogy.\nA nautical analogy on where we are today on AI for mission critical systems. Can you notice anything strange with this ship (Cumberland Basin, photo taken April 1844)?\nTo put order into the many approaches and methods for delivering AI in our lives, DARPA classified AI development in terms of \u0026ldquo;waves\u0026rdquo;.\n         Wave I: GOFAI Wave II: Connectionism Wave III: AGI   In the 1980s Rule Based Engines started to be applied manifesting the first wave of AI introduction. In this example you see a system that performs highway trajectory planning. A combination of cleverly designed rules does work and offers real time performance but cannot generalize and therefore have acceptable performance in other environments.\nWave II srarted soon after 2010 - we started to apply a different philosophy in solving intelligent tasks such as object classification. The philosophy of connectionism and the so called deep neural network architectures, dominate today relative simple (and mostly self-contained) tasks.\nWave III is at present an active research area driven primarily from our inability to implement with just deep neural networks things like long-term goal planning, causality, extract meaning from text like humans do, explain the decisions of neural networks, transfer the learnings from one task to another, even similar, task. Artificial General Intelligence is the term usually associated with such capabilities.\nFurther, we will see a fusion of disciplines such as physical modeling and simulation with representation learning to help deep neural networks learn using data generated by domain specific simulation engines.\nReveal the stenosis:Generative augmented physical (Computational Fluid Dynamics) modeling from Computer Tomography Scans\nFor example in the picture above a CFD simulation is used to augment ML algorithms that predict and explain those predictions. I mission critical systems (such as medical diagnostic systems) everything must be explainable.\nThe course syllabus to understand what elements of Wave II AI systems we will cover in this course.\n"});index.add({'id':1,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/systems-approach/','title':"A systems approach to AI",'content':"The four approaches towards AI The Turing Test Approach A 5-min behavioral intelligence test, where an interrogator chats with the player and at the end it guesses if the conversation is with a human or with a programmed machine. A Turing contest (Loebner Prize) is is held annually since 1991.\nThis course\u0026rsquo;s projects includes the Alexa prize which is not a turing test. The Alexa Prize creates social bots that engage in interesting, human-like conversations, not to make them indistinguishable from a human when compared side-by-side. Social bots may have ready access to much more information than a human.\n       Summary of the Turing Test The Alexa Prize is not a Turing Test   What capabilities we need to have to pass a turing test.\n Natural Language Processing Knowledge Representation Automated Reasoning Machine Learning Computer Vision Robotics  The last two capabilities are not needed for the verbal oriented Turing test but they are needed for what is called the total Turing test. According to this test, the player and the interrogator can communicate physically. For example, there is a hatch where the interrogator can pass objects to the player through. Obviously the player must have perception abilities to understand what object it is (5) and possibly a body that can manipulate the object (6). Embodied AI research is one of the hotter areas of AI today as we will see in the Agents section.\nThe Cognitive Model approach Newell, in his book \u0026ldquo;Unified Theories of Cognition\u0026rdquo;, defines cognition as:\n Memory, learning, skill Perception, motor behavior Problem solving, decision making, routine action Language Motivation, emotion Imagining, dreaming   How do we differentiate a cognitive model from a conceptual or statistical model? “Cognitive science is concerned with understanding the processes that the brain uses to accomplish complex tasks including perceiving, learning, remembering, thinking, predicting, problem solving, decision making, planning, and moving around the environment. The goal of a cognitive model is to scientifically explain one or more of these basic cognitive processes, or explain how these processes interact.”, \u0026ndash;Busemeyer \u0026amp; Diederich (2010)\n These theories of cognitive processes are tested via various cognitive architectures. Its important to realize that much of todays\u0026rsquo; debate about the path ahead in AI maps to the few different architectures. The hybrid architecture (symbolic and connection-oriented) is what is being investigated today by many research institutions.\nThe Syllogism-based approach The translation from Greek of the word syllogism is to support logic.\nThis approach emphasizes how we think the right way and encodes the pattern of logical arguments that can reach correct conclusions from a set of propositions (premises). Problem solving where the problem statement is expressed in a logic notation, matured in the 60s. As we have seen in the course introduction such rule-based systems are still with us and for small and deterministic state spaces can provide a very compact and elegant way to inference.\nLogic-based reasoning is coming back to fashion. One of the most promising areas is their application to interpretability (also known as explainability ) of deep learning based methods for e.g. classification in medical diagnosis. Probabilistic Logic Networks (PLN) are extensions of this approach to address problems with uncertainty.\nThe Rational Agent approach A rational agent acts to achieve the best outcome. The rational approach encompasses the syllogism and Turing-test approaches. We still need provably correct inference and the formal representations of logic as well as the ability to perceive, communicate, and learn to achieve a good outcome. But we need to generalize these approaches to include \u0026ldquo;good-enough\u0026rdquo; inference and adaptation to the changing environment contexts that the agent is facing without giving up on the mathematical formality that utility theory allows us to design such agents.\nThe agent facing a fire is an instructive example. There maybe no time for optimal reasoning to a conclusion (e.g. run) but a simple reflexive plan can offer the best outcome.\nAI as a distributed system approach As its evident from all existing approaches towards AI, multidisciplinary science that aims to create agents that can think and act humanly or rationally. This course starts the new decade filled with the promises of the previous one - AI is not only around the corner and it can take several decades of R\u0026amp;D for it to match human intelligence. Our purpose here is to (a) understand and appreciate the significant progress that certain components of AI have made over the last few years and (b) to be able to synthesize such components into AI systems that can at least solve domain-specific problems. In other words we are not trying to solve the most difficult and general AI problem as we don\u0026rsquo;t know its solution. We also can\u0026rsquo;t wait as we would like to participate in the GAI developments to begin with.\nA substantial part of AI is machine learning (ML) and that component alone is worth of at least a couple semesters. ML nowadays is used to process the visual sensing (computer vision), verbal commands (speech to text) and many other front-end functions using structures known as Deep Neural Networks (DNNs). These functions are usually effective in modeling the reflexive part of human brain. Their performance sometimes hides the enormous efforts by R\u0026amp;D teams to create carefully curated datasets for the task at hand. When supervised datasets are not enough for the design of reflexive agents policies, we need additional tools such as Deep Reinforcement Learning that offer the possibility to learn agent control policies from world models (or even without them) that in many instances means spending considerable time simulating the environment.\nAI is a system with the ability to represent the world and abstract concepts at multiple levels. If we are to draw the architecture of such system, it will have the ability to quickly change depending on the domain and task at hand. Just like origami, AI systems will morph into a suitable architecture, facilitated by high speed interconnections between its subsystems. The controller that controls such changes must be topology aware i.e. knowing the functional decomposition of the AI system and what support for representations and abstractions each subsystem can offer. How these can be combined and ultimately used, is something that needs to be learned. To generalize, such morphic control agents must be able to perform across task domains.\nAI distributed system comprising from a number of high-speed interconnected subsystems that are loosely coupled and communicate via a universal language. Line thickness indicates stronger coupling / dependecies between subsystems for the task at hand at this moment in time.\nIn a limited demonstration of such ability, closed worlds such as games, we have agents that can process thousands of pixels and can create abstractions at the symbolic level. Are they able to generalize ? Doubtful. Which brings us to the very interesting thought. For the vast majority of mission critical industries, we may reach in this decade a good enough performance level. The internet didn\u0026rsquo;t have 1 Gbps at each household even 5 years ago. But the moment we crossed the 1 Mbps level per user, at the hands of innovators, it managed to change the world as we know it despite its many initial performance issues. The internet does not kill, many people will argue but if anyone believes this analogy, todays\u0026rsquo; AI architecture, a bunch of service-oriented silos (APIs) offered by major technology firms, resembles the disconnected/siloed PC before the invention of HTTP and the internet era of the 90s. The protocol and controls that will allow such AI systems to communicate and by doing so demonstrate an ability to synthesize a non-trivial version of intelligence is one of the missing links.\nThe architecture of serf-driving cars in the late 2010s. To avoid wondering around the various disconnected use cases, we need to pick a domain that we can use as an application theme. Given the importance of the mission critical industries in the economy of every country, in this course we have selected robotics / self-driving cars. This domain requires the design of advanced agents that perceive the environment using noisy sensors, make decisions under uncertainty, actuate a host of electronics to execute decisions, communicate with humans in natural language or be able to sense driver psychological state and many more.\n"});index.add({'id':2,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/','title':"AI Software Stack",'content':"A typical AI stack today As we have seen from the syllabus, this course approaches supervised and unsupervised learning methods from an applied perspective - this means teaching concepts but at the same time looking how these concepts are applied in the industry to solve real world problems. In this respect here we take an architecture driven AI, presenting the components of AI in a form of a software stack but also how the components are mechanized in what we call ML Pipelines to provide the ML utility to applications. For a complete overview of real world ML pipelines used today go through the TFX paper in its entirety.\n{{ readFile \u0026ldquo;images/ai-stack.svg\u0026rdquo; }}\nAI Stack circa 2019\nLandscape of the AI ecosystem Due to the complexity and common interest to addresses industrial players are partnering to define and implement the necessary components for the complete automation of AI pipelines. This work is going in within the Linux Foundation and Deep Learning Foundation amongst many other open source communities.\nLinux Foundation - Deep Learning Foundation    Deep Learning Foundation ecosystem.\nThe four pipelines of an end to end data mining platform Example of end to end pipeline - serial arrangement\nExample of Data Pipeline\nExample of Model Training Pipeline\nExample of Model Evaluation and Validation Pipeline\nExample of Serving Pipeline\n"});index.add({'id':3,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/images/ai-way-of-working/','title':"The Way of Working in AI",'content':"A Positive Reinforcement Loop What are the disciplines that need to cross fertilize to get a system that possesses intelligence? Lets start with a diagram that show not only the disciplines but also a way of working for the many specialists involved.\nThe diagram above highlights three fundamental axes that can deliver a system-based approach to AI. The Z axis is the scientific axis where many disciplines such as psychology, neuroscience, mathematics and others make progress on. The X axis involves the ML/AI communities that borrow ideas from their colleagues in sciences and convert those theories and pragmatic findings into abstractions (models and methods). The model of the neuron, the perceptron, appeared in psychology journals many decades ago and despite its simplicity it is still the unit via which much more complicated neural networks are constructed from. Todays\u0026rsquo; models of Long-Term Short-Term Memory (LSTM), Replay Memory and many others not shown in the diagram (as its currently in draft form) are abstractions (models) of discoveries that scientists produced after tens of years of research. To use however these methods and models effectively, major hardware and software components need to be developed also known as computing and frameworks - these live in the Y axis. They are very important for the development of AI field that is known to be heavily experimental, requiring especially at the perceptive frontend significant computational power and automation.\nAt a very high level, progress in AI is made via the counterclockwise iteration Z -\u0026gt; X -\u0026gt; Y -\u0026gt; Z. AI engineers look at the neuroscience/psychology axis, map discoveries to points in the methods / models axis, and finally develop these methods in hardware architectures and software frameworks. But what can explain the Y -\u0026gt; Z flow? Frameworks in turn help the neuroscientists and psychologists as they can provide generative models of their own discoveries or help them simulate conditions that are not possible using their native tools.\nThis counter-clockwise multidisciplinary iteration acts as a positive feedback loop accelerating the progress in the AI space.\nIn this course we will be focusing on the methods/models and frameworks axis and understand what these models can offer us and how we can apply them in synthesizing an AI system at least for a domain of interest.\nLets us now look on how AI started, branched off to multiple schools of thought and view some examples.\nRoles in AI product development Who data scientists need to interact with, during the development of an app?\nData Scientist - Engineering Job Description This is a sample job description from Google.\nTechnologies machine-learning\nJob description\nMinimum qualifications:\n MS degree in a quantitative discipline (e.g., statistics, operations research, bioinformatics, economics, computational biology, computer science, mathematics, physics, electrical engineering, industrial engineering). 2 years of relevant work experience in data analysis or related field. (e.g., as a statistician / data scientist / computational biologist / bioinformatician). Experience with statistical software (e.g., R, Python, Julia, MATLAB, pandas) and database languages (e.g., SQL).  Preferred qualifications:\n PhD degree in a quantitative discipline as listed in Minimum Qualifications. 4 years of relevant work experience (e.g., as a statistician / computational biologist bioinformatician / data scientist), including deep expertise and experience with statistical data analysis such as linear models, multivariate analysis, stochastic models, sampling methods. Analytical engagements outside class work while at school can be included. Applied experience with machine learning on large datasets. Experience articulating business questions and using mathematical techniques to arrive at an answer using available data. Experience translating analysis results into business recommendations. Demonstrated skills in selecting the right statistical tools given a data analysis problem. Demonstrated effective written and verbal communication skills. Demonstrated leadership and self-direction. Demonstrated willingness to both teach others and learn new techniques.  About the job\nAs a Data Scientist, you will evaluate and improve Google\u0026rsquo;s products. You will collaborate with a multi-disciplinary team of engineers and analysts on a wide range of problems. This position will bring analytical rigor and statistical methods to the challenges of measuring quality, improving consumer products, and understanding the behavior of end-users, advertisers, and publishers.\nGoogle is and always will be an engineering company. We hire people with a broad set of technical skills who are ready to take on some of technology\u0026rsquo;s greatest challenges and make an impact on millions, if not billions, of users. At Google, data scientists not only revolutionize search, they routinely work on massive scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, Social to Local, Google engineers are changing the world one technological achievement after another.\nResponsibilities\n Work with large, complex data sets. Solve difficult, non-routine analysis problems, applying advanced analytical methods as needed. Conduct end-to-end analysis that includes data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations. Build and prototype analysis pipelines iteratively to provide insights at scale. Develop comprehensive understanding of Google data structures and metrics, advocating for changes where needed for both products development and sales activity. Interact cross-functionally with a wide variety of people and teams. Work closely with engineers to identify opportunities for, design, and assess improvements to google products. Make business recommendations (e.g. cost-benefit, forecasting, experiment analysis) with effective presentations of findings at multiple levels of stakeholders through visual displays of quantitative information. Research and develop analysis, forecasting, and optimization methods to improve the quality of Google\u0026rsquo;s user facing products; example application areas include ads quality, search quality, end-user behavioral modeling, and live experiments. At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google\u0026rsquo;s EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by emailing candidateaccommodations@google.com  Reearch Scientist, Machine Learning and Intelligence, Job Description This is another sample job description from Google. Spot the differences with the previous one.\nMinimum Qualifications\n PhD in Computer Science, related technical field or equivalent practical experience. Programming experience in one or more of the following: C, C++ and/or Python. Experience in Natural Language Understanding, Computer Vision, Machine Learning, Algorithmic Foundations of Optimization, Data Mining or Machine Intelligence (Artificial Intelligence). Contribution to research communities and/or efforts, including publishing papers at conferences such as NIPS, ICML, ACL, CVPR, etc.  Preferred Qualifications\n Relevant work experience, including experience working within the industry or as a researcher in a lab. Ability to design and execute on research agenda. Strong publication record.  About The Job Research in machine intelligence has already impacted user-facing services across Google including Search, Maps and Google Now. Google Research \u0026amp; Machine Intelligence teams are actively pursuing the next generation of intelligent systems for application to even more Google products. To achieve this, we’re working on projects that utilize the latest techniques in Machine Learning (including Deep Learning approaches like Google Brain) and Natural Language Understanding.\nWe’ve already been joined by some of the best minds, and we’re looking for talented Research Scientists that have applied experience in the fields of Machine Learning, Natural Language Processing and Machine Intelligence to join our team.\nWe do research differently here at Google. Research Scientists aren\u0026rsquo;t cloistered in the lab, but instead they work closely with Software Engineers to discover, invent, and build at the largest scale. Ideas may come from internal projects as well as from collaborations with research programs at partner universities and technical institutes all over the world. From creating experiments and prototyping implementations to designing new architectures, Research Scientists and Software Engineers work on challenges in machine perception, data mining, machine learning, and natural language understanding. You stay connected to your research roots as an active contributor to the wider research community by partnering with universities and publishing papers.\nThere is always more information out there, and Research and Machine Intelligence teams have a never-ending quest to find it and make it accessible. We\u0026rsquo;re constantly refining our signature search engine to provide better results, and developing offerings like Google Instant, Google Voice Search and Google Image Search to make it faster and more engaging. We\u0026rsquo;re providing users around the world with great search results every day, but at Google, great just isn\u0026rsquo;t good enough. We\u0026rsquo;re just getting started.\nResponsibilities\n Participate in cutting edge research in machine intelligence and machine learning applications. Develop solutions for real world, large scale problems.  At Google, we don’t just accept difference - we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires accommodation, please let us know.\nTo all recruitment agencies Google does not accept agency resumes. Please do not forward resumes to our jobs alias, Google employees or any other company location. Google is not responsible for any fees related to unsolicited resumes\nA Note Conways\u0026rsquo; Law  \u0026ldquo;Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\u0026rdquo; http://www.melconway.com/Home/Conways_Law.html\n \u0026ldquo;We do research differently here at Google. Research Scientists aren\u0026rsquo;t cloistered in the lab, but instead they work closely with Software Engineers to discover, invent, and build at the largest scale.\u0026rdquo;\nContrast this to an organizational structure that isolates researchers from product development. What about Alphabet\u0026rsquo;s X https://x.company/ ?\n"});index.add({'id':4,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/','title':"Intelligent Agents and Representations",'content':"Agent-Environment Interface  An agent is a computer system that is situated in some environment, and that is capable of autonomous action in this environment in order to meet its design objectives.\n $$x^2$$\n"});index.add({'id':5,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/test/','title':"Math Typesetting",'content':"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so:  {{ if or .Params.math .Site.Params.math }} {{ partial \u0026quot;math.html\u0026quot; . }} {{ end }}  To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files.  Note: Use the online reference of Supported TeX Functions Examples Inline math: $$ \\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887… $$\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n"});index.add({'id':6,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-intro-deck/','title':"Ai Intro Deck",'content':"footer: CS-GY-6613 slidenumbers: true autoscale: true\nIntroduction to AI Pantelis Monogioudis, Jan 2020\n Anything wrong with this boat? ^ Imagine you are in 1844 (Cumberland Basin, April 1844) Value of Commerce. Anything wrong with this boat?\n AI Development - Wave I  Wave I is known as Good Old-Fashioned AI (GOFAI) It is still being used today.   AI Development - Wave II  Wave II is known as Connectionism It is what Deep Learning is today.   AI Development - Wave III  Wave III is known as AGI The vide is not representative of AGI. Its an illustration on what Waves I \u0026amp; II can do in robotics if we know all dynamics of the anthropoid.   The four approaches towards AI  Turing Test Approach  A 5-min behavioral intelligence test, where an interrogator chats with the player and at the end it guesses if the conversation is with a human or with a programmed machine. What capabilities we need to have to pass a turing test.   Natural Language Processing Knowledge Representation Automated Reasoning Machine Learning Computer Vision Robotics   Is the Alexa Prize a Turing Test ?  Cognitive Models   A cognitive model is a theory on how the mind works. These theories of cognitive processes are tested via various cognitive architectures.\n  Historically, there are many architectures and open source implementations available such as ACT-R, SOAR (U of Michigan).\n  Its important to realize that much of todays\u0026rsquo; debate about the path ahead in AI maps to the few different architectures. The hybrid architecture (symbolic and connection-oriented) is what is being investigated today by many research institutions. Many publications from big AI shops (Deep Mind, OpenAI, Facebook, etc.) tests at least an element of a theory of cognition and suggest a corresponding architecture.\n   The Syllogism-based approach   The translation from Greek of the word syllogism is to support logic.\n  This approach emphasizes how we think the right way and encodes the pattern of logical arguments that can reach correct conclusions from a set of propositions (premises). Problem solving where the problem statement is expressed in a logic notation, matured in the 60s. Such rule-based systems are still with us and for small and deterministic state spaces can provide a very compact and elegant way to inference.\n  Logic-based reasoning is coming back to fashion. One of the most promising areas is their application to interpretability (also known as explainability ) of deep learning based methods for e.g. classification in medical diagnosis. Probabilistic Logic Networks (PLN) are extensions of this approach to address problems with uncertainty.\n   The Rational Agent approach  A rational agent acts to achieve the best outcome according to an internal performance measure given the evidence provided by the percept sequence and prior knowledge the agent has. Does this definition remind you of something ?  Bayes Theorem   The rational approach encompasses the syllogism and Turing-test approaches. We still need provably correct inference and the formal representations of logic as well as the ability to perceive, communicate, and learn to achieve a good outcome. But we need to generalize these approaches to include \u0026ldquo;good-enough\u0026rdquo; inference and adaptation to the changing environment contexts that the agent is facing without giving up on the mathematical formality that utility theory allows us to design such agents. The agent facing a fire is an instructive example. There maybe no time for optimal reasoning to a conclusion (e.g. run) but a simple reflexive plan can offer the best outcome.   AI Problem Specification for Rational Agents   Performance\n  Environment\n  Actuators\n  Sensors\n  Sometimes we have no physical environments but virtual ones. Its not only the various chatbots and smart speakers of today - its also simulated task environments where we host the digital twin models of everything we care about.\n   The Bell Labs research AI environment  This maze was constructed in Bell Labs for the Unix 50 competition to demonstrate the ability of intelligent robotic agents to search for unknown objects in it. It is constantly enhanced with more challenging parts such as collaboration between robots. A digital twin of the robot and the environment is modeled inside Unity.   A real self-driving car circa 2018 "});index.add({'id':7,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/','title':"Course Introduction",'content':""});index.add({'id':8,'href':'/cs-gy-6613-spring-2020/docs/projects/','title':"Projects",'content':"The following projects needs to be delivered by the deadlines.\n Surface Type Classification - Due 2/23/2020 11:59pm Lifelong Learning - Robotic Vision - Due 3/29/2020 11:59pm TBD - Due 5/10/2020 11:59pm  "});index.add({'id':9,'href':'/cs-gy-6613-spring-2020/docs/projects/setup/','title':"Setup",'content':""});index.add({'id':10,'href':'/cs-gy-6613-spring-2020/categories/','title':"Categories",'content':""});index.add({'id':11,'href':'/cs-gy-6613-spring-2020/','title':"CS-GY-6613 Artificial Intelligence - Spring 2020",'content':"Welcome to CS-GY-6613 ! Logistics Time/location: Brooklyn Campus, Mon 6.00 PM - 8.30 PM at RGSH 315.\nCommunication: We will use Slack for all communications: announcements and questions related to lectures, assignments, and projects. All registered students got an gmail invitation to Slack on 1/28/2020.\nInstructor Pantelis Monogioudis, Ph.D (Bell Labs) Head of Applied ML Research Group Murray Hill, NJ\nTeaching Assistants TA\u0026rsquo;s contact will be announced as soon as I receive confirmation from HR that were hired. I am targeting two TAs for this class.\nWhat is this course about This course is all about the algorithms and methods that will enable agents that exhibit forms of intelligence and autonomy.\nGrading  Final (30%) Midterm (30%) Projects (40%)  "});index.add({'id':12,'href':'/cs-gy-6613-spring-2020/docs/','title':"Docs",'content':""});index.add({'id':13,'href':'/cs-gy-6613-spring-2020/docs/projects/project-1/','title':"Project 1 - Surface Type Classification",'content':"  Your first project description is published in https://www.kaggle.com/c/career-con-2019/overview\n  You must submit your assignment with the results by 11:59pm 2/23/2020. The submission will be done by sharing the github/kaggle notebook with the TA.\n  "});index.add({'id':14,'href':'/cs-gy-6613-spring-2020/docs/projects/project-2/','title':"Project 2 - Continual Learning for Robotic Perception",'content':"One of the greatest goals of AI is building an artificial continual learning agent which can construct a sophisticated understanding of the external world from its own experience through the adaptive, goal-oriented and incremental development of ever more complex skills and knowledge. Continual learning is essential in robotics where high dimensional data streams need to be constantly processed and where naïve continual learning strategies have been shown to suffer from catastrophic forgetting.\nYou will use this dataset and evaluate your methods for New Classes (NC) scenario. This is a very active area in AI right now. See here\n"});index.add({'id':15,'href':'/cs-gy-6613-spring-2020/docs/projects/project-3/','title':"Project 3",'content':""});index.add({'id':16,'href':'/cs-gy-6613-spring-2020/tags/','title':"Tags",'content':""});index.add({'id':17,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/','title':"Week 1 - Introduction to AI",'content':"Summary of the Lecture and Action Items Action Items Technical  Read Chapters 1 and 2 of the AIMA book. Set up your programming environment following the Projects -\u0026gt; Setup guide.  Administrative  Join Slack workplace that was sent to you by gmail. Fill the form asking about your background.  "});index.add({'id':18,'href':'/cs-gy-6613-spring-2020/docs/projects/setup/env/','title':"Your Programming Environment",'content':"Starting Jupyter in Google Colab The runtime performance will greatly improve for some projects using the free GPU resources provided by Google Colab. In this course we will make use of these facilities - the good news is that you have an account in Google Colab as most of you have a google account. If not go ahead and create one to be able to login into Google colab. You will need Google Colab for all your projects so that you can demonstrate that your results can be replicated. In addition Colab has many features that come handy.\nI heavily borrowed from Geron\u0026rsquo;s book for the following.\nSetup Anaconda Python When using Anaconda, you need to create an isolated Python environment dedicated to this course. This is recommended as it makes it possible to have a different environment for each project, with potentially different libraries and library versions:\n$ conda create -n cs634 python=3.6 anaconda $ conda activate cs634  This creates a fresh Python 3.6 environment called cs634 (you can change the name if you want to), and it activates it. This environment contains all the scientific libraries that come with Anaconda. This includes all the libraries we will need (NumPy, Matplotlib, Pandas, Jupyter and a few others), except for TensorFlow, so let\u0026rsquo;s install it:\n$ conda install -n cs634 -c conda-forge tensorflow  This installs the latest version of TensorFlow available for Anaconda (which is usually not the latest TensorFlow version) in the cs634 environment (fetching it from the conda-forge repository). If you chose not to create an cs634 environment, then just remove the -n cs634 option.\nNext, you can optionally install Jupyter extensions. These are useful to have nice tables of contents in the notebooks, but they are not required.\n$ conda install -n cs634 -c conda-forge jupyter_contrib_nbextensions  Kaggle Assuming you have activated the cs634 conda environment, follow the directions here to install the Kaggle command line interface (CLI). You will need Kaggle for all your projects. You guessed it right - all the projects in this course are in fact Kaggle competitions. Not only you will get to compete (your ranking relative to others does not matter per se), but as you improve your knowledge over time you can revisit these competitions and see how your score improves.\nYou are all set! Next, jump to the Starting Jupyter section.\nStarting Jupyter locally If you want to use the Jupyter extensions (optional, they are mainly useful to have nice tables of contents), you first need to install them:\n$ jupyter contrib nbextension install --user  Then you can activate an extension, such as the Table of Contents (2) extension:\n$ jupyter nbextension enable toc2/main  Okay! You can now start Jupyter, simply type:\n$ jupyter notebook  This should open up your browser, and you should see Jupyter\u0026rsquo;s tree view, with the contents of the current directory. If your browser does not open automatically, visit localhost:8888. Click on index.ipynb to get started!\nNote: you can also visit http://localhost:8888/nbextensions to activate and configure Jupyter extensions.\nStarting Jupyter in Google Colab The runtime performance will greatly improve for some projects using the free GPU resources provided by Google Colab. In this course we will make use of these facilities - the good news is that you have an account in Google Colab as most of you have a google account. If not go ahead and create one to be able to login into Google colab. You will need Google Colab for all your projects so that you can demonstrate that your results can be replicated. In addition Colab has many features that come handy.\nGit / Github Git is the defacto standard when it comes to code version control. Learning basic git commands takes less than half an hour. However, to install git and understand the principle behind git, please go over Chapters 1 and 2 of the ProGit book.\nAs we have discussed in class you need to be able to publish your work in Github so you need to create a Github account. Then you will use the git client for your operating system to interact with github and iterate on your projects. You may be using Kaggle or Colab hosted notebooks but the underlying technology that powers such web-frontends when it comes to committing the code and seeing version numbers in your screen is git.\nIn addition, almost no data science project starts in vacuum - there is almost always software that will be inherited to be refined.\n"});})();