'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/','title':"Course Introduction",'content':" AI Evolution according to DARPA If engineering difficulty has a pinnacle today this must be in AI domains that combines ML, optimal control and planning. autonomous cars and humanoids from Boston Dynamics fit the bill.\nInitially there were rules.\n In the 1980s knowledge-base systems that hard-coded knowledge about the world in formal languages.  IF this happens, THEN do that.  They failed to get significant traction as the number of rules that are needed to model the real world exploded. However, they are still in use today in vertical modeling domains e.g. fault management. For example Rule Based Engines are used today in many complex systems that manage mission critical infrastructures e.g. ONAP.  The introduction of advanced AI methods few years ago, created a situation we can explain with the following analogy.\nA nautical analogy on where we are today on AI for mission critical systems. Can you notice anything strange with this ship (Cumberland Basin, photo taken April 1844)?\nTo put order into the many approaches and methods for delivering AI in our lives, DARPA classified AI development in terms of \u0026ldquo;waves\u0026rdquo;.\n         Wave I: GOFAI Wave II: Connectionism Wave III: AGI   In the 1980s Rule Based Engines started to be applied manifesting the first wave of AI introduction. In this example you see a system that performs highway trajectory planning. A combination of cleverly designed rules does work and offers real time performance but cannot generalize and therefore have acceptable performance in other environments.\nWave II srarted soon after 2010 - we started to apply a different philosophy in solving intelligent tasks such as object classification. The philosophy of connectionism and the so called deep neural network architectures, dominate today relative simple (and mostly self-contained) tasks.\nWave III is at present an active research area driven primarily from our inability to implement with just deep neural networks things like long-term goal planning, causality, extract meaning from text like humans do, explain the decisions of neural networks, transfer the learnings from one task to another, even similar, task. Artificial General Intelligence is the term usually associated with such capabilities.\nFurther, we will see a fusion of disciplines such as physical modeling and simulation with representation learning to help deep neural networks learn using data generated by domain specific simulation engines.\nReveal the stenosis:Generative augmented physical (Computational Fluid Dynamics) modeling from Computer Tomography Scans\nFor example in the picture above a CFD simulation is used to augment ML algorithms that predict and explain those predictions. I mission critical systems (such as medical diagnostic systems) everything must be explainable.\nConsult the course Syllabus to understand what elements of Wave II AI systems we will cover in this course.\n"});index.add({'id':1,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/numpy-pandas/','title':"Numerical Python (Numpy/Scipy and Pandas) Tutorials",'content':" Standard Python Below is a list of recommended courses you can attend to. We will go over briefly basic Python in this lecture. The tutorials below are self contained and can remind you the basics.\n CodeAcademy Data Science Path. Take Python modules 4-10. This course contains Numpy and Panda intro as well.\n Kaggle Python Course\n Google Python Class This is a bit dated as it covers Python 2, but it is still highly regarded as Python 3 and 2 have few differences.\n  Numpy specific We will go Numpy Tutorial from Stanford\u0026rsquo;s CS231n. \u0026gt; Numpy Cheatsheet\nPandas (optional) Pandas may not be needed for the AI course. In any case, this is the effectively \u0026ldquo;official\u0026rdquo; documentation on Pandas: Pandas: powerful Python data analysis toolkit\n"});index.add({'id':2,'href':'/cs-gy-6613-spring-2020/docs/syllabus/','title':"Syllabus",'content':" Syllabus The course schedule below highlights our journey to understand the multiple subsystems and how they can be connected together to create compelling but, currently, domain specific forms of intelligence.\nBooks Artificial Intelligence: A Modern Approach, by Stuart Russell, 3rd edition, 2010 and also here.\nThe publisher is about to release the 4th edition (2020) of this classic. We will be monitoring availability in bookstores but it does not seem likely this edition to appear on time for the Spring 2020 class.\nOther recommended texts are: (a) DRL: \u0026ldquo;Foundations of Deep Reinforcement Learning\u0026rdquo;, by Graesser \u0026amp; Keng, 2020. (b) GERON: \u0026ldquo;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\u0026rdquo;, 2nd Edition, by Geron, 2019. \u0026copy; DL: https://www.deeplearningbook.org/ (free)\nSchedule The schedule is based on Academic Calendar Spring 2020:\nPart I: Perception and Machine Learning  Week 1 (1/27/2020) We start with an introduction to AI and present a systems approach towards it. We develop a map that will guide us through the rest of the course as we deep dive into each component embedded into AI agents. Reading: AIMA Chapters 1 \u0026amp; 2.\n Week 2 (2/3/2020) The perception subsystem is the first stage of many AI systems including our brain. Its function is to process and fuse multimodal sensory inputs. Perception is implemented via a number of reflexive agents that map directly perceived state to an primitive action such as regressing on the frame coordinates of an object in the scene. We present the supervised learning problem both for classification and regression, starting with classical ML algorithms. Reading: AIMA Chapter 18.\n Week 3 (2/10/2020) We expand into Deep neural networks. DNNs are developed bottom up from the Perceptron algorithm. MLPs learn via optimization approaches such as Stochastic Gradient Descent. We deep-dive into back-propagation - a fundamental algorithm that efficiently trains DNNs. Reading: DL Chapter 6\n Week 4: (2/[17]/2020) Since this is President\u0026rsquo;s Day, we need to decide which day we can have the lecture. We dive into the most dominant DNN architecture today - Convolutional Neural Networks (CNNs) and Recursive Neural Networks (RNNs). Reading: DL Chapter 9 \u0026amp; 10.\n Week 5: (2/24/2020) When agents move in the environment they need to abilities such as scene understanding. We will go through few key perception building blocks such as Object Detection, Semantic and Instance Segmentation. Some of these building blocks (autoencoders) are instructive examples of representations learning that will be shown to be an essential tool in the construction of environment state representations. Reading: Various papers\n  Part II: Reasoning and Planning  Week 7: (3/2/2020) In this lecture we introduce probabilistic models that process the outputs of perception (measurement / sensor model) and the state transitions and understand how the agent will track / update its belief state over time. This is a achieved with probabilistic recursive state estimation algorithms and dynamic bayesian networks. Reading: AIMA Chapters 14 \u0026amp; 15.\n Week 6: (3/9/2020) After the last lecture, the agent has a clear view of the environment state such as what and where the objects that surround it are, its able to track them as they potentially move. It needs to plan the best sequence of actions to reach its goal state and the approach we take here is that of problem solving. In fact planning and problem solving are inherently connected as concepts. If the goal state is feasible then the problem to solve becomes that of search. For instructive purposes we start from simple environmental conditions that are fully observed, known and deterministic. This is where the A* algorithm comes in. We then relax some of the assumptions and treat environments that are deterministic but the agent takes stochastic actions or when both the environment and agent actions are stochastic. Reading: AIMA Chapters 3 \u0026amp; 4.\n Week 8: (3/16/2020) Enjoy your Spring Break.\n Week 9: (3/23/2020) - This is your Midterm Test (2h)\n Week 10: (3/30/2020) We continue on our path (literally :) ) to investigate what happens when we do not just care about reaching our goal state, but when we, in addition, need to do so with optimality. Optimal planning under uncertainty is perhaps the cornerstone application today in robotics and other fields. Readings: AIMA Chapters 10 \u0026amp; 11 and selected papers.\n  Part III: Deep Reinforcement Learning  Week 11: (4/6/2020) We now make a considerable extension to our assumptions: the utility of the agent now depends on a sequence of decisions and, further, the stochastic environment offers a feedback signal to the agent called reward. We review how the agent\u0026rsquo;s policy, the sequence of actions, can be calculated when it fully observes its current state (MDP) and also when it can only partially do so (POMDP). The algorithms that learn optimal policies in such settings are known as Reinforcement Learning (RL). Today they are enhanced with the Deep Neural Networks that we met in Part I, to significantly improve the expected reward since DNNs are excellent approximators to the various functions embedded in such problems. We conclude with the basic taxonomy of the algorithm space for DRL Problems. In this course we are focusing on model free methods that have general applicability. Readings: AIMA Chapter 16 \u0026amp; 17, DRL Chapter 1. This lectured will be delivered in person by Gurudutt Hossangadi as I will be out of town.\n Week 12: (4/13/2020) In this lecture we start on the exploration of the various algorithms that do not depend on learning any model of the environment dynamics. The first algorithm is the policy-based algorithm called REINFORCE and its extensions especially the Advantage Actor Critic (A2C). DRL Chapter 2, 6 and 7.\n Week 13: (4/20/2020) Staying in the setting of model-free algorithms we will work with the so-called value-based methods and the State Action Reward State Action (SARSA) algorithms. This is the ancestor of algorithms such as DQN, DDQN with Prioritized Experience Replay (PER) that will also be covered. Readings: DRL Chapter 3, 4 and 5.\n  Part IV: Knowledge Bases and Communication  Week 14: (4/27/2019) Every intelligent agent needs to know how the world works for each task it encounters. These facts are stored in its Knowledge Base also known as Knowledge Graph. In addition as the agent affects the environment it must be able to create the right representations using its perception systems and update the knowledge base with dynamic content. Finally it needs to draw conclusions - aka infer new facts from existing ones - that will help the task at hand. Readings: AIMA Chapter 12 and selected papers.\n Week 15: (5/05/2020) We are all familiar that natural language is the prime means of communication between humans to collaboratively complete successfully tasks or simply share our knowledge bases. How can we achieve the same objectives when we enable communicate with intelligent agents. Is natural language the universal language that together with imitation is the missing link in our ability to (re)task robots from intelligent assistants to cognitive collaborative robots in our factories of the future? Readings: AIMA Chapter 23 and selected papers.\n Week 16: (5/11/2020) In this last lecture, we review the main points of what we learned and emphasize what kind of questions you are expected to answer in the final exam. Final projects are due 5/10/2020 11:59pm.\n Week 17: (5/18/2020) Good luck with your final test.\n  "});index.add({'id':3,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/','title':"A systems approach to AI",'content':" The four approaches towards AI The Turing Test Approach A 5-min behavioral intelligence test, where an interrogator chats with the player and at the end it guesses if the conversation is with a human or with a programmed machine. A Turing contest (Loebner Prize) is is held annually since 1991.\nThis course\u0026rsquo;s projects includes the Alexa prize which is not a turing test. The Alexa Prize creates social bots that engage in interesting, human-like conversations, not to make them indistinguishable from a human when compared side-by-side. Social bots may have ready access to much more information than a human.\n       Summary of the Turing Test The Alexa Prize is not a Turing Test   What capabilities we need to have to pass a turing test.\n Natural Language Processing Knowledge Representation Automated Reasoning Machine Learning Computer Vision Robotics  The last two capabilities are not needed for the verbal oriented Turing test but they are needed for what is called the total Turing test. According to this test, the player and the interrogator can communicate physically. For example, there is a hatch where the interrogator can pass objects to the player through. Obviously the player must have perception abilities to understand what object it is (5) and possibly a body that can manipulate the object (6). Embodied AI research is one of the hotter areas of AI today as we will see in the Agents section.\nThe Cognitive Model approach Newell, in his book \u0026ldquo;Unified Theories of Cognition\u0026rdquo;, defines cognition as:\n Memory, learning, skill Perception, motor behavior Problem solving, decision making, routine action Language Motivation, emotion Imagining, dreaming   How do we differentiate a cognitive model from a conceptual or statistical model? “Cognitive science is concerned with understanding the processes that the brain uses to accomplish complex tasks including perceiving, learning, remembering, thinking, predicting, problem solving, decision making, planning, and moving around the environment. The goal of a cognitive model is to scientifically explain one or more of these basic cognitive processes, or explain how these processes interact.”, \u0026ndash;Busemeyer \u0026amp; Diederich (2010)\n These theories of cognitive processes are tested via various cognitive architectures. Its important to realize that much of todays\u0026rsquo; debate about the path ahead in AI maps to the few different architectures. The hybrid architecture (symbolic and connection-oriented) is what is being investigated today by many research institutions.\nThe Syllogism-based approach The translation from Greek of the word syllogism is to support logic.\nThis approach emphasizes how we think the right way and encodes the pattern of logical arguments that can reach correct conclusions from a set of propositions (premises). Problem solving where the problem statement is expressed in a logic notation, matured in the 60s. As we have seen in the course introduction such rule-based systems are still with us and for small and deterministic state spaces can provide a very compact and elegant way to inference.\nLogic-based reasoning is coming back to fashion. One of the most promising areas is their application to interpretability (also known as explainability ) of deep learning based methods for e.g. classification in medical diagnosis. Probabilistic Logic Networks (PLN) are extensions of this approach to address problems with uncertainty.\nThe Rational Agent approach A rational agent acts to achieve the best outcome. The rational approach encompasses the syllogism and Turing-test approaches. We still need provably correct inference and the formal representations of logic as well as the ability to perceive, communicate, and learn to achieve a good outcome. But we need to generalize these approaches to include \u0026ldquo;good-enough\u0026rdquo; inference and adaptation to the changing environment contexts that the agent is facing without giving up on the mathematical formality that utility theory allows us to design such agents.\nThe agent facing a fire is an instructive example. There maybe no time for optimal reasoning to a conclusion (e.g. run) but a simple reflexive plan can offer the best outcome.\nAI as a distributed system approach As its evident from all existing approaches towards AI, multidisciplinary science that aims to create agents that can think and act humanly or rationally. This course starts the new decade filled with the promises of the previous one - AI is not only around the corner and it can take several decades of R\u0026amp;D for it to match human intelligence. Our purpose here is to (a) understand and appreciate the significant progress that certain components of AI have made over the last few years and (b) to be able to synthesize such components into AI systems that can at least solve domain-specific problems. In other words we are not trying to solve the most difficult and general AI problem as we don\u0026rsquo;t know its solution. We also can\u0026rsquo;t wait as we would like to participate in the GAI developments to begin with.\nA substantial part of AI is machine learning (ML) and that component alone is worth of at least a couple semesters. ML nowadays is used to process the visual sensing (computer vision), verbal commands (speech to text) and many other front-end functions using structures known as Deep Neural Networks (DNNs). These functions are usually effective in modeling the reflexive part of human brain. Their performance sometimes hides the enormous efforts by R\u0026amp;D teams to create carefully curated datasets for the task at hand. When supervised datasets are not enough for the design of reflexive agents policies, we need additional tools such as Deep Reinforcement Learning that offer the possibility to learn agent control policies from world models (or even without them) that in many instances means spending considerable time simulating the environment.\nAI is a system with the ability to represent the world and abstract concepts at multiple levels. If we are to draw the architecture of such system, it will have the ability to quickly change depending on the domain and task at hand. Just like origami, AI systems will morph into a suitable architecture, facilitated by high speed interconnections between its subsystems. The controller that controls such changes must be topology aware i.e. knowing the functional decomposition of the AI system and what support for representations and abstractions each subsystem can offer. How these can be combined and ultimately used, is something that needs to be learned. To generalize, such morphic control agents must be able to perform across task domains.\nAI distributed system comprising from a number of high-speed interconnected subsystems that are loosely coupled and communicate via a universal language. Line thickness indicates stronger coupling / dependecies between subsystems for the task at hand at this moment in time.\nIn a limited demonstration of such ability, closed worlds such as games, we have agents that can process thousands of pixels and can create abstractions at the symbolic level. Are they able to generalize ? Doubtful. Which brings us to the very interesting thought. For the vast majority of mission critical industries, we may reach in this decade a good enough performance level. The internet didn\u0026rsquo;t have 1 Gbps at each household even 5 years ago. But the moment we crossed the 1 Mbps level per user, at the hands of innovators, it managed to change the world as we know it despite its many initial performance issues. The internet does not kill, many people will argue but if anyone believes this analogy, todays\u0026rsquo; AI architecture, a bunch of service-oriented silos (APIs) offered by major technology firms, resembles the disconnected/siloed PC before the invention of HTTP and the internet era of the 90s. The protocol and controls that will allow such AI systems to communicate and by doing so demonstrate an ability to synthesize a non-trivial version of intelligence is one of the missing links.\nThe architecture of serf-driving cars in the late 2010s. To avoid wondering around the various disconnected use cases, we need to pick a domain that we can use as an application theme. Given the importance of the mission critical industries in the economy of every country, in this course we have selected robotics / self-driving cars. This domain requires the design of advanced agents that perceive the environment using noisy sensors, make decisions under uncertainty, actuate a host of electronics to execute decisions, communicate with humans in natural language or be able to sense driver psychological state and many more.\n"});index.add({'id':4,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/','title':"Week 1 - Introduction to AI",'content':""});index.add({'id':5,'href':'/cs-gy-6613-spring-2020/docs/lectures/learning-problem/','title':"Week 2a - The Learning Problem",'content':" The Learning Problem We have seen various agent designs but the ones that we will concentrate on this course are ones that can form partially observed (PO) environment states using various sensing architectures. The perception block we have seen in the case where the agent is an autonomous car achieves that for example, and perception is rich in what is called Machine Learning (ML). ML is a substantial subset of AI today and is not limited to just the perception part of an agent.\nAlmost all machine learning algorithms depend heavily on the representation of the data they are given. Each piece of, relevant to the problem, information that is included in the representation is known as a feature. Today\u0026rsquo;s ML approaches such as deep learning actually learn the most suitable representations for the task at hand (still with a some help from experts) - an example is shown in the picture below.\nHierarchical Feature Learning\nThe Supervised Learning Problem Statement Let us start with a classic formal definition of the supervised learning problem.\nVapnik\u0026rsquo;s formulation of the learning problem (enhanced)\nThe description below is taken from Vadimir Vapnik\u0026rsquo;s classic book Statistical Learing Theory, albeit with some enhancements on terminology to make it more in line with our purposes.\nThe generator is a source of situations that determines the environment in which the target function (he calls it supervisor) and the learning algorithm act. Here we consider the simplest environment: the data generator generates the vectors $\\mathbf{x} \\in \\mathcal{X}$ independently and identically distributed (i.i.d.) according to some unknown (but fixed) probability distribution function $p(x)$.\nThese vectors are inputs to the target function (or operator); the target operator returns the output values $\\mathbf{y}$. The target operator which transforms the vectors $\\mathbf{x}$ into values y, is unknown but we know that it exists and does not change.\nThe learning algorithm observes the training dataset,\n$${ (\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_m, y_m) }$$\nwhich contain input vectors $\\mathbf{x}$ and the target response $\\mathbf{y}$. During this period, the learning algorithm constructs some operator which will he used for prediction of the supervisor\u0026rsquo;s answer $y_i$ on any specific vector $\\mathbf{x}_i$ generated by the generator. The goal of the learning algorithm is to construct an appropriate approximation of the target function - we will call this a hypothesis. The hypothesis can be iteratively constructed so the final hypothesis is the one that is used to produce the label $\\hat{y}$.\nTo be a mathematically correct, this general scheme of learning from examples needs some clarification. First of all, we have to describe what kind of operators are used by the target function. In this book. we suppose that the target function returns the output $\\mathbf{y}$ on the vector $\\mathbf{x}$ according to a conditional distribution function $p(\\mathbf{y} | \\mathbf{x})$ (this includes the case when the supervisor uses some function $\\mathbf{y} = f(\\mathbf{x}))$.\nThe learning algorithm observes the training set which is drawn randomly and independently according to a joint distribution function $p(\\mathbf{x} , \\mathbf{y}) = p(\\mathbf{x}) p(\\mathbf{y} | \\mathbf{x})$. Recall that we do not know this function but we do know that it exists. Using this training set, the learning algorithm constructs an approximation to the unknown function. The ability to correctly predict / classify when observing the test set, is called generalization.\nA couple of examples of supervised learning are shown below:\nExamples from the MNIST training dataset used for classification\nBirdseye view of home prices - Zillow predicts prices for similar homes in the same market. This is a regression problem.\nUnsupervised Learning In unsupervised learning, we present a training set ${ \\mathbf{x}_1, \\dots, \\mathbf{x}_m }$ without labels. The most common unsupervised learning method is clustering. We construct a partition of the data into a number of $K$ clusters, such that a suitably chosen loss function is minimized for a different set of input data (test).\nClustering showing two classes and the exemplars per class\nSemi-supervised Learning and Active Learning Semi-supervised learning stands between the supervised and unsupervised methods. One of the hottest methods in this category is the so called Active learning. In many practical settings we simply cannot afford to label /annotate all $\\mathbf x$ for very large $m$, and we need to select the ones that greedily result into the biggest performance metric gain (e.g. accuracy).\nReinforcement Learning In reinforcement learning, in this course we will treat RL in detail later, a teacher is not providing a label (as in supervised learning) but rather a reward that judges whether the agent\u0026rsquo;s action results on favorable environment states. In reinforcement learning we can learn end to end optimal mappings from perceptions to actions.\n"});index.add({'id':6,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/linear-regression/','title':"Linear Regression",'content':" Linear Regression Now that we have introduced somewhat more formally the learning problem and its notation lets us study a simple but instructive regression problem from Chapter 1 of Bishop\u0026rsquo;s book that is known in the statistics literature as shrinkage. Note that in many figures below the label is denoted as $t$ rather than $y$ as used in the equations below.\nSuppose that we are given the training set $\\mathbf{x} = {x_1,\u0026hellip;,x_m}$ together with their labels, the vectors $\\mathbf{y}$. We need to construct a model such that a suitably chosen loss function is minimized for a different set of input data, the so-called test set. The ability to correctly predict when observing the test set, is called generalization.\nTraining Dataset (m=10) for the Regression Model. The green curve is the uknown target function.\nSince the output $y$ is a continuous variable then the supervised learning problem is called a regression problem (otherwise its a classification problem). The dataset is generated (in data scienece these datasets are called synthetic) by the function $sin(2 \\pi x) + ϵ$ where $x$ is a uniformly distributed random variable and $ϵ$ is $N(\\mu=0.0, \\sigma^2=0.3)$. This target function is completely unknown to us - we just mention it here for completeness.\nLet us now pick the hypothesis set that correspond to polynomials of the following form,\n$$g(\\mathbf{w},x_i) = w_0 + w_1 x_i + w_2 x_i^2 + \u0026hellip; + w_M x_i^M$$\nOur job is to find $\\mathbf{w}$ such that the polynomial above fits the data we are given - as we will see there are multiple hypothesis that can satisfy this requirement. To gauge our investigation, we need to define a metric, an error or loss function in fact, that is also a common metric in regression problems of this nature. This is the Mean Squared Error (MSE) function.\n$$L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m {(g(\\mathbf{w},x_i)-y_i)}^2$$\nThe loss function chosen for this regression problem, corresponds to the sum of the squares of the displacements of each data point and our hypothesis. The sum of squares in the case of Gaussian errors gives raise to an (unbiased) Maximum Likelihood estimate of the model parameters. Contrast this to sum of absolute differences.\nNow our job has become to choose two things: the weight vector $\\mathbf{w^*}$ and $M$ the order of the polynomial. Both define our hypothesis. If you think about it, the order $M$ defines the model complexity in the sense that the larger $M$ becomes the more the number of weights we need to estimate and store. Obviously this is a trivial example and storage is not a concern here but treat this example as instructive for that it applies in many far for complicated settings.\nObviously you can reduce the training error to almost zero by selecting a model that is complicated enough (M=9) to perfectly fit the training data (if m is small).\nBut this is not what you want to do. Because when met with test data, the model will perform far worse than a less complicated model that is closer to the true model (e.g. M=3). This is a central observation in statistical learning called overfitting. In addition, you may not have the time to iterate over M (very important in online learning settings).\nTo avoid overfitting we have multiple strategies. One straightforward one is evident by observing the wild oscillations of the $\\mathbf{w}$ elements as the model complexity increases. We can penalize such oscillations by introducing the $l_2$ norm of $\\mathbf{w}$ in our loss function.\n$$L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m {(g(\\mathbf{w},x_i)-y_i)}^2 + \\frac{\\lambda}{2} ||\\mathbf{w}||^2$$\nThis type of solution is called regularization and because we effectively shrink the weight dynamic range it is also called in statistics shrinkage or ridge regression. We have introduced a new parameter $\\lambda$ that regulates the relative importance of the penalty term as compared to the MSE. This parameter together with the polynomial order is what we call hyperparameters and we need to optimize them as both are needed for the determination of our final hypothesis $g$.\nThe graph below show the results of each search iteration on the $\\lambda$ hyperparameter.\nLets reflect on the MSE and how model complexity gives raise to various generalization errors.\n$$MSE = \\mathbb{E}[\\hat{y}_i - y_i)^2] = \\mathrm{Bias}(\\hat{y}_i)^2 + \\mathrm{Var}(\\hat{y}_i)$$\nwhich means that the MSE captures both bias and variance of the estimated target variables and as shown in the plots above, increasing model capacity can really increase the variance of $\\hat{y}$. We have seen that as the $\\mathbf{w}$ is trying to exactly fit, or memorize, the data, it minimizes the bias (in fact for model complexity M=9 the bias is 0) but it also exhibits significant variability that is itself translated to $\\hat{y}$. Although the definition of model capacity is far more rigorous, we will broadly associate complexity with capacity and borrow the figure below from Ian Goodfellow\u0026rsquo;s book to demosntrate the tradeoff between bias and variance. What we have done with regularization is to find the $\\lambda$ that minimized generalization error aka. find the optimal model capacity.\nAs capacity increases (x-axis), bias (dotted) tends to decrease and variance(dashed) tends to increase, yielding another U-shaped curve for generalization error (bold curve). If we vary capacity along one axis, there is an optimal capacity, with underﬁtting when the capacity is below this optimum and overﬁtting when it is above.\n"});index.add({'id':7,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/','title':"The Way of Working in AI",'content':" A Positive Reinforcement Loop What are the disciplines that need to cross fertilize to get a system that possesses intelligence? Lets start with a diagram that show not only the disciplines but also a way of working for the many specialists involved.\nThe diagram above highlights three fundamental axes that can deliver a system-based approach to AI. The Z axis is the scientific axis where many disciplines such as psychology, neuroscience, mathematics and others make progress on. The X axis involves the ML/AI communities that borrow ideas from their colleagues in sciences and convert those theories and pragmatic findings into abstractions (models and methods). The model of the neuron, the perceptron, appeared in psychology journals many decades ago and despite its simplicity it is still the unit via which much more complicated neural networks are constructed from. Todays\u0026rsquo; models of Long-Term Short-Term Memory (LSTM), Replay Memory and many others not shown in the diagram (as its currently in draft form) are abstractions (models) of discoveries that scientists produced after tens of years of research. To use however these methods and models effectively, major hardware and software components need to be developed also known as computing and frameworks - these live in the Y axis. They are very important for the development of AI field that is known to be heavily experimental, requiring especially at the perceptive frontend significant computational power and automation.\nAt a very high level, progress in AI is made via the counterclockwise iteration Z -\u0026gt; X -\u0026gt; Y -\u0026gt; Z. AI engineers look at the neuroscience/psychology axis, map discoveries to points in the methods / models axis, and finally develop these methods in hardware architectures and software frameworks. But what can explain the Y -\u0026gt; Z flow? Frameworks in turn help the neuroscientists and psychologists as they can provide generative models of their own discoveries or help them simulate conditions that are not possible using their native tools.\nThis counter-clockwise multidisciplinary iteration acts as a positive feedback loop accelerating the progress in the AI space.\nIn this course we will be focusing on the methods/models and frameworks axis and understand what these models can offer us and how we can apply them in synthesizing an AI system at least for a domain of interest.\nA typical AI stack today As we have seen from the syllabus, this course approaches AI from an applied perspective - this means teaching concepts but at the same time looking how these concepts are applied in the industry to solve real world problems. In this respect here we take an architecture driven AI, presenting the components of AI in a form of a software stack but also how the components are mechanized in what we call ML Pipelines to provide the ML utility to applications. For a complete overview of real world ML pipelines used today go through the TFX paper in its entirety.\nAI Stack circa 2019\nLandscape of the AI ecosystem Due to the complexity and common interest to addresses industrial players are partnering to define and implement the necessary components for the complete automation of AI pipelines. This work is going in within the Linux Foundation AI (sub)Foundationamongst many other open source communities.\n   The four pipelines of an end-to-end ML platform Example of end to end pipeline - serial arrangement\nExample of Data Pipeline\nExample of Model Training Pipeline\nExample of Model Evaluation and Validation Pipeline\nExample of Serving Pipeline\nRoles in AI product development Who data scientists need to interact with, during the development of AI systems?\n \u0026ldquo;Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\u0026rdquo; http://www.melconway.com/Home/Conways_Law.html\n \u0026ldquo;We do research differently here at Google. Research Scientists aren\u0026rsquo;t cloistered in the lab, but instead they work closely with Software Engineers to discover, invent, and build at the largest scale.\u0026rdquo;\nContrast this to an organizational structure that isolates researchers from product development. What about Alphabet\u0026rsquo;s X https://x.company/ ?\n"});index.add({'id':8,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/','title':"Week 2b - Regression",'content':""});index.add({'id':9,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/','title':"Week 2c - Linear Classification",'content':" Linear Classification This section captures the main principles of linear classification which is a fundamental ability for the perception system of an AI agent.\nClassification Principle There are three broad classes of methods for determining the parameters $\\mathbf{w}$ of a linear classifier:\n Discriminative Models, which form a discriminant function that maps directly test data $\\mathbf{x}$ to classes $\\mathcal{C}_k$. In this case, probabilities play no role. Examples include the Perceptron and Support Vector Machines (SVMs).\n Probabilistic Discrimitative Models, First solve the inference problem of determining the posterior class probabilities $p(\\mathcal{C}_k|\\mathbf{x})$ and then subsequently assign each new $\\mathbf{x}$ to one of the classes. Approaches that model the posterior probabilities directly are called discriminative models. Examples of discriminative training of linear classifiers include:\n Logistic regression—maximum likelihood estimation of $\\mathbf{w}$ assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.  Probabilistic Generative Models, which infer the posterior $p(\\mathcal{C}_k|\\mathbf{x})$ using Bayessian approach and we therefore generate the class-conditional density $p(\\mathbf{x}|\\mathcal{C}_k)$ and the prior $p(\\mathcal{C}_k)$. Examples of such algorithms include:\n Linear Discriminant Analysis (or Fisher\u0026rsquo;s linear discriminant) (LDA)—assumes Gaussian conditional density models Naive Bayes classifier with multinomial or multivariate Bernoulli event models.   Discriminative Models Synthetic Dataset with multiple discriminant functions\n The learning algorithm is asked to assign the input vector $\\mathbf{x} \\in \\mathbb{R}^n$ to one of the $k$ classes. The learning algorithm will need to produce a function $\\mathbf{y}=g(\\mathbf{x})$ also called discriminant function.\n In binary classification, the target variable (label) $y$ belongs to either of the two classes ${\\mathcal{C}_1, \\mathcal{C}_2}$ - for multi-class it is taking a value out of a finite set of classes. Contrast this to regression where $y$ is taking any value out of a infinite possible set of values ($y \\in \\mathbb{R}$). For example, we can choose convenient labels $y=0$ for class $\\mathcal{C}_1$, and $y=1$ for class $\\mathcal{C}_2$.\n The Bayesian setting is now very handy, $ p(\\mathcal{C}_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{p(\\mathbf{x})}$ where $p(\\mathcal{C}_k)$ is the prior probability for the corresponding class. The equation above is similar to what we have seen in regression, and can be used to update the posterior probability $p(\\mathcal{C}_k|\\mathbf{x})$ given the likelihood function, prior and evidence. As an example, we look at the CT-scan of a patient and obtain the posterior probability based on the formula above and can now diagnose the patient as a cancer free if $p(\\mathcal{C}_0|\\mathbf{x}) \u0026gt; p(\\mathcal{C}_1|\\mathbf{x})$ where $\\mathcal{C}_0$ is the cancer free class.\n  Joint probabilities involved in binary classification. By sweeping the discrimination function (which in this example is the threshold $\\hat{x}$) left or right we are adjusting the areas shown as red blue and green that are the main determinants of classifier performance.\nReceiver Operating Curve and Classification Metrics Obviously the criticality of estimating the right posterior for the treatment of the patient is very high - we have therefore developed metrics that gauge such detection. It is extremely important to understand the Receiver Operating Curve that gives raise to many classification quality metrics. Where the name comes from? The top of this page is decorated by a photo of one of the hundreds of RADAR towers installed in England just before the WWII. The job of the RADAR was to detect incoming Nazi air bombings. RADAR was one of the key technologies that won the war (the other was cryptography).\nLet us consider a two-class prediction problem (binary classification), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p.\nTo get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:\nJoint probabilities and ROC Curve (Wikipedia)\nConfusion Matrix (Wikipedia)\nFor an instructional example of determining the confusion matrix of classification models using scikit-learn see the Iris case study.\n"});index.add({'id':10,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/ML-Bayesian-estimation/','title':"Bayesian Linear Regression",'content':" Bayesian Linear Regression Thomas Bayes (1701-1761)\nThe treatment in this section will proceed as follows. First we will discuss the Ordinary Least Squares (or the Maximum Likelihood) estimate of the model parameters. We will then introduce the Bayesian approach to estimating the model parameters and look at the so called predictive distribution for the simple data set we started our journey on.\nOLS as Maximum Likelihood Estimation These notes summarize the Maximum Likelihood approach and are provided here as they are more expansive compared to textbooks and at the same time do make the connection to the Linear Algebra background you should have.\nGeometrical Interpretation of Ordinary Least Squares (OLS)\nBayesian Linear Regression In the probability section we have covered an instructive example of coin tossing and saw how the posterior distribution is updated with the draws. The Bayesian setting is obviously generic enough and here we provide just the intuition (for the interested reader the mathematical treatment is in Bishop Section 3.3) behind its application to linear regression.\nThe Bayesian update of the posterior can be intuitively understood using a graphical example of a linear model of the form: $$g(x,\\mathbf{w})= w_0 + w_1 x$$ (our hypothesis). The reason why we pick this example is illustrative as the model has just two parameters and is amendable to visualization.\nInstructive example of Bayesian learning as data points are streamed in. Notice the dramatic improvement in the posterior the moment the 2nd data point arrives. Why is that?\nBack to our model for the sinusoidal dataset, and assuming Gaussian basis functions, we can look at the predictive distribution as the number of data points are coming in.\n$$p(\\mathbf{w}|y) = \\frac{p(y|\\mathbf{w}) p(\\mathbf{w})}{\\int p(y|\\mathbf{w}) p(\\mathbf{w}) d\\mathbf{w}}$$\n posterior $\\propto$ likelihood $\\times$ prior The likelihood function is central to both Bayesian and frequentist paradigms. In the frequentist paradigm the $\\mathbf{w}$ is treated as a known quantity with an estimate $\\hat{\\mathbf{w}}$ that has a mean and variance resulting from the distribution of $y$.\n In the Bayesian setting, we are integrating over the distribution of $\\mathbf{w}$ given the data i.e. we are not making a point estimate of $\\mathbf{w}$ but we marginalize out $\\mathbf{w}$.  "});index.add({'id':11,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/','title':"Intelligent Agents and Representations",'content':" Agent-Environment Interface  An agent is a computer system that is situated in some environment, and that is capable of autonomous action in this environment in order to meet its design objectives.\n In general sensor data are converted via the agent function (that is implemented via a program) into actions as shown below.\nGeneral Agent-Environment Interface\nThe two most important agent architectures that we will deal with in this course are the utility and learning-based agents architectures. To start with we recognize that most of the problems we will face as agent designers are for agents operating in environments that are:\n Partially Observed (PO). This means that we cant see all the variables that constitute the state and we need to maintain an internal belief of the state variables that we cant perceive. Stochastic. This means that the environment state is affected by random events and can not be determined by the previous state and the actions of the agent (which in this case we are talking about deterministic environments). Such probabilistic characterization of the environment state is the norm in many settings such as as robotics, elf-driving cars etc.\n Sequential As compared to episodic, in sequential environments actions now can have long term effects into the future. Dynamic In this setting, the environment state changes all the time, even while the agent is taking the action based on the sequence of percepts up to this point in time. In most settings the environments we deal will not be static. Continuous When the variables that constitute the environment state are defined in continuous domains. Time is usually considered a special variable and we may have environments where the time variable is discrete while other variables are continuous. Known This refers to the knowledge of the agent rather than the environment. In most instances we are dealing with environments where there is a set of known rules that govern the state transition. In driving for example, we know what steering does.  Architectures Rational Agent Architecture In the rational agent architecture we meet three key concepts:\n The need for the agent to keep internally the environment state (in probabilistic terms a belief). This is needed due to the the partially observed environment the agent is interfacing with. The presence of a world model that helps the agent to update its belief. The presence of a utility function that the agent can use to produce the value (happiness) when its action transitions the environment to a new state. Obvious the agent will try to optimize its actions in what we earlier described stochastic environments and therefore it will try to maximize the value (hapiness) on average (strictly in expectation) where the average is taken across the distribution of all possible states across time.  Learning Agent Architecture The learning agent architecture builds on top of the rational agent (the performance element in the figure below), additional functions that:\n Embeds a learner that learns the various models needed by the rational agent as well as allowing the rational agent to operate on unknown environments. In this respect it learns the world model, some elements of the utility function itself or the desirability of each actions it takes. To enable learning, the rational agent sends training data to the learner. Introduces a critic that transmits a positive or negative reward to the learner based on its own view of how the agent is doing. The learner can modify these models to make the rational agent perform better in the future. Introduces the problem generator that can change the problem statement of the rational agent. Obviously the expected utility objective will not change but the utility function itself may in fact change to lead the agent to perform more exploration (increase its risk) in its environment.  We will see in Deep Reinforcement Learning that this architecture is able to accommodate such end to end learning approach. In that setting the critic is part of the environment - see Solving sparse-reward tasks with Curiosity for an example where the critic is inside the agent generating intrinsic rewards.\n"});index.add({'id':12,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/knn/','title':"k-Nearest Neighbors (kNN) Classification",'content':" k-Nearest Neighbors (kNN) Classification kNN belongs to the class of algorithms that were extensively treated in pattern recognition literature many years ago. It is still extensively being used today especially in settings that require very fast decision/classifications. The general block diagram governing such systems is shown below.\nOne example that was originally treated by Patrick Winston (MIT) is the conveyor belt classification use case.\nLets assume that in a factory a high speed conveyor belt is carrying hundreds of widgets per minute and a system of cameras is used to classify them and instruct actuators that place them into bins. Obviously the system must satisfy strict throughput requirements. Lets assume for purely instructive purposes that the assignment of each widget is based on two features that shown below.\nExample decision boundaries between labels in the feature space.Stars are the distinct widgets (labels)\nEach pair of prototype widgets can be considered as defining a linear decision boundary - the line perpendicular to the line that connects them. So if the widget was screws, figure below shows two features categories, head-type and length.\nIn a hypothetical case of manufacturing screws with two lengths $l_1, l_2$ and two head types (flat and round) we will have four possible labels (each associated wiht the combination ${(l_1, F), (l_2, F), (l_1,R), (l_2, R)}$ as shown in the picture above and we can draw corresponding decision boundaries. Why the decision boundaries are like this (also called perpendicular bisectors) ? The decision rule is a Euclidean distance metric - any point in that line has the same distance to either of the two labels involved.\nAs screws go through the conveyor depth, manufacturing defects cause each screw to appear almost anywhere in the feature space. One intuitive and straightforward approach is to assign the label associated with the area enclosed by the decision boundaries (and the axes) and classify the screw as the label of the corresponding prototype widget. This is in essence the principle behind the k nearest points (or neighbors) algorithm.\nIn a sightly more formal setting, and to be able to address far more complex decision boundaries than the above, we are given data points in a training set $D = {(x_i,y_i)}, i={1, \u0026hellip;, m}$ and we are asked to classify points that are in the test set. The only variable of the kNN algorithm is the number $k$ which is the number of nearest neighbors that we consider in the classification decision. An example for two classes is shown in the figures below for two cases of $k$. The plot corresponds to the case we have two features like before $x_1, x_2$.\nk=3\nk=1\nThe algorithm effectively positions a sphere on the data point we want to classify whose radius is large as the it needs to be to enclose $k$ closest points irrespectively of their class. Obviously for the dimensions of the examples above, the sphere is a circle. As expected, we see that $k$ controls the degree of smoothing, so that small $k$ produces many small regions of each class, whereas large $k$ leads to fewer larger regions. In essence the algorithm for $k\u0026gt;1$, considers a majority vote between the $k$ closest points to the point we need to classify with ties broken at random.\nOne of the limitations of the knn algorithm is the requirement that the dataset $D$ is stored in memory and that the algorithm itself is dependent on efficient search algorithms that allow it to go over the data and find the nearest neighbors. There are solutions to both of these problems though and if implemented properly we get to be able to train for fairly complex decision boundaries that generalize well without the complexity associated with learning parametric models.\nNOTE: Although not part of this class, think about how to generalize knn. What happens when the votes are not binary ? How the algorithm can be adapted to online learning settings?\n"});index.add({'id':13,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/perceptron/','title':"The Perceptron",'content':" This section captures the main principles of the perceptron algorithm which is the essential building block for neural networks.\nArchitecture of a single neuron The perceptron algorithm invented 60 years ago by Frank Rosenblatt in Cornell Aeronautical Laboratory. Neural networks are constructed from neurons - each neuron is a perceptron with a specific activation function. A single neuron is itself capable of learning \u0026ndash; indeed,various standard statistical methods can be viewed in terms of single neurons \u0026ndash; so this model will serve as a first and simple example of a supervised neural network.\nA single neuron has a number $n$ of inputs $x_i$ (note the figure is not compatible with this notation) and one output which we will here call $\\hat{y}$. Associated with each input is a weight $w_i$ ($i = 1 ,\\ldots, n$). The additional parameter $w_0$ of the neuron called a bias which we may view as being the weight associated with an input $x_0$ that is permanently set to 1. The single neuron is a feedforward device \u0026ndash; the connections are directed from the inputs to the output of the neuron. Feedforward neural networks are called Multi-Layer Perceptrons (MLPs).\nWhat does the perceptron compute? First, in response to the imposed inputs $\\mathbf{x}$, we compute the activation of the neuron,\n$$a = \\sum_{i=0}^n w_i x_i = \\mathbf{w}^T \\mathbf{x}$$\nSecond, the output (also called the activity of the neuron) is produced by passing the activation through a non linear activation function $\\hat{y} = g(a)$. The activation function of the perceptron is the step function - we will cover more of such functions in the treatment of neural networks.\n$$g(a) = \\begin{cases}1 \u0026amp; \\text{if }\\ a \\ge 0, \\\\ -1 \u0026amp; \\text{otherwise}\\end{cases}$$\nPerceptron Learning Algorithm The algorithm is derived from the application of the SGD to a suitably chosen loss function. The loss function can be easily designed if we start thinking about the class labels as belonging to the set ${+1,-1}$ (rather than the more usual ${0,1}$) and considering the value of the products $\\mathbf{w}^T x_j y_j$. If there are no classification errors for the chosen non-linear activation function above such products will result into positive numbers irrespectively of the class. For these cases we assign zero to the loss function. If there are errors however, these products will be negative and the sum of all these negative product terms we must maximize - or equivalently minimize the negative of such loss as below:\n$$L(\\mathbf{w}) = - \\sum_{j: \\hat{y_j} \\ne y} \\mathbf{w^T}x_j y_j$$\nWe will find the $\\mathbf{w}$ that minimize such loss using the familiar Stochastic Gradient Descent algorithm. Noting that the gradient of the loss function at $\\mathbf{w}$ is $x_j y_j$ we can write the SGD algorithm as follows:\nLet $t$ denote the iteration index and $r$ the learning rate.\n Initialize the weights and the threshold. Weights may be initialized to zero or to a small random value.\n For each example in our training set, perform the following steps over the input $\\mathbf{x}_j$ and desired output $y_j$:\n$$\\hat{y}_j(t) = g[\\mathbf{w}(t)^T \\mathbf{x}_j]$$\n Update the weights:\n$$w_i(t+1) = w_i(t) + r (y_j - \\hat y_j(t)) x_j$$\n  for all features $0 \\leq i \\leq n$.\nThe updated weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.\nInitial parameter vector $\\mathbf w$ shown as a black arrow together with the corresponding decision boundary (black line), in which the arrow points towards the decision region which classified as belonging to the red class. The data point circled in green is misclassified and so its feature vector is added to the current weight vector, giving the new decision boundary shown in the plot below.\nThe next misclassified point to be considered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundary shown in the plot below for which all data points are correctly classified.  NOTE: For offline learning, the second step may be repeated until the iteration error $\\frac{1}{s} \\sum_{j=1}^s |y_j - \\hat{y}_j(t)| $ is less than a user-specified error threshold $\\gamma $, or a predetermined number of iterations have been completed, where \u0026ldquo;s\u0026rdquo; is the size of the training set.\n The perceptron is a linear classifier, therefore it will never get to the state with all the input vectors classified correctly if the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane. In this case, no \u0026ldquo;approximate\u0026rdquo; solution will be gradually approached under the standard learning algorithm, but instead learning will fail completely. Even in the case of linearly separable datasets, the algorithm may exhibit significant variance while it is executing as previously correctly classified examples may \u0026ldquo;fall\u0026rdquo; into the wrong decision region by an update that considers a currently misclassified example.\nFurther, the perceptron solution will depend on the initial choices of the parameters as well as the order of the training dataset presented. Support Vector Machines avoid such pitfalls which can motivate the question why we insisted on learning the perceptron algorithm: both architecturally and the functionally the linear combination of features followed by a non-linearity is the fundamental building block of far more complicated neural networks.\nPerceptron performance For a live demo of perceptron performance see the single neuron example for separable datasets in Tensorflow Playground\n"});index.add({'id':14,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/logistic-regression/','title':"Logistic Regression",'content':" Logistic Regression Logistic regression is used in machine learning extensively - every time we need to provide probabilistic semantics to an outcome e.g. predicting the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.), whether an voter will vote for a given party, predicting the probability of failure of a given process, system or product, predicting a customer\u0026rsquo;s propensity to purchase a product or halt a subscription, predicting the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in Natural Language Processing (NLP).\nBinary case If we consider the two class problem, we can write the posterior probability as,\n$$p(\\mathcal{C}_1|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1)}{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1) + p(\\mathbf{x}|\\mathcal{C}_2) p(\\mathcal{C}_2)} = \\frac{1}{1 + \\exp(-\\alpha)} = \\sigma(\\alpha)$$\nwhere $\\alpha = \\ln \\frac{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1)}{p(\\mathbf{x}|\\mathcal{C}_2) p(\\mathcal{C}_2)}$ and $\\sigma$ is given by the logistic function we met in probability review.\nGiven the posterior distribution above we have for the specific linear activation,\n$$p(\\mathcal{C}_1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x})$$\nThis model is what statisticians call logistic regression - despite its name its a model for classification. The model has significant advantages in that it does require the estimation of far fewer parameters compared to the case where the class conditional distributions involved in the posterior were parametric. For example if we had Gaussian class conditionals we would had to estimate (using Maximum Likelihood) their parameters $\\mathbf \\mu$ and $\\mathbf \\Sigma$ that grow quadratically to the number of features $n$. With logistic regression we only have an evident linear relationship between parameters and features.\nThe figure below shows the corresponding posterior distribution $p(\\mathcal{C}_1|\\mathbf{x})$\nThe class-conditional densities for two classes, denoted red and blue. Here the class-conditional densities $p(\\mathbf{x}|\\mathcal{C}_1)$ and $p(\\mathbf{x}|\\mathcal{C}_2)$ are Gaussian\nThe corresponding posterior probability for the red class, which is given by a logistic sigmoid of a linear function of $\\mathbf{x}$.\nAs we said, with logistic regression we skip the assumption about the class-conditional densities as they add parameters to our problem that grow quadratic to the number of dimensions and we attempt to find the $n$ parameters of the model directly (the number of features) and sure enough we will use ML to do so.\nBy repeating the classical steps in ML methodology i.e. writing down the expression of the likelihood function (this will now be a product of binomials), we can write down the negative log likelihood function as,\n$$L(\\mathbf{w}) = - \\ln p(\\mathbf{y},\\mathbf{w}) = - \\sum_{i=1}^m {y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) }$$\nwhich is called cross entropy error function - probably the most widely used error function in classification due to its advantages such as its probabilistic and information theoretic roots as well as its shape shown in the figure below. Sometimes it is also called log-loss.\nMinimizing the error function with respect to $\\mathbf{w}$ by taking its gradient\n$$\\nabla L = \\sum_{i=1}^m (\\hat{y}_i - y_i) x_i$$\nthat defines the batch gradient decent algorithm. We can then readily convert this algorithm to SGD by considering mini-batch updates.\n"});index.add({'id':15,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/kernels/','title':"Kernels and the Kernel Trick",'content':" Kernels and the Kernel Trick Introduction In linear regression we have seen a simple dataset from an unknown non-linear target function. We then proceeded and chose a hypothesis from the polynomial family that mapped each input example $x$ into a function $g(x, \\mathbf w) = \\mathbf w^T \\phi(\\mathbf{x})$, found the optimal $\\mathbf w$ by maximizing the likelihood (ML) function using the MSE as the loss function. The moment we have $\\mathbf w$ we can use it to do new predictions. In this regression problem we have used a transformation from the raw data $\\mathbf x$ to the feature $\\phi(\\mathbf x)$ and more specifically we have used basis functions $\\phi_i(\\mathbf x)$ from the set of polynomials shown below.\nPolynomial basis functions\nWe could have chosen other sets as well:\nGaussian basis functions\nIn classification, we have seen lastly logistic regression which despite the non-linear (probabilistic) interpretation of its output it is still a linear classifier as it presents a linear decision boundary - linear in the feature space $(\\mathbf{w}^T\\mathbf{x})$. Which posses the question: what we do when we have a problem that has a non-linear decision boundary? The answer is via the concept of kernels that we describe next.\nMotivation Lets revisit the perceptron algorithm where we started from $\\mathbf{w}=\\mathbf{0}$ (or a random vector) and we visited each and every example, changing the direction of the decision boundary when we met a miss-classified example. In the plots of that lecture for example we have seen that after two misclassified examples (the green circled examples indexed 9 and 6) the algorithm converged to a final $\\mathbf{w}$. Note that the indices9 and 6 selected here are arbitrary. These two steps can be written as:\n$\\mathbf{w}^{(1)} = \\mathbf{w}^{(0)} + \\mathbf{x}^{(9)}$\n$\\mathbf{w}^{(2)} = \\mathbf{w}^{(1)} + \\mathbf{x}^{(6)}$\nWe can compress this into $\\mathbf{w}^{(2)} = \\mathbf{w}^{(0)} + {\\mathbf{x}^{(9)} + \\mathbf{x}^{(6)}}$ and in addition this example indicates that we just need to keep the so called sparse representation where we represent the final $\\mathbf{w}$ with the array ${[9, +1], [6, +1]}$, where \u0026ldquo;+1\u0026rdquo; happened to be the class of the two examples involved in the adjustment of the weight. Saying it a bit differently, the algorithm results in the set of points that are miss-classified throughput the journey of visiting each and every of our examples.\nFor non-linear decision boundaries, the straightforward approach we can take is to throw into the problem more features. This is visualized as shown in this animation for one of the datasets we have seen in Tensorflow Playground where we admitted that neither the perceptron or logistic regression cannot separate the two classes.\n In the visualization above we added the feature $\\mathbf{x}^2$ and therefore expanded the feature space $\\phi$ to ${\\mathbf{x}, \\mathbf{x}^2}$ and surely in the expanded space there is a linear decision boundary (a plane) that can separate the previously the linearly inseparable classes. Moreover this plane can be found by various linear classification methods and for the perceptron we can then using the same reasoning as above we can end up with a sparse representation for the weight that is normal to the plane and can be written as:\n${\\phi(\\mathbf{x}^{(k)}), \u0026hellip;, \\phi(\\mathbf{x}^{(d)})}$\nthat we can also programmatically represent as a linked list as before.\nThe problem with adding more features is that in high dimensional spaces (e.g. medical imaging) we may end up with problems of very high number of dimensions as we start from a large number of dimensions to begin with. For example, in imaging we are dealing with at least as many dimensions as the number of pixels in the image and sometimes many more. So although with the sparse representation we have avoiding writing down $\\mathbf{w}$ explicitly in the high dimensional space as we can represented as a linked list of indices and labels of the examples of interest, we still need to make operations on those high dimensional examples and therefore the problem now becomes how to avoid the explicit specification of $\\phi(\\mathbf{x})$. This is what we do with the kernel trick described next.\n Where in the perceptron algorithm we need to make operations in high dimensional (expanded) space? Hint: How do we determine if the example is a misclassified during training? How we make inference predictions?\n Kernel trick Now that we have motivated the need for the trick lets see what the trick is after all.\nThe dot product $\\mathbf{w}^T\\phi(\\mathbf{x})$ will be needed throughout training as well as inference.Given the sparse representation obtained earlier we realize that we will be dealing with lots of dot products of the form $\\phi(\\mathbf{x}^{(k)})^T \\phi(\\mathbf{x})$.\nThe kernel trick at a high level is this: instead of doing these dot products in the expanded high dimensional space, we will do them in the original lower dimensional space saving significant computations (even ensuring feasibility in some cases). The trick, no matter how unbelievable it may look at a first glance, is becoming a reality by choosing suitable kernels\n$$k(\\mathbf x_i, \\mathbf x_j) = \\phi(\\mathbf x_i)^T \\phi(\\mathbf x_j)$$\n$$= \\sum_{i=1}^M \\phi_i(\\mathbf x_i)^T \\phi_i(\\mathbf x_j)$$\nwhere $\\phi_i(\\mathbf x)$ are basis functions. Lets look at an example to see in practice the properties of such kernel functions.\nLets look at the kernel function we used in the previous example where we expanded the space to include a quadratic term but now we include all quadratic terms that correspond to basis function from the polynomial set. This means $\\phi(\\mathbf x) = (x_1^2, x_2^2, x_1 x_2)$. Lets us now form the dot product at the feature space and see if this choice of basis function has the property that this dot product can be written as a dot product of terms in the original space.\n$$k(\\mathbf x, \\mathbf z) = \\phi(\\mathbf x)^T \\phi(\\mathbf z)$$ $$=(x_1^2, \\sqrt{2} x_1 x_2, x_2^2) (z_1^2, \\sqrt{2} z_1 z_2, z_2^2)^T = x_1^2z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2$$ $$= (x_1 z_1 + z_2 z_2)^2 = (\\mathbf x^T \\mathbf z)^2$$\nSo this kernel has the desired property that allows us to execute the perceptron algorithm (or other classification algorithms as we will see shortly) at the expanded feature space, without requiring for us to do the processing at the feature space but at the original data space. We call such kernels, valid. Apart from polynomial basis functions we have just seen, the most trivial kernel is as you can imagine $k(\\mathbf x, \\mathbf z) = \\mathbf x^T \\mathbf z$ which by definition is not doing any space expansion and therefore degenerates to a linear decision boundary. This kernel can be used though to generate other kernels. One of them is the so called Gaussian kernel that is worth learning about:\n$k(\\mathbf x, \\mathbf z) = \\exp(-||\\mathbf x - \\mathbf z||^2 / {2\\sigma^2})$\n We can see kernels functions from another perspective as well. Kernel functions that possess the properties required by the kernel trick, i.e. dot products in the high dimensional space that degenerate to dot product in the original data space are similarity functions because dot products are just that: they express how similar (direction wise) a vector is to another.\n "});index.add({'id':16,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/','title':"Background - Math for ML",'content':""});index.add({'id':17,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/','title':"Background - ML Frameworks",'content':" The Zillow App The Zillow app is based on the end to end machine learning example in Chapter 2 of Geron\u0026rsquo;s book. We can go through this end to end example during a recitation.\nAlthough the ML project checklist provided in Appendix B of Garon\u0026rsquo;s book is extensive (we will go through this list in the lecture as we go through your first ML application) for now focus on the eight steps as shown below.\nSteps in workflow (from here)\n As discussed the data pipeline is responsible for providing the training datasets if the aim is to train (or retrain) a model. For the purposes of this lecture we assume that we deal with small data aka. data fit in the memory of today\u0026rsquo;s typical workstations/laptops (\u0026lt; 16 GB). Therefore you will be given a URL from where compressed data files can be downloaded. For structured data, these files when uncompressed will be typically CSV. For unstructured they will be in various formats depending on the use case. In most instances, ML frameworks that implement training will require certain transformations to optimize the format for the framework at hand (see TFrecords in tensorflow).\n Appendix B of Garon\u0026rsquo;s book goes into more detail on the steps suggested to be followed in an end to end ML project.\n Key Questions  Is the dataset appropriate for training?   Any unexpected ranges, any range heterogeneity, any clipping? Do we face long-tails? What options do we have to glean the data?\n  What will happen if we remove the following line from the split_train_set function?\nshuffled_indices = np.random.permutation(len(data))  "});index.add({'id':18,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/face-recognition/','title':"Face Recognition - SVM Case Study",'content':" "});index.add({'id':19,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/clustering/','title':"K-means Clustering",'content':" K-means Clustering Up to now in this lecture series we have seen parametric models using in regression and classification. The complexity of such models was not very high despite the heany sometimes math. Most models we have seen took the form $\\mathbf w^T \\phi(\\mathbf x)$. Now we switch to a very frequently met case where we dont have any labels $y$. Most of the treatment here is from Bishop section 9.1 - the equivalent section in Tan\u0026rsquo;s book is section 7.2.\nSuppose we have a data set $D={ \\mathbf x_1,\u0026hellip;,\\mathbf x_m}$ consisting of $m$ observations of a random n-dimensional Euclidean variable $\\mathbf x$. Our goal is to partition the data set into some number $K$ of clusters, where we shall suppose for the moment that the value of $K$ is given. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. This is shown in the figure below.\nWe can formalize this notion by first introducing a set of n-dimensional vectors $\\mathbf \\mu_k$ where $k ={1,\u0026hellip;,K}$, in which $\\mathbf \\mu_k$ is a prototype associated with the kth cluster. We can think of the $\\mathbf \\mu_k$ as representing the centres of the clusters. Our goal is then to find an assignment of data points to clusters, as well as a set of prototypes. To do so we need to define as before a loss function.The loss function is selected to be the sum of the squares of the distances of each data point to its closest vector $\\mathbf \\mu_k$ is a minimum.\n$L = \\sum{i=1}^m \\sum{k=1}^K r_{ik} ||\\mathbf x_i - \\mathbf \\mu_k||^2$\nEverything is as expected in the loss function above expept from the variable $r_{ik}$ which is a binary indicator variable that will simply add the squared Euclidean distance between the point $\\mathbf x_i$ and the candidate cluster prototype if the point is assigned to that cluster or add 0.0 to the loss if it doesnt. As an example for two cluster prototypes (as evidently will be the number of clusters in the above example) $r_k \\in {(0 1), (1 0)}$. This is also called one-hot encoding of the corresponding category where category \u0026ldquo;cluster 1\u0026rdquo; is coded as $(0 1)$ and category \u0026ldquo;cluster 2\u0026rdquo; is coded as $(1 0)$.\nThe K-means algorithm is iterative and each iteration includes two steps after an initialization of the $\\mathbf \\mu_k$ to random locations.\nStep 1 We go over each data point $\\mathbf x_i$ and we assign it to the closest custer center for this iteration.\nIteration 1, Step-1: Assignment of data points to cluster centers\nMathematically this means determining:\n$$r_{ik} = \\begin{cases}1, \u0026amp; \\text{if } k = \\arg \\min_j ||\\mathbf x_i - \\mathbf \\mu_j||^2\\ 0, \u0026amp; \\text{otherwise. } \\end{cases}$$\nStep 2 In the second step we move each cluster center to the average of the data points assigned to that cluster. Mathematically this is:\n$$ \\mathbf \\mu_k = \\frac{\\sumi r{ik} \\mathbf x_i}{\\sum_i r_ik}$$\nIteration 1, Step-2: Moving the cluster centers to the mean of the data points assigned to each cluster\nThe following figures show subsequent steps until its becoming clear that the algorithm will convergence.\nIteration 2, Step-1: Assignment of data points to cluster centers\nIteration 2, Step-2: Moving the cluster centers to the mean of the data points assigned to each cluster\nThe algorithm will converge as shown below:\nBlue circles (Step 1), interleaved with red circles (step 2) towardds k-means alg convergence\n"});index.add({'id':20,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/','title':"Support Vector Machines",'content':" Support Vector Machines In the development of the concept of kernels, we mentioned that these can be used to derive non-linear decision boundaries. What we haven\u0026rsquo;t addressed, is how good these can be - for example in separable datasets there can be many (or infinite) number of boundaries that separate the two classes but we need a metric to gauge the quality of separation. This metric is called the margin and will be geometrically explained next.\nClassification Margin Intuitively margin is the distance between the classification boundary and the closest data point(s) of the two classes, as shown below:\nClassification Margin\nAssuming a linear classifier of $g(\\mathbf x) = \\mathbf w^T \\phi(\\mathbf x) + b$ and a separable dataset for the moment, the maximization of the margin leads to a decision boundary, $y(\\mathbf x)=0$ that depends only on a subset of data that are calling support vectors as shown below for the specific dataset.\nSupport Vectors\nJust like in the perceptron case, since\n$$y( \\mathbf x_i ) = \\begin{cases}\u0026gt;0 \u0026amp; \\text{if} \\ y_i = +1, \\\\ \u0026lt;0 \u0026amp; \\text{if} \\ y_i = -1 \\end{cases}$$\nfor all training data we have,\n$$y_i(\\mathbf w^T x_i + b) \u0026gt; 0$$\nSo we are after a $\\mathbf w \\in \\mathbb{R}^n$ that satisfies this constraint. Before we proceed, let us define geometrically what the margin looks like. Let us define two parallel lines on either side of the decision boundary as shown in the above figure,\n$\\mathbf w^T \\phi(\\mathbf x) + b = 1$\n$\\mathbf w^T \\phi(\\mathbf x) + b = -1$\nLet us assume now that there is a data point that is very close to the decision boundary and just marginally satisfied the inequality $y_i(\\mathbf w^T x_i + b) \u0026gt; 0$ for example $y_i(\\mathbf w^T x_k + b) = 0.2$. If we scale both sides we can reach an equivalent condition that is appealing mathematically $y_i(\\mathbf w^T x_i + b) \u0026gt; 1$ - equivalent as it will result in the same $\\mathbf w$. In addition to this scaling, it is very useful to normalize $\\mathbf w$ as\n$\\hat \\mathbf w = \\mathbf w / ||\\mathbf w||$\nFor a point $\\mathbf z$ that is in the margin defining line $\\mathbf w^T \\phi(\\mathbf x) + b = 1$ we can calculate given a margin distance $\\gamma$ the point it projects to the decision boundary line which is defined by the normalized normal vector $\\mathbf w$ as, $\\mathbf z - \\gamma \\hat \\mathbf w$. Then we can write two equations that satisfy these two points:\n$\\mathbf w^T \\phi(\\mathbf z) + b = 1$\n$\\mathbf w^T \\phi(\\mathbf z - \\gamma \\hat \\mathbf w) + b = 1$\ngiven that we are dealing with linear decision boundary in this specific problem, we can eliminate $\\phi$ as there is no transformation involved and subtract the two equations. Solving for the margin $\\gamma$ we obtain,\n$\\gamma = \\frac{1}{||w||}$\nOptimization problem statement We are now ready to write the optimization problem that will maximize $\\gamma$ or minimize $||\\mathbf w||$ or equivalently minimize a monotonic function of $||\\mathbf w||$.\n$\\min \\frac{1}{2}||w||^2$\n$\\ \\text{s.t.} \\ y_i\\mathbf w^T x_i + b \u0026gt; 1$\nThis is a convex optimization problem (we have linear constraints and convex objective function) and therefore there are efficient ways to solve it. In practice, ML frameworks have built in solvers that can provide the $\\mathbf w$ given a dataset. Understanding the solution is instructive but it requires background in convex optimization theory and the concept of Langrange multipliers. For this reason, the math in the \u0026ldquo;Under the hood\u0026rdquo; section of chapter 5 of Geron\u0026rsquo;s book will not be in the midterm or final exams but the intuition behind maximizing the margin is in the scope of the final exam.\nTo provide an intuition of how the kernels and the SVM classifiers are connected, it suffices to write down the dual form of the optimization problem and surely enough in this form the solution involves dot products in the data space. In general, any SVM problem can be kernelized if we expand the data space using a suitable kernel which means that SVM is able via the kernel trick to find efficiently max margin decision boundaries of any shape.\nFor non separable datasets the optimization problem statement can be written using slack variables $\\xi_i$ that relax the constraints and therefore result into $\\mathbf w$ and $b$ that tolerate some classification errors for good generalization ability.\n$\\min \\frac{1}{2}||w||^2 + C \\sum_i \\xi_i$\n$\\ \\text{s.t.} \\ y_i\\mathbf w^T x_i + b \\ge 1 - \\xi_i$\n$ \\ \\ \\ \\ \\ \\ \\ \\xi_i \\ge 0$\nThe slack variable defining how much on the wrong side the 𝑖th training example is. If $\\xi =0$, the point was classified correctly and by enough of a margin; if it\u0026rsquo;s between 0 and 1, the point was classified correctly but by less of a margin than the SVM wanted; if it\u0026rsquo;s more than 1, the point was classified incorrectly. A geometrical view of the above is useful.\nOnce again the ML frameworks that you will work with, provide in their documentation explicit mention about variables of the optimization problem statement above (such as $C$). However as $C$ must be explicitly set by the data scientist, or optimized using hyperparameter optimization techniques, it is instructive to comment that $C$ is equivalent to the $1/\\lambda$ regularization parameter we met in the very beginning of these lectures where we penalized the weights of linear regression to avoid overfitting. $C$ as you can see from the optimization problem objective function if set too high, even a slightest ammount of slack will greatly penalize (add) to the objective and therefore will drive the decision boundary towards \u0026ldquo;hard\u0026rdquo; SVM decisions. This is not good news as the SVM must in many practical cases avoid over-sensitivity to outliers - too large $C$ can lead to driving the decision boundary towards outliers significantly increasing the test (generalization) error. If $C$ is set to $C=0$ then any ammount of slack will be tolerated and then the decision boundary may result into too many misclassifications.\n"});index.add({'id':21,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/mnist/','title':"MNIST Classification - SVM Case Study",'content':" MNIST Classification - SVM Case Study  "});index.add({'id':22,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/iris/','title':"Iris Classification - SVM Case Study",'content':" Iris Classification - SVM Case Study  "});index.add({'id':23,'href':'/cs-gy-6613-spring-2020/docs/projects/','title':"Projects",'content':"The following projects needs to be delivered by the deadlines.\n Surface Type Classification - Due 2/23/2020 11:59pm Lifelong Learning - Robotic Vision - Due 3/29/2020 11:59pm TBD - Due 5/10/2020 11:59pm  "});index.add({'id':24,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/ai-pipelines-slides/','title':"Ai Pipelines Slides",'content':""});index.add({'id':25,'href':'/cs-gy-6613-spring-2020/docs/lectures/learning-problem/learning-problem-slides/','title':"Learning Problem Slides",'content':""});index.add({'id':26,'href':'/cs-gy-6613-spring-2020/categories/','title':"Categories",'content':""});index.add({'id':27,'href':'/cs-gy-6613-spring-2020/','title':"CS-GY-6613 Artificial Intelligence - Spring 2020",'content':" Welcome to CS-GY-6613 ! Logistics Time/location: Brooklyn Campus, Mon 6.00 PM - 8.30 PM at RGSH 315.\nCommunication: We will use Slack for all communications: announcements and questions related to lectures, assignments, and projects. All registered students with NYU email addresses can click here to join - link expires after 30 days.\nInstructor Pantelis Monogioudis, Ph.D (Bell Labs) Head of Applied ML Research Group Murray Hill, NJ\nTeaching Assistants TA\u0026rsquo;s contact will be announced as soon as I receive confirmation from HR that were hired. I am targeting two TAs for this class.\nWhat is this course about This course is all about the algorithms and methods that will enable agents that exhibit forms of intelligence and autonomy.\nGrading  Final (30%) Midterm (30%) Projects (40%)  "});index.add({'id':28,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/decision-trees/','title':"Decision Trees",'content':" Decision Trees Continuing our path into non-parametric methods, the decision tree is one of the most popular ML algorithms. Its popularity stems also from yet another attribute that is becoming very important in the application of ML/AI in mission critical industries such as health: its ability to offer interpretable results and be visualized easily.\n Note: The material below is due to (a) \u0026ldquo;ML with Random Forests and Decision Trees\u0026rdquo; by Scott Hartshorn, (b) Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning by Criminisi et.a.l.\n Introduction A Decision Tree is simply a step by step process to go through to decide a category something belongs to - in the case of classification. They are non-parametric models because they dont use a predetermined set of parameters as in parametric models - rather the tree fits the data very closely and often overfits using as many parameters are required during training.\nFor example, let’s say that you had a basket of fruit in front of you, and you were trying to teach someone who had never seen these types of fruit before how to tell them apart. How could you do it? The answer is shown pictorially below.\nA decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value).\nIf your decision tree is good, you can now pick up an unknown piece of fruit and follow the flow chart to classify it. If your decision tree is bad, you can go down the wrong path and put something in the wrong category. For instance, if you didn’t know yellow apples existed when you built your decision tree, you might have assumed that all yellow fruit are either bananas or plantains.\nIn what follows, we focus on a dataset with $m=88$ and 4 labels: Apples, Oranges, Bananas, Grapefruit. Each example has multiple features: color, width and length.\n   Fruit Colors     Apples Red, Green, or Yellow   Oranges Orange   Bananas Yellow or Green   Grapefruit  Orange or Yellow    Fruit Dataset\nIf we are to draw separation lines on feature space of length ($x_1$) and width ($x_2$) without using an ML algorithm but by hand, we probably would come up with the picture below.\nDraw by hand partition\nNow, lets try to solve the same problem using an algorithm bearing in mind that many real-life data sets might have dozens or hundreds of different features. CART algorithm One of the most popular algorithms that implement decision trees is the Classification and Regression Tree (CART) algorithm.\n NOTE: scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\n At its heart, the algorithm implements a recursive binary partitioning of the input feature space. The feature space in the example above is $\\mathbf{x} = (x_1, x_2, x_3)^T$ denoting length, width and color. Given a training dataset as usual $D={(\\mathbf x_i, y_i}$ for $i={1, \\dots m}$, we need to come up with an close to optimal partitioning for the generalization error.\nWe start at a root node that corresponds to the whole feature space (no partition) and design a test that is in its simplest form a conditional statement against a feature (a comparison if you like). Depending on the binary outcome of the test (either the input examples will satisfy the condition or not) we produce the corresponding child nodes each inheriting a subset of the input population and we repeat the exercise. The recursion stops when we reach the so called leaf nodes e.g. when the remaining examples in these nodes cannot be split further. We will come back at this terminal / leaf nodes later. An example tree and corresponding partition is shown in the two figures below.\nExample tree\nPartition for the example tree\nThe test specification consists of the variables $\\theta_i$ that are are also called thresholds as well as the specific feature $x_k$ that is being selected for the test.\n NOTE: Mind you that its not the example $\\mathbf x$ that is part of the test spec - its the feature.\n Final decision tree for the fruit classification problem\nLets see the three recursions of the algorithm as shown below.\nFirst split\nSecond split\nThird split\nThis brings up the question of how we select the test spec parameters $x_k$ and $\\theta_k$ to minimize a certain metric that is dependent on the type of the problem we deal with - classification or regression.\nSelecting the feature $x_k$ to split To gauge which feature we will choose split requires a review of certain probabilistic concepts namely the concept of entropy. We can develop on top of entropy the concept of information gain that is pictorially explained using an example as shown below\nInformation gain for two possible splits\nThe input dataset in this example has uniform distribution over classes - we have exactly the same number of points in each class. If we split the data horizontally (select feature $x_1$) this produces two sets of data. Each set is associated with a lower entropy (higher information, peakier class histograms) that is defined as usual\n$$H(D) = − \\sum_{c \\in C} p( c ) \\log(p( c ))$$\nThe entropy drops after any resonable split as we exclude labels from the original set and end up with more homogenous sets of labels. The gain of information achieved by splitting the data into two parts (1 / 2 or left / right) is computed as\n$$IG = H(D) − L(x_k, \\theta_k)$$\n$$ = H(D) - \\sum_{j \\in {1,2}} \\frac{|D^j|}{|D|} H(D^j)$$\nwhere $|.|$ is the cardinality operator i.e. the number of elements in the corresponding set and $L$ is the loss function that the algorithm is searching for its minima by trying $(x_k, \\theta_k)$ pairs. Apart from entropy we can also use another measure of impurity of the labels of the corresponding set - the Gini impurity measure. You are expected to understand the Entropy measure as it is used in other algorithms that we cover in this course. In programming, if you use sklearn, you need to explicitly change the default Gini measure to entropy.\nInference Training will result into the heuristically optimal decision tree. Inference is then straightforward as the input data will trasverse the tree and find itself into a leaf node. We can also estimate the probability that an instance belongs to a particular class k or $p(c_k|\\mathbf x)$. First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node.\nApplicability When we use decision trees for regression, the entropy is replaced with the usual MSE. We also have the possibility of parametric clustering in an unsupervised learning setting as shown below where the Gaussian distribution is being used to fit the data before and after the split. The Information Gain concept is generic enough to be applied in both discrete, continuous, supervised and unsupervised problems.\n"});index.add({'id':29,'href':'/cs-gy-6613-spring-2020/docs/','title':"Docs",'content':""});index.add({'id':30,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/tensorflow-introduction/','title':"Introduction to Tensorflow",'content':"This by no means is a tutorial introduction but rather a set of slides that we can use to describe the principle of computational graphs. This can be skipped and consulted later in the course.\nWe will cover slides #1 - #28 as shown below. The slides are from CS 20: Tensorflow for Deep Learning Research and despite the title are appropriate for Tensorflow beginners. Slides beyond #28 will be selectively consulted when we go over gradients and backpropagation.\n "});index.add({'id':31,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/','title':"Linear Algebra for Machine Learning",'content':" Linear Algebra for Machine Learning The corresponding chapter of Ian Goodfellow\u0026rsquo;s Deep Learning is essentially the background you need.\n Key Points We can now summarize the points to pay attention to for ML applications. In the following we assume a data matrix $A$ with \\(m\\) rows and $n$ columns. We also assume that the matrix is such that it has $r$ independent rows, called the matrix rank.\nThe Four Fundamental Subspaces The fundamental theorem of Linear Algebra specifies the effect of the multiplication operation of the matrix and a vector ($A\\mathbf{x}$). The matrix gives raise to 4 subspaces:\n The column space of $A$, denoted by $\\mathcal{R}(A)$, with dimension $r$. The nullspace of $A$, denoted by $\\mathcal{N}(A)$, with dimension $n-r$. The row space of $A$ which is the column space of $A^T$, with dimension $r$ The left nullspace of $A$, which is the nullspace of $A^T$, denoted by $\\mathcal{N}(A^T)$, with dimension $m-r$.  The real action that the matrix perform is to transform its row space to its column space.\nThe type of matrices that are common in ML are those that the number of rows $m$ representing observations is much larger than the number of columns $n$ that represent features. We will call these matrices \u0026ldquo;tall\u0026rdquo; for obvious reasons. Let us consider one trivial but instructive example of the smallest possible \u0026ldquo;tall\u0026rdquo; matrix:\n  \\( \\begin{bmatrix} a_{11} \u0026 a_{12} \\\\ a_{21} \u0026 a_{22} \\\\ a_{31} \u0026 a_{32} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ 5 \u0026 4 \\\\ 2 \u0026 4 \\end{bmatrix} \\)  In ML we are usually concerned with the problem of learning the weights $x_1, x_2$ that will combine the features and result into the given target variables $\\mathbf{b}$. The notation here is different and we have adopted the notation of many linear algebra textbooks.\n \\( \\begin{bmatrix} 1 \u0026 0 \\\\ 5 \u0026 4 \\\\ 2 \u0026 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\)  To make more explicit the combination of features we can write,\n\\( x_1 \\begin{bmatrix} 1 \\\\ 5 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 0 \\\\ 4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\)  Since $m=3 \u0026gt; n=2$, we have more equations than unknowns we in general we have no solutions - a system with $m \u0026gt; n$ will be solvable only for certain right hand sides $\\mathbf{b}$. Those are all the vectors $\\mathbf{b}$ that lie in the column space of $A$.\nIn this example, as shown in the picture $\\mathbf{b}$ must lie in the plane spanned by the two columns of $A$. The plane is a subspace of $\\mathbb{R}^m=\\mathbb{R}^3$ in this case.\nNow instead of looking at what properties $\\mathbf{b}$ must have for the system to have a solution, lets look at the dual problem i.e. what weights $\\mathbf{x}$ can attain those $\\mathbf{b}$. The right-hand side $\\mathbf{b}=0$ always allows the solution $\\mathbf{x}=0$ The solutions to $A \\mathbf{x} = \\mathbf{0}$ form a vector space - the nullspace $\\mathcal{N}(A)$. The nullspace is also called the kernel of matrix $A$ and the its dimension $n-r$ is called the nullity.\n$\\mathcal{N}(A)$ is a subspace of $\\mathbb{R}^n=\\mathbb{R}^2$ in this case. For our specific example,\n\\( x_1 \\begin{bmatrix} 1 \\\\ 5 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 0 \\\\ 4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\)  the only solution that can satisfy this set of homogenous equations is: $\\mathbf{x}=\\mathbf{0}$ and this means that the null space contains only the zero vector and this\nTwo vectors are independent when their linear combination cannot be zero, unless both $x_1$ and $x_2$ are zero. The columns of $A$ are therefore linearly independent and they span the column space. They have therefore all the properties needed for them to constitute a set called the basis for that space and we have two basis vectors (the rank is $r=2$ in this case). The dimension of the column space is in fact the same as the dimension of the row space ($r$) and the mapping from row space to column space is in fact invertible. Every vector $\\mathbf{b}$ comes from one and only one vector $\\mathbf{x}$ of the row space ($\\mathbf{x}_r$). And this vector can be found by the inverse operation - noting that only the inverse $A^{-1}$ is the operation that moves the vector correctly from the column space to the row space. The inverse exists only if $r=m=n$ - this is important as in most ML problems we are dealing with \u0026ldquo;tall\u0026rdquo; matrices with the number of equations much larger than the number of unknowns which makes the system inconsistent (or degenerate).\nProjection onto the column space\nGeometrically you can think about the basis vectors as the axes of the space. However, if the axes are not orthogonal, calculations will tend to be complicated not to mention that we usually attribute to each vector of the basis to have length one (1.0).\nEigenvalues and Eigenvectors The following video gives an intuitive explanation of eigenvalues and eigenvectors and its included here due to its visualizations that it offers. The video must be viewed in conjunction with Strang\u0026rsquo;s introduction\n During the lecture we will go through an example from how your brain processes the sensory input generated by the voice of the lecturer(unless you are already asleep by that time) to combine optimally the sound from both your ears.\nA geometric interpretation of the eigenvectors and eigenvalues is given in the following figure:\n"});index.add({'id':32,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/optimization/','title':"Optimization and Stochastic Gradient Descent",'content':" Optimization and Stochastic Gradient Descent In this lecture we will go over concepts from Ian Goodfellow\u0026rsquo;s chapter 4 below. Stochastic gradient descent is treated also in section 5.9.\n "});index.add({'id':33,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/probability/','title':"Probability and Information Theory Basics",'content':" Book Chapters From Ian Goodfellow\u0026rsquo;s book: \nWe will go through the main points during the lecture and treat also MacKay\u0026rsquo;s book (Chapter 2) that is also instructive and a much better in introducing probability concepts. If you are a visual learner, the visual information theory blog post is also a good starting point.\nKey Concepts to understand Probability The pictures below are from MacKays book and despite their conceptual simplicity they hide many questions that we will go over the lecture.\nProbability distributions Probability distribution over the letters of the English alphabet (letter 27 symbolizes space) as measured by reading the Linux FAQ document.\nJoint probability distributions Joint probability $P(x,y)$ distribution over the 27x27 possible bigrams $xy$ found in this document: https://www.tldp.org/FAQ/pdf/Linux-FAQ.pdf\nWhat is the marginal probability $P(x)$ ?\nConditional probability distribution\nConditional probability distribution over the 27x27 possible bigrams $xy$ found in this document: https://www.tldp.org/FAQ/pdf/Linux-FAQ.pdf\nAre $x$ and $y$ independent ?\nProbability Rules If H is the hypothesis governing the probabilities distributions,\nProduct or chain rule:\nThis is obtained from the definition of conditional probability:\n$P(x,y|H) = P(x | y,H)P(y | H) = P(y | x,H)P(x |H)$\nSum rule:\nThis is obtaining by rewriting of the marginal probability denition: $P(x |H) = \\sum_y P(x,y |H) = \\sum_y P(x | y,H)P(y |H)$\nKey probability distributions Multi-variate Gaussian distribution $$f_{\\mathbf X}(x_1,\\ldots,x_k) = \\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right)}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma|}}$$ where where \u0026lt;${\\mathbf x}$ is a real \u0026lsquo;n\u0026rsquo;-dimensional column vector and $|\\boldsymbol\\Sigma|\\equiv \\operatorname{det}\\boldsymbol\\Sigma$ is the determinant of $\\boldsymbol\\Sigma$.\nApart from the definition, you need to connect the geometric interpretation of the bivariate Gaussian distribution to the eigendecomposition in the linear algebra lecture as shown in the Figure 2.7 of Bishop:\nSuch geometric interpretations will be very useful when we study dimensionality reduction via Principal Component Analysis (PCA).\nProbabilistic Modeling  The whole purpose of probabilistic modeling is to introduce uncertainty into our problem statement. There are three types of uncertainties:\n Inherent stochasticity - e.g. impact of wind in self-driving car control systems at moderate to high speed. Incomplete observability - e.g. sensor imperfections causing loss of sensing information Incomplete modeling - e.g. models and algorithms that are not implementable to an analog world and need to be discretized.  Probabilities can be used in two ways.\n Probabilities can describe frequencies of outcomes in random experiments Probabilities can also be used, more generally, to describe degrees of belief in propositions that do not involve random variables. This more general use of probability to quantify beliefs is known as the Bayesian viewpoint. It is also known as the subjective interpretation of probability, since the probabilities depend on assumptions.  The Bayesian theorem is the cornerstone of probabilistic modeling. If $\\mathbf{\\theta}$ denotes the unknown parameters, $D$ denotes the dataset and $\\mathcal{H}$ denotes the hypothesis space - the model we have seen in the ML problem statement section.\n   $$ P(\\mathbf{\\theta} | D, \\mathcal{H}) = \\frac{P( D | \\mathbf{\\theta}, \\mathcal{H}) P(\\mathbf{\\theta} | \\mathcal{H}) }{ P(D|\\mathcal{H})} $$\nThe Bayesian framework allows the introduction of priors from a wide variety of sources (experts, other data, past posteriors, etc.) For example,a medical patient is exhibiting symptoms x, y and z. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor (the expert) has beliefs about which disease, but a second doctor may have slightly different beliefs.\nNOTE: The Probabilistic Programming \u0026amp; Bayesian Methods for Hackers book is one of the best resources out there containing practical python examples. In addition they have been recoded recently to work in Tensorflow Probability an industrial-strength framework that can bring together Deep Learning and domain-specific probabilistic modeling. The book cant match the rigorousness of Bishop\u0026rsquo;s book but it offers a good treatment on problems and use cases and should be considered complimentary.\n Bayesian update example  -- This example is instructive beyond the habit of having coin flip examples in every textbook in probability theory and statistics. It is useful to understand the conjugate prior distribution being discussed in Bishop\u0026rsquo;s section 2.1.1 and Figure 3 that the code above replicates. Armed with this understanding, we can now treat the Bayesian update for linear regression as described in the linear regression section.\nInformation-theoretic definitions Entropy An outcome $x_t$ carries information that is a function of the probability of this outcome $P(x_t)$ by,\n$I(x_t) = \\ln \\frac{1}{P(x_t)} = - \\ln P(x_t)$\nThis can be intuitively understood when you compare two outcomes. For example, consider someone is producing the result of the vehicular traffic outside of Holland tunnel on Monday morning. The information that the results is \u0026ldquo;low\u0026rdquo; carries much more information when the result is \u0026ldquo;high\u0026rdquo; since most people expect that there will be horrendous traffic outside of Holland tunnel on Monday mornings. When we want to represent the amount of uncertainty over a distribution (i.e. the traffic in Holland tunnel over all times) we can take the expectation over all possible outcomes i.e.\n$H(P) = - \\mathbb{E} \\ln P(x)$\nand we call this quantity the entropy of the probability distribution $P(x)$. When $x$ is continuous the entropy is known as differential entropy. Continuing the alphabetical example, we can determine the entropy over the distribution of letters in the sample text we met before as,\nThis is 4.1 bits (as the $\\log$ is taken with base 2). This represents the average number of bits required to transmit each letter of this text to a hypothetical receiver. Note that we used the information carried by each \u0026ldquo;outcome\u0026rdquo; (the letter) that our source produced. If the source was binary, we can plot the entropy of such source over the probability p that the outcome is a 1 as shown below,\nThe plot simply was produced by taking the definition of entropy and applying to the binary case,\n$H(p) = - [p \\ln p - (1-p) \\ln(1-p)]$\nAs you can see the maximum entropy is when the outcome is most unpredictable i.e. when a 1 can show up with uniform probability (in this case equal probability to a 0).\nRelative entropy or KL divergence (optional) In the ML problem statement, it is evident that the job of the learning algorithm is to come up with a final hypothesis that is close to the unknown target function. In other occasions, we need to approximate a distribution by sampling from another easier to model distribution. As in ML we work with probabilities, we need to have a metric that compares two probability distributions ${P(x),Q(x)}$ in terms of their \u0026ldquo;distance\u0026rdquo; from each other (the quotes will be explained shortly). This is given by the quantity known as relative entropy or KL divergence.\n$KL(P||Q)= \\mathbb{E}[\\ln P(x) - \\ln Q(x)]$\nIf the two distributions are identical, $KL=0$ - in general however $KL(P||Q) \\ge 0$. One key element to understand is that $KL$ is not a true distance metric as its assymetric. Ensure that you understand fully the following figure and caption.\nVery close to the relative entropy is probably one of the most used information theoretic concepts in ML: the cross-entropy. We will motivate cross entropy via a diagram shown below,\nBackground for logistic regression If $\\sigma$ is a probability of an event, then the ratio $\\frac{\\sigma}{1-\\sigma}$ is the corresponding odds, the ratio of the event occurring divided by not occurring. For example, if a race horse runs 100 races and wins 25 times and loses the other 75 times, the probability of winning is 25\u0026frasl;100 = 0.25 or 25%, but the odds of the horse winning are 25\u0026frasl;75 = 0.333 or 1 win to 3 loses. In the binary classification case, the log odds is given by\n$$ \\mathtt{logit}(\\sigma) = \\alpha = \\ln \\frac{\\sigma}{1-\\sigma} = \\ln \\frac{p(\\mathcal{C}_1|\\mathbf{x})}{p(\\mathcal{C}_2|\\mathbf{x})}$$\nWhat is used in ML though is the logistic function of any number $\\alpha$ that is given by the inverse logit:\n$$\\mathtt{logistic}(\\alpha) = \\sigma(\\alpha) = \\mathtt{logit}^{-1}(\\alpha) = \\frac{1}{1 + \\exp(-\\alpha)} = \\frac{\\exp(\\alpha)}{ \\exp(\\alpha) + 1}$$\nand is plotted below. It maps its argument to the \u0026ldquo;probability\u0026rdquo; space [0,1].\nLogistic sigmoid (red)\nThe sigmoid function satisfies the following symmetry:\n$$\\sigma(-\\alpha) = 1 - \\sigma(\\alpha)$$\nIn addition it offers very convenient derivatives and has been used extensively in deep neural networks (for many architectures has been superceded by RELU). The derivative can be obtained as follows:\nConsider $$ f(x)=\\dfrac{1}{\\sigma(x)} = 1+e^{-x} . $$ Then, on the one hand, the chain rule gives $$ f\u0026rsquo;(x) = \\frac{d}{dx} \\biggl( \\frac{1}{\\sigma(x)} \\biggr) = -\\frac{\\sigma\u0026rsquo;(x)}{\\sigma(x)^2} , $$ and on the other hand, $$ f\u0026rsquo;(x) = \\frac{d}{dx} \\bigl( 1+e^{-x} \\bigr) = -e^{-x} = 1-f(x) = 1 - \\frac{1}{\\sigma(x)} = \\frac{\\sigma(x)-1}{\\sigma(x)} $$\nEquating the two expressions we finally obtain,\n$$\\sigma\u0026rsquo;(x) = \\sigma(x)(1-\\sigma(x))$$\n"});index.add({'id':34,'href':'/cs-gy-6613-spring-2020/docs/projects/imu-classification/','title':"Project 1 - Surface Type Classification",'content':" Project 1 - Surface Type Classification  Your first project description is published in https://www.kaggle.com/c/career-con-2019/overview\n You must submit your assignment with the results by 11:59pm 2/23/2020. The submission will be done by sharing the github/kaggle notebook with the TA.\n  "});index.add({'id':35,'href':'/cs-gy-6613-spring-2020/docs/projects/continuous-learning/','title':"Project 2 - Continual Learning for Robotic Perception",'content':" Project 2 - Continual Learning for Robotic Perception One of the greatest goals of AI is building an artificial continual learning agent which can construct a sophisticated understanding of the external world from its own experience through the adaptive, goal-oriented and incremental development of ever more complex skills and knowledge. Continual learning is essential in robotics where high dimensional data streams need to be constantly processed and where naïve continual learning strategies have been shown to suffer from catastrophic forgetting.\nYou will use this dataset and evaluate your methods for New Classes (NC) scenario. This is a very active area in AI right now - see here\n"});index.add({'id':36,'href':'/cs-gy-6613-spring-2020/docs/projects/project-3/','title':"Project 3",'content':""});index.add({'id':37,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/random-forests/','title':"Random Forests",'content':" Random Forests Random forests is a popular and very strong ML algorithm that belongs to what is called ensemble learning methods. As the name implies they use many classification tree learners to improve on their generalization ability. Each of the trees is called the weak learner - when grouped together they form the strong learner.\nA key aspect of decision forests is the fact that its component trees are all randomly different from one another. This leads to de-correlation between the individual tree predictions and, in turn, to improved generalization. Forest randomness also helps achieve high robustness with respect to noisy data. Randomness is injected into the trees during the training phase. Two of the most popular ways of doing so are:\n Random training data set sampling (when sampling is performed with replacement, this is called bagging), Randomized node optimization.  These two techniques are not mutually exclusive and could be used together.\nIn the sklearn library, the Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity. In addition, we can make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds - also known as Extra-Trees.\nInference Example of three trees receiving the instance $\\mathbf x$ (shown as $\\mathbf v$ in the figure)\nDuring testing the same unlabelled test input data $\\mathbf x$ is pushed through each component tree. At each internal node a test is applied and the data point sent to the appropriate child. The process is repeated until a leaf is reached. At the leaf the stored posterior $p_t(c|\\mathbf x)$ is read off. The forest class posterior $p(c|\\mathbf x)$ is simply the average of all tree (T) posteriors.\n$p(c|\\mathbf x) = \\frac{1}{T} \\sum_{t=1}^T p_t(c|\\mathbf x) $\nImpact of random forest parameters Given a training dataset with two classes (a), different training trees produce different partitions and thus different leaf predictors. The colour of tree nodes and edges indicates the class probability of training points going through them (b) In testing, increasing the forest size $T$ produces smoother class posteriors.\n"});index.add({'id':38,'href':'/cs-gy-6613-spring-2020/tags/','title':"Tags",'content':""});index.add({'id':39,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/netflix/','title':"The Netflix Prize and Singular Value Decomposition",'content':" Introduction The following are based on the winning submission paper as well as their subsequent publication.\nThe Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users or the films being identified except by numbers assigned for the contest.\nThe competition was held by Netflix, an online DVD-rental and video streaming service, and was open to anyone who is neither connected with Netflix (current and former employees, agents, close relatives of Netflix employees, etc.) nor a resident of certain blocked countries (such as Cuba or North Korea).[1] On September 21, 2009, the grand prize of US \\$ 1,000,000 was given to the BellKor\u0026rsquo;s Pragmatic Chaos team which bested Netflix\u0026rsquo;s own algorithm for predicting ratings by 10.06\\%\nThis competition is instructive since:\n Collaborative filtering models try to capture the interactions between users and items that produce the different rating values. However, many of the observed rating values are due to effects associated with either users or items, independently of their interaction. A prime example is that typical CF data exhibit large user and item biases – i.e., systematic tendencies for some users to give higher ratings than others, and for some items to receive higher ratings than others. Observing the posted improvements in RMSE over time, the competition has become of little business value to Netflix after a while. This means that it was unlikely that any minute improvement to RMSE (e.g. 0.1%) will translate to additional revenue. The 10% improvement goal was a lucky number after all. Netflix had no clue as to if this was the right number when they defined the terms. Any small deviation from this number, would have made the competition either too easy or impossibly difficult.  Problem statement You are given the following dataset structure (will be explained in class) shown below,\nAssuming that the rating matrix A is an m x n matrix with m users (500K) and n items (17K movies), this matrix is extremely sparse - it has only 100 million ratings, the remaining 8.4 billion ratings are missing (about 99% of the possible ratings are missing, because a user typically rates only a small portion of the movies).\nGiven the very large data matrix it was only expected that competitors attempted to work out some form of dimensionality reductions and as it turns out this was the basis for the winning algorithm. If you recall the premise of SVD from linear algebra, it is a decomposition that can work with rectangular matrices and can result into a decomposition of the following kind:\n$$A = U \\Sigma V^†$$\nwhere $\\mathbf{U}$ is a $m \\times m$ real unitary matrix, $\\mathbf{\\Sigma}$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $\\mathbf{V}$ is a $n \\times n$ real unitary matrix. Remember that a unitary matrix $U$ is a matrix that its conjugate transpose $U^†$ is also its inverse - its the complex analog of the orthogonal matrix and we have by definition $UU^†=I$.\nThe columns of $U$ are eigenvectors of $AA^T$, and the columns of $V$ are eigenvectors of $A^TA$. The $r$ singular values on the diagonal of $\\Sigma$ are the square roots of the nonzero eigenvalues of both $AA^T$ and $A^TA$.\nIt is interesting to attribute the columns of these matrices with the four fundamental subspaces:\n The column space of $A$ is spanned by the first $r$ columns of $U$. The left nullspace of $A$ are the last $m-r$ columns of $U$. The row space of $A$ are the first $r$ columns of $V$. The nullspace of $A$ are the last $n-r$ columns of V.  We can write the SVD as,\n$$A = U \\sqrt{\\Sigma} \\sqrt{\\Sigma} V^†$$\nand given the span of the subspaces above we can now intuitively think what the terms $\\mathbf{p}_u = U \\sqrt{\\Sigma}$ and $\\mathbf{q}_i = \\sqrt{\\Sigma} V^†$ represent.\nSVD decomposition reveals latent features weighted by the underlying singular values of the data matrix\nThe first term represents the column space aka. it provides a representation of all inter-user factors (also called latent features, latent means hidden). The second term represents the row space aka. it provides a representation of all inter-movie factors. Which brings us to the major point of what the $\\sqrt{\\Sigma}$ is doing to both terms. It represents the significance of those factors and therefore we can very easily use the singular values it contains to \u0026ldquo;compress\u0026rdquo; our representation by selecting the $k$ largest singular values and ignoring the rest.\nGiven these vectors of factors we can now use them to predict the rating:\n*Prediction of a rating is the product between user latent features and movie latent features matrices.\nFor an intuitive description of the SVD solution see here.\n"});index.add({'id':40,'href':'/cs-gy-6613-spring-2020/docs/projects/env/','title':"Your Programming Environment",'content':" Your Programming Environment Starting Jupyter in Google Colab The runtime performance will greatly improve for some projects using the free GPU resources provided by Google Colab. In this course we will make use of these facilities - the good news is that you have an account in Google Colab as most of you have a google account. If not go ahead and create one to be able to login into Google colab. You will need Google Colab for all your projects so that you can demonstrate that your results can be replicated. In addition Colab has many features that come handy.\nI heavily borrowed from Geron\u0026rsquo;s book for the following.\nSetup Anaconda Python When using Anaconda, you need to create an isolated Python environment dedicated to this course. This is recommended as it makes it possible to have a different environment for each project, with potentially different libraries and library versions:\n$ conda create -n cs6613 python=3.6 anaconda $ conda activate cs6613 This creates a fresh Python 3.6 environment called cs6613 (you can change the name if you want to), and it activates it. This environment contains all the scientific libraries that come with Anaconda. This includes all the libraries we will need (NumPy, Matplotlib, Pandas, Jupyter and a few others), except for TensorFlow, so let\u0026rsquo;s install it:\n$ conda install -n cs6613 -c conda-forge tensorflow This installs the latest version of TensorFlow available for Anaconda (which is usually not the latest TensorFlow version) in the cs6613 environment (fetching it from the conda-forge repository). If you chose not to create an cs6613 environment, then just remove the -n cs6613 option.\nNext, you can optionally install Jupyter extensions. These are useful to have nice tables of contents in the notebooks, but they are not required.\n$ conda install -n cs6613 -c conda-forge jupyter_contrib_nbextensions Kaggle Assuming you have activated the cs6613 conda environment, follow the directions here to install the Kaggle command line interface (CLI). You will need Kaggle for all your projects. You guessed it right - all the projects in this course are in fact Kaggle competitions. Not only you will get to compete (your ranking relative to others does not matter per se), but as you improve your knowledge over time you can revisit these competitions and see how your score improves.\nYou are all set! Next, jump to the Starting Jupyter section.\nStarting Jupyter locally If you want to use the Jupyter extensions (optional, they are mainly useful to have nice tables of contents), you first need to install them:\n$ jupyter contrib nbextension install --user Then you can activate an extension, such as the Table of Contents (2) extension:\n$ jupyter nbextension enable toc2/main Okay! You can now start Jupyter, simply type:\n$ jupyter notebook This should open up your browser, and you should see Jupyter\u0026rsquo;s tree view, with the contents of the current directory. If your browser does not open automatically, visit localhost:8888. Click on index.ipynb to get started!\nNote: you can also visit http://localhost:8888/nbextensions to activate and configure Jupyter extensions.\nGit / Github Git is the defacto standard when it comes to code version control. Learning basic git commands takes less than half an hour. However, to install git and understand the principle behind git, please go over Chapters 1 and 2 of the ProGit book.\nAs we have discussed in class you need to be able to publish your work in Github so you need to create a Github account. Then you will use the git client for your operating system to interact with github and iterate on your projects. You may be using Kaggle or Colab hosted notebooks but the underlying technology that powers such web-frontends when it comes to committing the code and seeing version numbers in your screen is git.\nIn addition, almost no data science project starts in vacuum - there is almost always software that will be inherited to be refined.\n"});})();