'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/','title':"Course Introduction",'content':"AI Evolution according to DARPA If engineering difficulty has a pinnacle today this must be in AI domains that combines ML, optimal control and planning. autonomous cars and humanoids from Boston Dynamics fit the bill.\nInitially there were rules.\n In the 1980s knowledge-base systems that hard-coded knowledge about the world in formal languages.  IF this happens, THEN do that.   They failed to get significant traction as the number of rules that are needed to model the real world exploded. However, they are still in use today in vertical modeling domains e.g. fault management. For example Rule Based Engines are used today in many complex systems that manage mission critical infrastructures e.g. ONAP.  The introduction of advanced AI methods few years ago, created a situation we can explain with the following analogy.\nA nautical analogy on where we are today on AI for mission critical systems. Can you notice anything strange with this ship (Cumberland Basin, photo taken April 1844)?\nTo put order into the many approaches and methods for delivering AI in our lives, DARPA classified AI development in terms of \u0026ldquo;waves\u0026rdquo;.\n         Wave I: GOFAI Wave II: Connectionism Wave III: AGI   In the 1980s Rule Based Engines started to be applied manifesting the first wave of AI introduction. In this example you see a system that performs highway trajectory planning. A combination of cleverly designed rules does work and offers real time performance but cannot generalize and therefore have acceptable performance in other environments.\nWave II srarted soon after 2010 - we started to apply a different philosophy in solving intelligent tasks such as object classification. The philosophy of connectionism and the so called deep neural network architectures, dominate today relative simple (and mostly self-contained) tasks.\nWave III is at present an active research area driven primarily from our inability to implement with just deep neural networks things like long-term goal planning, causality, extract meaning from text like humans do, explain the decisions of neural networks, transfer the learnings from one task to another, even similar, task. Artificial General Intelligence is the term usually associated with such capabilities.\nFurther, we will see a fusion of disciplines such as physical modeling and simulation with representation learning to help deep neural networks learn using data generated by domain specific simulation engines.\nReveal the stenosis:Generative augmented physical (Computational Fluid Dynamics) modeling from Computer Tomography Scans\nFor example in the picture above a CFD simulation is used to augment ML algorithms that predict and explain those predictions. I mission critical systems (such as medical diagnostic systems) everything must be explainable.\nConsult the course Syllabus to understand what elements of Wave II AI systems we will cover in this course.\n"});index.add({'id':1,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/numpy-pandas/','title':"Numerical Python (Numpy/Scipy and Pandas) Tutorials",'content':"Standard Python Below is a list of recommended courses you can attend to. We will go over briefly basic Python in this lecture. The tutorials below are self contained and can remind you the basics.\n  CodeAcademy Data Science Path. Take Python modules 4-10. This course contains Numpy and Panda intro as well.\n  Kaggle Python Course\n  Google Python Class This is a bit dated as it covers Python 2, but it is still highly regarded as Python 3 and 2 have few differences.\n  Numpy specific We will go Numpy Tutorial from Stanford\u0026rsquo;s CS231n.\n Numpy Cheatsheet\n Pandas (optional) Pandas may not be needed for the AI course. In any case, this is the effectively \u0026ldquo;official\u0026rdquo; documentation on Pandas: Pandas: powerful Python data analysis toolkit\n"});index.add({'id':2,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/','title':"A systems approach to AI",'content':"The four approaches towards AI The Turing Test Approach A 5-min behavioral intelligence test, where an interrogator chats with the player and at the end it guesses if the conversation is with a human or with a programmed machine. A Turing contest (Loebner Prize) is is held annually since 1991.\nThis course\u0026rsquo;s projects includes the Alexa prize which is not a turing test. The Alexa Prize creates social bots that engage in interesting, human-like conversations, not to make them indistinguishable from a human when compared side-by-side. Social bots may have ready access to much more information than a human.\n       Summary of the Turing Test The Alexa Prize is not a Turing Test   What capabilities we need to have to pass a turing test.\n Natural Language Processing Knowledge Representation Automated Reasoning Machine Learning Computer Vision Robotics  The last two capabilities are not needed for the verbal oriented Turing test but they are needed for what is called the total Turing test. According to this test, the player and the interrogator can communicate physically. For example, there is a hatch where the interrogator can pass objects to the player through. Obviously the player must have perception abilities to understand what object it is (5) and possibly a body that can manipulate the object (6). Embodied AI research is one of the hotter areas of AI today as we will see in the Agents section.\nThe Cognitive Model approach Newell, in his book \u0026ldquo;Unified Theories of Cognition\u0026rdquo;, defines cognition as:\n Memory, learning, skill Perception, motor behavior Problem solving, decision making, routine action Language Motivation, emotion Imagining, dreaming   How do we differentiate a cognitive model from a conceptual or statistical model? “Cognitive science is concerned with understanding the processes that the brain uses to accomplish complex tasks including perceiving, learning, remembering, thinking, predicting, problem solving, decision making, planning, and moving around the environment. The goal of a cognitive model is to scientifically explain one or more of these basic cognitive processes, or explain how these processes interact.”, \u0026ndash;Busemeyer \u0026amp; Diederich (2010)\n These theories of cognitive processes are tested via various cognitive architectures. Its important to realize that much of todays\u0026rsquo; debate about the path ahead in AI maps to the few different architectures. The hybrid architecture (symbolic and connection-oriented) is what is being investigated today by many research institutions.\nThe Syllogism-based approach The translation from Greek of the word syllogism is to support logic.\nThis approach emphasizes how we think the right way and encodes the pattern of logical arguments that can reach correct conclusions from a set of propositions (premises). Problem solving where the problem statement is expressed in a logic notation, matured in the 60s. As we have seen in the course introduction such rule-based systems are still with us and for small and deterministic state spaces can provide a very compact and elegant way to inference.\nLogic-based reasoning is coming back to fashion. One of the most promising areas is their application to interpretability (also known as explainability ) of deep learning based methods for e.g. classification in medical diagnosis. Probabilistic Logic Networks (PLN) are extensions of this approach to address problems with uncertainty.\nThe Rational Agent approach A rational agent acts to achieve the best outcome. The rational approach encompasses the syllogism and Turing-test approaches. We still need provably correct inference and the formal representations of logic as well as the ability to perceive, communicate, and learn to achieve a good outcome. But we need to generalize these approaches to include \u0026ldquo;good-enough\u0026rdquo; inference and adaptation to the changing environment contexts that the agent is facing without giving up on the mathematical formality that utility theory allows us to design such agents.\nThe agent facing a fire is an instructive example. There maybe no time for optimal reasoning to a conclusion (e.g. run) but a simple reflexive plan can offer the best outcome.\nAI as a distributed system approach As its evident from all existing approaches towards AI, multidisciplinary science that aims to create agents that can think and act humanly or rationally. This course starts the new decade filled with the promises of the previous one - AI is not only around the corner and it can take several decades of R\u0026amp;D for it to match human intelligence. Our purpose here is to (a) understand and appreciate the significant progress that certain components of AI have made over the last few years and (b) to be able to synthesize such components into AI systems that can at least solve domain-specific problems. In other words we are not trying to solve the most difficult and general AI problem as we don\u0026rsquo;t know its solution. We also can\u0026rsquo;t wait as we would like to participate in the GAI developments to begin with.\nA substantial part of AI is machine learning (ML) and that component alone is worth of at least a couple semesters. ML nowadays is used to process the visual sensing (computer vision), verbal commands (speech to text) and many other front-end functions using structures known as Deep Neural Networks (DNNs). These functions are usually effective in modeling the reflexive part of human brain. Their performance sometimes hides the enormous efforts by R\u0026amp;D teams to create carefully curated datasets for the task at hand. When supervised datasets are not enough for the design of reflexive agents policies, we need additional tools such as Deep Reinforcement Learning that offer the possibility to learn agent control policies from world models (or even without them) that in many instances means spending considerable time simulating the environment.\nAI is a system with the ability to represent the world and abstract concepts at multiple levels. If we are to draw the architecture of such system, it will have the ability to quickly change depending on the domain and task at hand. Just like origami, AI systems will morph into a suitable architecture, facilitated by high speed interconnections between its subsystems. The controller that controls such changes must be topology aware i.e. knowing the functional decomposition of the AI system and what support for representations and abstractions each subsystem can offer. How these can be combined and ultimately used, is something that needs to be learned. To generalize, such morphic control agents must be able to perform across task domains.\nAI distributed system comprising from a number of high-speed interconnected subsystems that are loosely coupled and communicate via a universal language. Line thickness indicates stronger coupling / dependecies between subsystems for the task at hand at this moment in time.\nIn a limited demonstration of such ability, closed worlds such as games, we have agents that can process thousands of pixels and can create abstractions at the symbolic level. Are they able to generalize ? Doubtful. Which brings us to the very interesting thought. For the vast majority of mission critical industries, we may reach in this decade a good enough performance level. The internet didn\u0026rsquo;t have 1 Gbps at each household even 5 years ago. But the moment we crossed the 1 Mbps level per user, at the hands of innovators, it managed to change the world as we know it despite its many initial performance issues. The internet does not kill, many people will argue but if anyone believes this analogy, todays\u0026rsquo; AI architecture, a bunch of service-oriented silos (APIs) offered by major technology firms, resembles the disconnected/siloed PC before the invention of HTTP and the internet era of the 90s. The protocol and controls that will allow such AI systems to communicate and by doing so demonstrate an ability to synthesize a non-trivial version of intelligence is one of the missing links.\nThe architecture of serf-driving cars in the late 2010s. To avoid wondering around the various disconnected use cases, we need to pick a domain that we can use as an application theme. Given the importance of the mission critical industries in the economy of every country, in this course we have selected robotics / self-driving cars. This domain requires the design of advanced agents that perceive the environment using noisy sensors, make decisions under uncertainty, actuate a host of electronics to execute decisions, communicate with humans in natural language or be able to sense driver psychological state and many more.\n"});index.add({'id':3,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/linear-regression/','title':"Linear Regression",'content':"Linear Regression Now that we have introduced somewhat more formally the learning problem and its notation lets us study a simple but instructive regression problem from Chapter 1 of Bishop\u0026rsquo;s book that is known in the statistics literature as shrinkage. Note that in many figures below the label is denoted as $t$ rather than $y$ as used in the equations below.\nSuppose that we are given the training set $\\mathbf{x} = {x_1,\u0026hellip;,x_m}$ together with their labels, the vectors $\\mathbf{y}$. We need to construct a model such that a suitably chosen loss function is minimized for a different set of input data, the so-called test set. The ability to correctly predict when observing the test set, is called generalization.\nTraining Dataset (m=10) for the Regression Model. The green curve is the uknown target function.\nSince the output $y$ is a continuous variable then the supervised learning problem is called a regression problem (otherwise its a classification problem). The dataset is generated (in data scienece these datasets are called synthetic) by the function $sin(2 \\pi x) + ϵ$ where $x$ is a uniformly distributed random variable and $ϵ$ is $N(\\mu=0.0, \\sigma^2=0.3)$. This target function is completely unknown to us - we just mention it here for completeness.\nLet us now pick the hypothesis set that correspond to polynomials of the following form,\n$$g(\\mathbf{w},x_i) = w_0 + w_1 x_i + w_2 x_i^2 + \u0026hellip; + w_M x_i^M$$\nOur job is to find $\\mathbf{w}$ such that the polynomial above fits the data we are given - as we will see there are multiple hypothesis that can satisfy this requirement. To gauge our investigation, we need to define a metric, an error or loss function in fact, that is also a common metric in regression problems of this nature. This is the Mean Squared Error (MSE) function.\n$$L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m {(g(\\mathbf{w},x_i)-y_i)}^2$$\nThe loss function chosen for this regression problem, corresponds to the sum of the squares of the displacements of each data point and our hypothesis. The sum of squares in the case of Gaussian errors gives raise to an (unbiased) Maximum Likelihood estimate of the model parameters. Contrast this to sum of absolute differences.\nNow our job has become to choose two things: the weight vector $\\mathbf{w^*}$ *and* $M$ the order of the polynomial. **Both** define our hypothesis. If you think about it, the order $M$ defines the model complexity in the sense that the larger $M$ becomes the more the number of weights we need to estimate and store. Obviously this is a trivial example and storage is not a concern here but treat this example as instructive for that it applies in many far for complicated settings.\nObviously you can reduce the training error to almost zero by selecting a model that is complicated enough (M=9) to perfectly fit the training data (if m is small).\nBut this is not what you want to do. Because when met with test data, the model will perform far worse than a less complicated model that is closer to the true model (e.g. M=3). This is a central observation in statistical learning called overfitting. In addition, you may not have the time to iterate over M (very important in online learning settings).\nTo avoid overfitting we have multiple strategies. One straightforward one is evident by observing the wild oscillations of the $\\mathbf{w}$ elements as the model complexity increases. We can penalize such oscillations by introducing the $l_2$ norm of $\\mathbf{w}$ in our loss function.\n$$L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m {(g(\\mathbf{w},x_i)-y_i)}^2 + \\frac{\\lambda}{2} ||\\mathbf{w}||^2$$\nThis type of solution is called regularization and because we effectively shrink the weight dynamic range it is also called in statistics shrinkage or ridge regression. We have introduced a new parameter $\\lambda$ that regulates the relative importance of the penalty term as compared to the MSE. This parameter together with the polynomial order is what we call hyperparameters and we need to optimize them as both are needed for the determination of our final hypothesis $g$.\nThe graph below show the results of each search iteration on the $\\lambda$ hyperparameter.\nLets reflect on the MSE and how model complexity gives raise to various generalization errors.\n$$MSE = \\mathbb{E}[\\hat{y}_i - y_i)^2] = \\mathrm{Bias}(\\hat{y}_i)^2 + \\mathrm{Var}(\\hat{y}_i)$$\nwhich means that the MSE captures both bias and variance of the estimated target variables and as shown in the plots above, increasing model capacity can really increase the variance of $\\hat{y}$. We have seen that as the $\\mathbf{w}$ is trying to exactly fit, or memorize, the data, it minimizes the bias (in fact for model complexity M=9 the bias is 0) but it also exhibits significant variability that is itself translated to $\\hat{y}$. Although the definition of model capacity is far more rigorous, we will broadly associate complexity with capacity and borrow the figure below from Ian Goodfellow\u0026rsquo;s book to demosntrate the tradeoff between bias and variance. What we have done with regularization is to find the $\\lambda$ that minimized generalization error aka. find the optimal model capacity.\nAs capacity increases (x-axis), bias (dotted) tends to decrease and variance(dashed) tends to increase, yielding another U-shaped curve for generalization error (bold curve). If we vary capacity along one axis, there is an optimal capacity, with underﬁtting when the capacity is below this optimum and overﬁtting when it is above.\nOLS as Maximum Likelihood Estimation These notes summarize the Maximum Likelihood approach and are provided here as they are more expansive compared to textbooks and at the same time do make the connection to the Linear Algebra background you should have.\nGeometrical Interpretation of Ordinary Least Squares (OLS)\n"});index.add({'id':4,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/','title':"The Way of Working in AI",'content':"A Positive Reinforcement Loop What are the disciplines that need to cross fertilize to get a system that possesses intelligence? Lets start with a diagram that show not only the disciplines but also a way of working for the many specialists involved.\nThe diagram above highlights three fundamental axes that can deliver a system-based approach to AI. The Z axis is the scientific axis where many disciplines such as psychology, neuroscience, mathematics and others make progress on. The X axis involves the ML/AI communities that borrow ideas from their colleagues in sciences and convert those theories and pragmatic findings into abstractions (models and methods). The model of the neuron, the perceptron, appeared in psychology journals many decades ago and despite its simplicity it is still the unit via which much more complicated neural networks are constructed from. Todays\u0026rsquo; models of Long-Term Short-Term Memory (LSTM), Replay Memory and many others not shown in the diagram (as its currently in draft form) are abstractions (models) of discoveries that scientists produced after tens of years of research. To use however these methods and models effectively, major hardware and software components need to be developed also known as computing and frameworks - these live in the Y axis. They are very important for the development of AI field that is known to be heavily experimental, requiring especially at the perceptive frontend significant computational power and automation.\nAt a very high level, progress in AI is made via the counterclockwise iteration Z -\u0026gt; X -\u0026gt; Y -\u0026gt; Z. AI engineers look at the neuroscience/psychology axis, map discoveries to points in the methods / models axis, and finally develop these methods in hardware architectures and software frameworks. But what can explain the Y -\u0026gt; Z flow? Frameworks in turn help the neuroscientists and psychologists as they can provide generative models of their own discoveries or help them simulate conditions that are not possible using their native tools.\nThis counter-clockwise multidisciplinary iteration acts as a positive feedback loop accelerating the progress in the AI space.\nIn this course we will be focusing on the methods/models and frameworks axis and understand what these models can offer us and how we can apply them in synthesizing an AI system at least for a domain of interest.\nA typical AI stack today As we have seen from the syllabus, this course approaches AI from an applied perspective - this means teaching concepts but at the same time looking how these concepts are applied in the industry to solve real world problems. In this respect here we take an architecture driven AI, presenting the components of AI in a form of a software stack but also how the components are mechanized in what we call ML Pipelines to provide the ML utility to applications. For a complete overview of real world ML pipelines used today go through the TFX paper in its entirety.\nAI Stack circa 2019\nLandscape of the AI ecosystem Due to the complexity and common interest to addresses industrial players are partnering to define and implement the necessary components for the complete automation of AI pipelines. This work is going in within the Linux Foundation AI (sub)Foundationamongst many other open source communities.\n   The four pipelines of an end-to-end ML platform Example of end to end pipeline - serial arrangement\nExample of Data Pipeline\nExample of Model Training Pipeline\nExample of Model Evaluation and Validation Pipeline\nExample of Serving Pipeline\nRoles in AI product development Who data scientists need to interact with, during the development of AI systems?\n \u0026ldquo;Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\u0026rdquo; http://www.melconway.com/Home/Conways_Law.html\n \u0026ldquo;We do research differently here at Google. Research Scientists aren\u0026rsquo;t cloistered in the lab, but instead they work closely with Software Engineers to discover, invent, and build at the largest scale.\u0026rdquo;\nContrast this to an organizational structure that isolates researchers from product development. What about Alphabet\u0026rsquo;s X https://x.company/ ?\n"});index.add({'id':5,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/','title':"Intelligent Agents and Representations",'content':"Agent-Environment Interface  An agent is a computer system that is situated in some environment, and that is capable of autonomous action in this environment in order to meet its design objectives.\n In general sensor data are converted via the agent function (that is implemented via a program) into actions as shown below.\nGeneral Agent-Environment Interface\nThe two most important agent architectures that we will deal with in this course are the utility and learning-based agents architectures. To start with we recognize that most of the problems we will face as agent designers are for agents operating in environments that are:\n Partially Observed (PO). This means that we cant see all the variables that constitute the state and we need to maintain an internal belief of the state variables that we cant perceive. Stochastic. This means that the environment state is affected by random events and can not be determined by the previous state and the actions of the agent (which in this case we are talking about deterministic environments). Such probabilistic characterization of the environment state is the norm in many settings such as as robotics, self-driving cars etc. Sequential As compared to episodic, in sequential environments actions now can have long term effects into the future. Dynamic In this setting, the environment state changes all the time, even while the agent is taking the action based on the sequence of percepts up to this point in time. In most settings the environments we deal will not be static. Continuous When the variables that constitute the environment state are defined in continuous domains. Time is usually considered a special variable and we may have environments where the time variable is discrete while other variables are continuous. Known This refers to the knowledge of the agent rather than the environment. In most instances we are dealing with environments where there is a set of known rules that govern the state transition. In driving for example, we know what steering does.  Architectures Rational Agent Architecture In the rational agent architecture we meet three key concepts:\n The need for the agent to keep internally the environment state (in probabilistic terms a belief). This is needed due to the the partially observed environment the agent is interfacing with. The presence of a world model that helps the agent to update its belief. The presence of a utility function that the agent can use to produce the value (happiness) when its action transitions the environment to a new state. Obvious the agent will try to optimize its actions in what we earlier described stochastic environments and therefore it will try to maximize the value (hapiness) on average (strictly in expectation) where the average is taken across the distribution of all possible states across time.  Learning Agent Architecture The learning agent architecture builds on top of the rational agent (the performance element in the figure below), additional functions that:\n Embeds a learner that learns the various models needed by the rational agent as well as allowing the rational agent to operate on unknown environments. In this respect it learns the world model, some elements of the utility function itself or the desirability of each actions it takes. To enable learning, the rational agent sends training data to the learner. Introduces a critic that transmits a positive or negative reward to the learner based on its own view of how the agent is doing. The learner can modify these models to make the rational agent perform better in the future. Introduces the problem generator that can change the problem statement of the rational agent. Obviously the expected utility objective will not change but the utility function itself may in fact change to lead the agent to perform more exploration (increase its risk) in its environment.  We will see in Deep Reinforcement Learning that this architecture is able to accommodate such end to end learning approach. In that setting the critic is part of the environment - see Solving sparse-reward tasks with Curiosity for an example where the critic is inside the agent generating intrinsic rewards.\nSolution space For each environment and each architecture there is a number of solutions that may be appropriate. The following figure presents the environment type to algorithm mapping that we will cover in this book.\n"});index.add({'id':6,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/knn/','title':"k-Nearest Neighbors (kNN) Classification",'content':"k-Nearest Neighbors (kNN) Classification kNN belongs to the class of algorithms that were extensively treated in pattern recognition literature many years ago. It is still extensively being used today especially in settings that require very fast decision/classifications. The general block diagram governing such systems is shown below.\nOne example that was originally treated by Patrick Winston (MIT) is the conveyor belt classification use case.\nLets assume that in a factory a high speed conveyor belt is carrying hundreds of widgets per minute and a system of cameras is used to classify them and instruct actuators that place them into bins. Obviously the system must satisfy strict throughput requirements. Lets assume for purely instructive purposes that the assignment of each widget is based on two features that shown below.\nExample decision boundaries between labels in the feature space.Stars are the distinct widgets (labels)\nEach pair of prototype widgets can be considered as defining a linear decision boundary - the line perpendicular to the line that connects them. So if the widget was screws, figure below shows two features categories, head-type and length.\nIn a hypothetical case of manufacturing screws with two lengths $l_1, l_2$ and two head types (flat and round) we will have four possible labels (each associated wiht the combination ${(l_1, F), (l_2, F), (l_1,R), (l_2, R)}$ as shown in the picture above and we can draw corresponding decision boundaries. Why the decision boundaries are like this (also called perpendicular bisectors) ? The decision rule is a Euclidean distance metric - any point in that line has the same distance to either of the two labels involved.\nAs screws go through the conveyor depth, manufacturing defects cause each screw to appear almost anywhere in the feature space. One intuitive and straightforward approach is to assign the label associated with the area enclosed by the decision boundaries (and the axes) and classify the screw as the label of the corresponding prototype widget. This is in essence the principle behind the k nearest points (or neighbors) algorithm.\nIn a sightly more formal setting, and to be able to address far more complex decision boundaries than the above, we are given data points in a training set $D = {(x_i,y_i)}, i={1, \u0026hellip;, m}$ and we are asked to classify points that are in the test set. The only variable of the kNN algorithm is the number $k$ which is the number of nearest neighbors that we consider in the classification decision. An example for two classes is shown in the figures below for two cases of $k$. The plot corresponds to the case we have two features like before $x_1, x_2$.\nk=3\nk=1\nThe algorithm effectively positions a sphere on the data point we want to classify whose radius is large as the it needs to be to enclose $k$ closest points irrespectively of their class. Obviously for the dimensions of the examples above, the sphere is a circle. As expected, we see that $k$ controls the degree of smoothing, so that small $k$ produces many small regions of each class, whereas large $k$ leads to fewer larger regions. In essence the algorithm for $k\u0026gt;1$, considers a majority vote between the $k$ closest points to the point we need to classify with ties broken at random.\nOne of the limitations of the knn algorithm is the requirement that the dataset $D$ is stored in memory and that the algorithm itself is dependent on efficient search algorithms that allow it to go over the data and find the nearest neighbors. There are solutions to both of these problems though and if implemented properly we get to be able to train for fairly complex decision boundaries that generalize well without the complexity associated with learning parametric models.\nNOTE: Although not part of this class, think about how to generalize knn. What happens when the votes are not binary ? How the algorithm can be adapted to online learning settings?\n"});index.add({'id':7,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/perceptron/','title':"The Perceptron",'content':"The Perceptron Frank Rosenblatt\nThis section captures the main principles of the perceptron algorithm which is the essential building block for neural networks.\nArchitecture of a single neuron The perceptron algorithm invented 60 years ago by Frank Rosenblatt in Cornell Aeronautical Laboratory. Neural networks are constructed from neurons - each neuron is a perceptron with a specific activation function. A single neuron is itself capable of learning \u0026ndash; indeed,various standard statistical methods can be viewed in terms of single neurons \u0026ndash; so this model will serve as a first and simple example of a supervised neural network.\nA single neuron has a number $n$ of inputs $x_i$ (note the figure is not compatible with this notation) and one output which we will here call $\\hat{y}$. Associated with each input is a weight $w_i$ ($i = 1 ,\\ldots, n$). The additional parameter $w_0$ of the neuron called a bias which we may view as being the weight associated with an input $x_0$ that is permanently set to 1. The single neuron is a feedforward device \u0026ndash; the connections are directed from the inputs to the output of the neuron. Feedforward neural networks are called Multi-Layer Perceptrons (MLPs).\nWhat does the perceptron compute? First, in response to the imposed inputs $\\mathbf{x}$, we compute the activation of the neuron,\n$$a = \\sum_{i=0}^n w_i x_i = \\mathbf{w}^T \\mathbf{x}$$\nSecond, the output (also called the activity of the neuron) is produced by passing the activation through a non linear activation function $\\hat{y} = g(a)$. The activation function of the perceptron is the step function - we will cover more of such functions in the treatment of neural networks.\n$$g(a) = \\begin{cases}1 \u0026amp; \\text{if }\\ a \\ge 0, \\\\ -1 \u0026amp; \\text{otherwise}\\end{cases}$$\nPerceptron Learning Algorithm The algorithm is derived from the application of the SGD to a suitably chosen loss function. The loss function can be easily designed if we start thinking about the class labels as belonging to the set ${+1,-1}$ (rather than the more usual ${0,1}$) and considering the value of the products $\\mathbf{w}^T x_j y_j$. If there are no classification errors for the chosen non-linear activation function above such products will result into positive numbers irrespectively of the class. For these cases we assign zero to the loss function. If there are errors however, these products will be negative and the sum of all these negative product terms we must maximize - or equivalently minimize the negative of such loss as below:\n$$L(\\mathbf{w}) = - \\sum_{j: \\hat{y_j} \\ne y} \\mathbf{w^T}x_j y_j$$\nWe will find the $\\mathbf{w}$ that minimize such loss using the familiar Stochastic Gradient Descent algorithm. Noting that the gradient of the loss function at $\\mathbf{w}$ is $x_j y_j$ we can write the SGD algorithm as follows:\nLet $t$ denote the iteration index and $r$ the learning rate.\n  Initialize the weights and the threshold. Weights may be initialized to zero or to a small random value.\n  For each example in our training set, perform the following steps over the input $\\mathbf{x}_j$ and desired output $y_j$:\n$$\\hat{y}_j(t) = g[\\mathbf{w}(t)^T \\mathbf{x}_j]$$\n  Update the weights:\n$$w_i(t+1) = w_i(t) + r (y_j - \\hat y_j(t)) x_j$$\n  for all features $0 \\leq i \\leq n$.\nThe updated weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.\nInitial parameter vector $\\mathbf w$ shown as a black arrow together with the corresponding decision boundary (black line), in which the arrow points towards the decision region which classified as belonging to the red class. The data point circled in green is misclassified and so its feature vector is added to the current weight vector, giving the new decision boundary shown in the plot below.\nThe next misclassified point to be considered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundary shown in the plot below for which all data points are correctly classified.  NOTE: For offline learning, the second step may be repeated until the iteration error $\\frac{1}{s} \\sum_{j=1}^s |y_j - \\hat{y}_j(t)| $ is less than a user-specified error threshold $\\gamma $, or a predetermined number of iterations have been completed, where \u0026lsquo;\u0026lsquo;s\u0026rsquo;\u0026rsquo; is the size of the training set.\n The perceptron is a linear classifier, therefore it will never get to the state with all the input vectors classified correctly if the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane. In this case, no \u0026ldquo;approximate\u0026rdquo; solution will be gradually approached under the standard learning algorithm, but instead learning will fail completely. Even in the case of linearly separable datasets, the algorithm may exhibit significant variance while it is executing as previously correctly classified examples may \u0026ldquo;fall\u0026rdquo; into the wrong decision region by an update that considers a currently misclassified example.\nFurther, the perceptron solution will depend on the initial choices of the parameters as well as the order of the training dataset presented. Support Vector Machines avoid such pitfalls which can motivate the question why we insisted on learning the perceptron algorithm: both architecturally and the functionally the linear combination of features followed by a non-linearity is the fundamental building block of far more complicated neural networks.\nPerceptron performance For a live demo of perceptron performance see the single neuron example for separable datasets in Tensorflow Playground\n"});index.add({'id':8,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/logistic-regression/','title':"Logistic Regression",'content':"Logistic Regression Logistic regression is used in machine learning extensively - every time we need to provide probabilistic semantics to an outcome e.g. predicting the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.), whether an voter will vote for a given party, predicting the probability of failure of a given process, system or product, predicting a customer\u0026rsquo;s propensity to purchase a product or halt a subscription, predicting the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in Natural Language Processing (NLP).\nBinary case If we consider the two class problem, we can write the posterior probability as,\n$$p(\\mathcal{C}_1|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1)}{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1) + p(\\mathbf{x}|\\mathcal{C}_2) p(\\mathcal{C}_2)} = \\frac{1}{1 + \\exp(-\\alpha)} = \\sigma(\\alpha)$$\nwhere $\\alpha = \\ln \\frac{p(\\mathbf{x}|\\mathcal{C}_1) p(\\mathcal{C}_1)}{p(\\mathbf{x}|\\mathcal{C}_2) p(\\mathcal{C}_2)}$ and $\\sigma$ is given by the logistic function we met in probability review.\nGiven the posterior distribution above we have for the specific linear activation,\n$$p(\\mathcal{C}_1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x})$$\nThis model is what statisticians call logistic regression - despite its name its a model for classification. The model has significant advantages in that it does require the estimation of far fewer parameters compared to the case where the class conditional distributions involved in the posterior were parametric. For example if we had Gaussian class conditionals we would had to estimate (using Maximum Likelihood) their parameters $\\mathbf \\mu$ and $\\mathbf \\Sigma$ that grow quadratically to the number of features $n$. With logistic regression we only have an evident linear relationship between parameters and features.\nThe figure below shows the corresponding posterior distribution $p(\\mathcal{C}_1|\\mathbf{x})$\nThe class-conditional densities for two classes, denoted red and blue. Here the class-conditional densities $p(\\mathbf{x}|\\mathcal{C}_1)$ and $p(\\mathbf{x}|\\mathcal{C}_2)$ are Gaussian\nThe corresponding posterior probability for the red class, which is given by a logistic sigmoid of a linear function of $\\mathbf{x}$.\nAs we said, with logistic regression we skip the assumption about the class-conditional densities as they add parameters to our problem that grow quadratic to the number of dimensions and we attempt to find the $n$ parameters of the model directly (the number of features) and sure enough we will use ML to do so.\nBy repeating the classical steps in ML methodology i.e. writing down the expression of the likelihood function (this will now be a product of binomials), we can write down the negative log likelihood function as,\n$$L(\\mathbf{w}) = - \\ln p(\\mathbf{y},\\mathbf{w}) = - \\sum_{i=1}^m {y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) }$$\nwhich is called cross entropy error function - probably the most widely used error function in classification due to its advantages such as its probabilistic and information theoretic roots as well as its shape shown in the figure below. Sometimes it is also called log-loss.\nMinimizing the error function with respect to $\\mathbf{w}$ by taking its gradient\n$$\\nabla L = \\sum_{i=1}^m (\\hat{y}_i - y_i) x_i$$\nthat defines the batch gradient decent algorithm. We can then readily convert this algorithm to SGD by considering mini-batch updates.\n"});index.add({'id':9,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/kernels/','title':"Kernels and the Kernel Trick",'content':"Kernels and the Kernel Trick Introduction In linear regression we have seen a simple dataset from an unknown non-linear target function. We then proceeded and chose a hypothesis from the polynomial family that mapped each input example $x$ into a function $g(x, \\mathbf w) = \\mathbf w^T \\phi(\\mathbf{x})$, found the optimal $\\mathbf w$ by maximizing the likelihood (ML) function using the MSE as the loss function. The moment we have $\\mathbf w$ we can use it to do new predictions. In this regression problem we have used a transformation from the raw data $\\mathbf x$ to the feature $\\phi(\\mathbf x)$ and more specifically we have used basis functions $\\phi_i(\\mathbf x)$ from the set of polynomials shown below.\nPolynomial basis functions\nWe could have chosen other sets as well:\nGaussian basis functions\nIn classification, we have seen lastly logistic regression which despite the non-linear (probabilistic) interpretation of its output it is still a linear classifier as it presents a linear decision boundary - linear in the feature space $(\\mathbf{w}^T\\mathbf{x})$. Which posses the question: what we do when we have a problem that has a non-linear decision boundary? The answer is via the concept of kernels that we describe next.\nMotivation Lets revisit the perceptron algorithm where we started from $\\mathbf{w}=\\mathbf{0}$ (or a random vector) and we visited each and every example, changing the direction of the decision boundary when we met a miss-classified example. In the plots of that lecture for example we have seen that after two misclassified examples (the green circled examples indexed 9 and 6) the algorithm converged to a final $\\mathbf{w}$. Note that the indices9 and 6 selected here are arbitrary. These two steps can be written as:\n$\\mathbf{w}^{(1)} = \\mathbf{w}^{(0)} + \\mathbf{x}^{(9)}$\n$\\mathbf{w}^{(2)} = \\mathbf{w}^{(1)} + \\mathbf{x}^{(6)}$\nWe can compress this into $\\mathbf{w}^{(2)} = \\mathbf{w}^{(0)} + {\\mathbf{x}^{(9)} + \\mathbf{x}^{(6)}}$ and in addition this example indicates that we just need to keep the so called sparse representation where we represent the final $\\mathbf{w}$ with the array ${[9, +1], [6, +1]}$, where \u0026ldquo;+1\u0026rdquo; happened to be the class of the two examples involved in the adjustment of the weight. Saying it a bit differently, the algorithm results in the set of points that are miss-classified throughput the journey of visiting each and every of our examples.\nFor non-linear decision boundaries, the straightforward approach we can take is to throw into the problem more features. This is visualized as shown in this animation for one of the datasets we have seen in Tensorflow Playground where we admitted that neither the perceptron or logistic regression cannot separate the two classes.\n In the visualization above we added the feature $\\mathbf{x}^2$ and therefore expanded the feature space $\\phi$ to ${\\mathbf{x}, \\mathbf{x}^2}$ and surely in the expanded space there is a linear decision boundary (a plane) that can separate the previously the linearly inseparable classes. Moreover this plane can be found by various linear classification methods and for the perceptron we can then using the same reasoning as above we can end up with a sparse representation for the weight that is normal to the plane and can be written as:\n${\\phi(\\mathbf{x}^{(k)}), \u0026hellip;, \\phi(\\mathbf{x}^{(d)})}$\nthat we can also programmatically represent as a linked list as before.\nThe problem with adding more features is that in high dimensional spaces (e.g. medical imaging) we may end up with problems of very high number of dimensions as we start from a large number of dimensions to begin with. For example, in imaging we are dealing with at least as many dimensions as the number of pixels in the image and sometimes many more. So although with the sparse representation we have avoiding writing down $\\mathbf{w}$ explicitly in the high dimensional space as we can represented as a linked list of indices and labels of the examples of interest, we still need to make operations on those high dimensional examples and therefore the problem now becomes how to avoid the explicit specification of $\\phi(\\mathbf{x})$. This is what we do with the kernel trick described next.\n Where in the perceptron algorithm we need to make operations in high dimensional (expanded) space? Hint: How do we determine if the example is a misclassified during training? How we make inference predictions?\n Kernel trick Now that we have motivated the need for the trick lets see what the trick is after all.\nThe dot product $\\mathbf{w}^T\\phi(\\mathbf{x})$ will be needed throughout training as well as inference.Given the sparse representation obtained earlier we realize that we will be dealing with lots of dot products of the form $\\phi(\\mathbf{x}^{(k)})^T \\phi(\\mathbf{x})$.\nThe kernel trick at a high level is this: instead of doing these dot products in the expanded high dimensional space, we will do them in the original lower dimensional space saving significant computations (even ensuring feasibility in some cases). The trick, no matter how unbelievable it may look at a first glance, is becoming a reality by choosing suitable kernels\n$$k(\\mathbf x_i, \\mathbf x_j) = \\phi(\\mathbf x_i)^T \\phi(\\mathbf x_j)$$\n$$= \\sum_{i=1}^M \\phi_i(\\mathbf x_i)^T \\phi_i(\\mathbf x_j)$$\nwhere $\\phi_i(\\mathbf x)$ are basis functions. Lets look at an example to see in practice the properties of such kernel functions.\nLets look at the kernel function we used in the previous example where we expanded the space to include a quadratic term but now we include all quadratic terms that correspond to basis function from the polynomial set. This means $\\phi(\\mathbf x) = (x_1^2, x_2^2, x_1 x_2)$. Lets us now form the dot product at the feature space and see if this choice of basis function has the property that this dot product can be written as a dot product of terms in the original space.\n$$k(\\mathbf x, \\mathbf z) = \\phi(\\mathbf x)^T \\phi(\\mathbf z)$$ $$=(x_1^2, \\sqrt{2} x_1 x_2, x_2^2) (z_1^2, \\sqrt{2} z_1 z_2, z_2^2)^T = x_1^2z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2$$ $$= (x_1 z_1 + z_2 z_2)^2 = (\\mathbf x^T \\mathbf z)^2$$\nSo this kernel has the desired property that allows us to execute the perceptron algorithm (or other classification algorithms as we will see shortly) at the expanded feature space, without requiring for us to do the processing at the feature space but at the original data space. We call such kernels, valid. Apart from polynomial basis functions we have just seen, the most trivial kernel is as you can imagine $k(\\mathbf x, \\mathbf z) = \\mathbf x^T \\mathbf z$ which by definition is not doing any space expansion and therefore degenerates to a linear decision boundary. This kernel can be used though to generate other kernels. One of them is the so called Gaussian kernel that is worth learning about:\n$k(\\mathbf x, \\mathbf z) = \\exp(-||\\mathbf x - \\mathbf z||^2 / {2\\sigma^2})$\n We can see kernels functions from another perspective as well. Kernel functions that possess the properties required by the kernel trick, i.e. dot products in the high dimensional space that degenerate to dot product in the original data space are similarity functions because dot products are just that: they express how similar (direction wise) a vector is to another.\n "});index.add({'id':10,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/face-recognition/','title':"Face Recognition - SVM Case Study",'content':" "});index.add({'id':11,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/clustering/','title':"K-means Clustering",'content':"K-means Clustering Up to now in this lecture series we have seen parametric models using in regression and classification. The complexity of such models was not very high despite the heany sometimes math. Most models we have seen took the form $\\mathbf w^T \\phi(\\mathbf x)$. Now we switch to a very frequently met case where we dont have any labels $y$. Most of the treatment here is from Bishop section 9.1 - the equivalent section in Tan\u0026rsquo;s book is section 7.2.\nSuppose we have a data set $D={ \\mathbf x_1,\u0026hellip;,\\mathbf x_m}$ consisting of $m$ observations of a random n-dimensional Euclidean variable $\\mathbf x$. Our goal is to partition the data set into some number $K$ of clusters, where we shall suppose for the moment that the value of $K$ is given. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. This is shown in the figure below.\nWe can formalize this notion by first introducing a set of n-dimensional vectors $\\mathbf \\mu_k$ where $k ={1,\u0026hellip;,K}$, in which $\\mathbf \\mu_k$ is a prototype associated with the kth cluster. We can think of the $\\mathbf \\mu_k$ as representing the centres of the clusters. Our goal is then to find an assignment of data points to clusters, as well as a set of prototypes. To do so we need to define as before a loss function.The loss function is selected to be the sum of the squares of the distances of each data point to its closest vector $\\mathbf \\mu_k$ is a minimum.\n$L = \\sum_{i=1}^m \\sum_{k=1}^K r_{ik} ||\\mathbf x_i - \\mathbf \\mu_k||^2$\nEverything is as expected in the loss function above expept from the variable $r_{ik}$ which is a binary *indicator* variable that will simply add the squared Euclidean distance between the point $\\mathbf x_i$ and the candidate cluster prototype if the point is assigned to that cluster or add 0.0 to the loss if it doesnt. As an example for two cluster prototypes (as evidently will be the number of clusters in the above example) $r_k \\in {(0 1), (1 0)}$. This is also called one-hot encoding of the corresponding category where category \u0026ldquo;cluster 1\u0026rdquo; is coded as $(0 1)$ and category \u0026ldquo;cluster 2\u0026rdquo; is coded as $(1 0)$.\nThe K-means algorithm is iterative and each iteration includes two steps after an initialization of the $\\mathbf \\mu_k$ to random locations.\nStep 1 We go over each data point $\\mathbf x_i$ and we assign it to the closest custer center for this iteration.\nIteration 1, Step-1: Assignment of data points to cluster centers\nMathematically this means determining:\n$$r_{ik} = \\begin{cases}1, \u0026amp; \\text{if } k = \\arg \\min_j ||\\mathbf x_i - \\mathbf \\mu_j||^2\\ 0, \u0026amp; \\text{otherwise. } \\end{cases}$$\nStep 2 In the second step we move each cluster center to the average of the data points assigned to that cluster. Mathematically this is:\n$$ \\mathbf \\mu_k = \\frac{\\sum_i r_{ik} \\mathbf x_i}{\\sum_i r_ik}$$\nIteration 1, Step-2: Moving the cluster centers to the mean of the data points assigned to each cluster\nThe following figures show subsequent steps until its becoming clear that the algorithm will convergence.\nIteration 2, Step-1: Assignment of data points to cluster centers\nIteration 2, Step-2: Moving the cluster centers to the mean of the data points assigned to each cluster\nThe algorithm will converge as shown below:\nBlue circles (Step 1), interleaved with red circles (step 2) towardds k-means alg convergence\n"});index.add({'id':12,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/','title':"Support Vector Machines",'content':"Support Vector Machines In the development of the concept of kernels, we mentioned that these can be used to derive non-linear decision boundaries. What we haven\u0026rsquo;t addressed, is how good these can be - for example in separable datasets there can be many (or infinite) number of boundaries that separate the two classes but we need a metric to gauge the quality of separation. This metric is called the margin and will be geometrically explained next.\nClassification Margin Intuitively margin is the distance between the classification boundary and the closest data point(s) of the two classes, as shown below:\nClassification Margin\nAssuming a linear classifier of $g(\\mathbf x) = \\mathbf w^T \\phi(\\mathbf x) + b$ and a separable dataset for the moment, the maximization of the margin leads to a decision boundary, $y(\\mathbf x)=0$ that depends only on a subset of data that are calling support vectors as shown below for the specific dataset.\nSupport Vectors\nJust like in the perceptron case, since\n$$y( \\mathbf x_i ) = \\begin{cases}\u0026gt;0 \u0026amp; \\text{if} \\ y_i = +1, \\\\ \u0026lt;0 \u0026amp; \\text{if} \\ y_i = -1 \\end{cases}$$\nfor all training data we have,\n$$y_i(\\mathbf w^T x_i + b) \u0026gt; 0$$\nSo we are after a $\\mathbf w \\in \\mathbb{R}^n$ that satisfies this constraint. Before we proceed, let us define geometrically what the margin looks like. Let us define two parallel lines on either side of the decision boundary as shown in the above figure,\n$\\mathbf w^T \\phi(\\mathbf x) + b = 1$\n$\\mathbf w^T \\phi(\\mathbf x) + b = -1$\nLet us assume now that there is a data point that is very close to the decision boundary and just marginally satisfied the inequality $y_i(\\mathbf w^T x_i + b) \u0026gt; 0$ for example $y_i(\\mathbf w^T x_k + b) = 0.2$. If we scale both sides we can reach an equivalent condition that is appealing mathematically $y_i(\\mathbf w^T x_i + b) \u0026gt; 1$ - equivalent as it will result in the same $\\mathbf w$. In addition to this scaling, it is very useful to normalize $\\mathbf w$ as\n$\\hat \\mathbf w = \\mathbf w / ||\\mathbf w||$\nFor a point $\\mathbf z$ that is in the margin defining line $\\mathbf w^T \\phi(\\mathbf x) + b = 1$ we can calculate given a margin distance $\\gamma$ the point it projects to the decision boundary line which is defined by the normalized normal vector $\\mathbf w$ as, $\\mathbf z - \\gamma \\hat \\mathbf w$. Then we can write two equations that satisfy these two points:\n$\\mathbf w^T \\phi(\\mathbf z) + b = 1$\n$\\mathbf w^T \\phi(\\mathbf z - \\gamma \\hat \\mathbf w) + b = 1$\ngiven that we are dealing with linear decision boundary in this specific problem, we can eliminate $\\phi$ as there is no transformation involved and subtract the two equations. Solving for the margin $\\gamma$ we obtain,\n$\\gamma = \\frac{1}{||w||}$\nOptimization problem statement We are now ready to write the optimization problem that will maximize $\\gamma$ or minimize $||\\mathbf w||$ or equivalently minimize a monotonic function of $||\\mathbf w||$.\n$\\min \\frac{1}{2}||w||^2$\n$\\ \\text{s.t.} \\ y_i\\mathbf w^T x_i + b \u0026gt; 1$\nThis is a convex optimization problem (we have linear constraints and convex objective function) and therefore there are efficient ways to solve it. In practice, ML frameworks have built in solvers that can provide the $\\mathbf w$ given a dataset. Understanding the solution is instructive but it requires background in convex optimization theory and the concept of Langrange multipliers. For this reason, the math in the \u0026ldquo;Under the hood\u0026rdquo; section of chapter 5 of Geron\u0026rsquo;s book will not be in the midterm or final exams but the intuition behind maximizing the margin is in the scope of the final exam.\nTo provide an intuition of how the kernels and the SVM classifiers are connected, it suffices to write down the dual form of the optimization problem and surely enough in this form the solution involves dot products in the data space. In general, any SVM problem can be kernelized if we expand the data space using a suitable kernel which means that SVM is able via the kernel trick to find efficiently max margin decision boundaries of any shape.\nFor non separable datasets the optimization problem statement can be written using slack variables $\\xi_i$ that relax the constraints and therefore result into $\\mathbf w$ and $b$ that tolerate some classification errors for good generalization ability.\n$\\min \\frac{1}{2}||w||^2 + C \\sum_i \\xi_i$\n$\\ \\text{s.t.} \\ y_i\\mathbf w^T x_i + b \\ge 1 - \\xi_i$\n$ \\ \\ \\ \\ \\ \\ \\ \\xi_i \\ge 0$\nThe slack variable defining how much on the wrong side the 𝑖th training example is. If $\\xi =0$, the point was classified correctly and by enough of a margin; if it\u0026rsquo;s between 0 and 1, the point was classified correctly but by less of a margin than the SVM wanted; if it\u0026rsquo;s more than 1, the point was classified incorrectly. A geometrical view of the above is useful.\nOnce again the ML frameworks that you will work with, provide in their documentation explicit mention about variables of the optimization problem statement above (such as $C$). However as $C$ must be explicitly set by the data scientist, or optimized using hyperparameter optimization techniques, it is instructive to comment that $C$ is equivalent to the $1/\\lambda$ regularization parameter we met in the very beginning of these lectures where we penalized the weights of linear regression to avoid overfitting. $C$ as you can see from the optimization problem objective function if set too high, even a slightest ammount of slack will greatly penalize (add) to the objective and therefore will drive the decision boundary towards \u0026ldquo;hard\u0026rdquo; SVM decisions. This is not good news as the SVM must in many practical cases avoid over-sensitivity to outliers - too large $C$ can lead to driving the decision boundary towards outliers significantly increasing the test (generalization) error. If $C$ is set to $C=0$ then any ammount of slack will be tolerated and then the decision boundary may result into too many misclassifications.\n"});index.add({'id':13,'href':'/cs-gy-6613-spring-2020/docs/syllabus/','title':"Syllabus",'content':"Syllabus The course schedule below highlights our journey to understand the multiple subsystems and how they can be connected together to create compelling but, currently, domain specific forms of intelligence.\nBooks Artificial Intelligence: A Modern Approach, by Stuart Russell, 3rd edition, 2010 and also here.\nThe publisher is about to release the 4th edition (2020) of this classic. We will be monitoring availability in bookstores but it does not seem likely this edition to appear on time for the Spring 2020 class.\nOther recommended texts are: (a) DRL: \u0026ldquo;Foundations of Deep Reinforcement Learning\u0026rdquo;, by Graesser \u0026amp; Keng, 2020. (b) GERON: \u0026ldquo;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\u0026rdquo;, 2nd Edition, by Geron, 2019. (c) DL: https://www.deeplearningbook.org/ (free)\nSchedule The schedule is based on Academic Calendar Spring 2020:\nPart I: Perception and Machine Learning   Lecture 1 (1/27/2020) We start with an introduction to AI and present a systems approach towards it. We develop a map that will guide us through the rest of the course as we deep dive into each component embedded into AI agents. Reading: AIMA Chapters 1 \u0026amp; 2.\n  Lecture 2 (2/3/2020) The perception subsystem is the first stage of many AI systems including our brain. Its function is to process and fuse multimodal sensory inputs. Perception is implemented via a number of reflexive agents that map directly perceived state to an primitive action such as regressing on the frame coordinates of an object in the scene. We present the supervised learning problem both for classification and regression, starting with classical ML algorithms. Reading: AIMA Chapter 18.\n  Lecture 3 (2/10/2020) We expand into Deep neural networks. DNNs are developed bottom up from the Perceptron algorithm. MLPs learn via optimization approaches such as Stochastic Gradient Descent. We deep-dive into back-propagation - a fundamental algorithm that efficiently trains DNNs. Reading: DL Chapter 6\n  3/16/2020 Enjoy President\u0026rsquo;s Day holiday.\n Lecture 4: (2/24/2020) We dive into the most dominant DNN architecture today - Convolutional Neural Networks (CNNs) and Recursive Neural Networks (RNNs). Reading: DL Chapter 9 \u0026amp; 10.\n  Lecture 5: (3/2/2020) When agents move in the environment they need to abilities such as scene understanding. We will go through few key perception building blocks such as Object Detection, Semantic and Instance Segmentation. Some of these building blocks (autoencoders) are instructive examples of representations learning that will be shown to be an essential tool in the construction of environment state representations. Reading: Various papers\n  Part II: Reasoning and Planning Lecture 6: (3/9/2020) In this lecture we introduce probabilistic models that process the outputs of perception (measurement / sensor model) and the state transitions and understand how the agent will track / update its belief state over time. This is a achieved with probabilistic recursive state estimation algorithms and dynamic bayesian networks. Reading: AIMA Chapters 14 \u0026amp; 15.  3/16/2020 Enjoy your Spring Break.\n3/23/2020 - This is your Midterm Test (2h)\nLecture 7: (3/30/2020) After the last lecture, the agent has a clear view of the environment state such as what and where the objects that surround it are, its able to track them as they potentially move. It needs to plan the best sequence of actions to reach its goal state and the approach we take here is that of problem solving. In fact planning and problem solving are inherently connected as concepts. If the goal state is feasible then the problem to solve becomes that of search. For instructive purposes we start from simple environmental conditions that are fully observed, known and deterministic. This is where the A* algorithm comes in. We then relax some of the assumptions and treat environments that are deterministic but the agent takes stochastic actions or when both the environment and agent actions are stochastic. Reading: AIMA Chapters 3 \u0026amp; 4. We also investigate what happens when we do not just care about reaching our goal state, but when we, in addition, need to do so with optimality. Optimal planning under uncertainty is perhaps the cornerstone application today in robotics and other fields. Readings: AIMA Chapters 10 \u0026amp; 11 and selected papers.  Part III: Deep Reinforcement Learning  Lecture 8: (4/6/2020) We now make a considerable extension to our assumptions: the utility of the agent now depends on a sequence of decisions and, further, the stochastic environment offers a feedback signal to the agent called reward. We review how the agent\u0026rsquo;s policy, the sequence of actions, can be calculated when it fully observes its current state (MDP) and also when it can only partially do so (POMDP). The algorithms that learn optimal policies in such settings are known as Reinforcement Learning (RL). Today they are enhanced with the Deep Neural Networks that we met in Part I, to significantly improve the expected reward since DNNs are excellent approximators to the various functions embedded in such problems. We conclude with the basic taxonomy of the algorithm space for DRL Problems. In this course we are focusing on model free methods that have general applicability. Readings: AIMA Chapter 16 \u0026amp; 17, DRL Chapter 1. This lectured will be delivered in person by Gurudutt Hossangadi as I will be out of town.\n  Lecture 9: (4/13/2020) In this lecture we start on the exploration of the various algorithms that do not depend on learning any model of the environment dynamics. The first algorithm is the policy-based algorithm called REINFORCE and its extensions especially the Advantage Actor Critic (A2C). DRL Chapter 2, 6 and 7.\n  Lecture 10: (4/20/2020) Staying in the setting of model-free algorithms we will work with the so-called value-based methods and the State Action Reward State Action (SARSA) algorithms. This is the ancestor of algorithms such as DQN, DDQN with Prioritized Experience Replay (PER) that will also be covered. Readings: DRL Chapter 3, 4 and 5.\n  Part IV: Knowledge Bases and Communication  Lecture 11: (4/27/2019) Every intelligent agent needs to know how the world works for each task it encounters. These facts are stored in its Knowledge Base also known as Knowledge Graph. In addition as the agent affects the environment it must be able to create the right representations using its perception systems and update the knowledge base with dynamic content. Finally it needs to draw conclusions - aka infer new facts from existing ones - that will help the task at hand. Readings: AIMA Chapter 12 and selected papers.\n  Lecture 12: (5/05/2020) We are all familiar that natural language is the prime means of communication between humans to collaboratively complete successfully tasks or simply share our knowledge bases. How can we achieve the same objectives when we enable communicate with intelligent agents. Is natural language the universal language that together with imitation is the missing link in our ability to (re)task robots from intelligent assistants to cognitive collaborative robots in our factories of the future? Readings: AIMA Chapter 23 and selected papers.\n  Lecture 13: (5/11/2020) In this last lecture, we review the main points of what we learned and emphasize what kind of questions you are expected to answer in the final exam. Final projects are due 5/10/2020 11:59pm.\n  Lecture 14: (5/18/2020) Good luck with your final test.\n  "});index.add({'id':14,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/mnist/','title':"MNIST Classification - SVM Case Study",'content':"MNIST Classification - SVM Case Study  "});index.add({'id':15,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/svm/iris/','title':"Iris Classification - SVM Case Study",'content':"Iris Classification - SVM Case Study  "});index.add({'id':16,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/','title':"Lecture 1 - Introduction to AI",'content':""});index.add({'id':17,'href':'/cs-gy-6613-spring-2020/docs/lectures/learning-problem/','title':"Lecture 2a - The Learning Problem",'content':"The Learning Problem We have seen various agent designs but the ones that we will concentrate on this course are ones that can form partially observed (PO) environment states using various sensing architectures. The perception block we have seen in the case where the agent is an autonomous car achieves that for example, and perception is rich in what is called Machine Learning (ML). ML is a substantial subset of AI today and is not limited to just the perception part of an agent.\nAlmost all machine learning algorithms depend heavily on the representation of the data they are given. Each piece of, relevant to the problem, information that is included in the representation is known as a feature. Today\u0026rsquo;s ML approaches such as deep learning actually learn the most suitable representations for the task at hand (still with a some help from experts) - an example is shown in the picture below.\nHierarchical Feature Learning\nThe Supervised Learning Problem Statement Let us start with a classic formal definition of the supervised learning problem.\nVapnik\u0026rsquo;s formulation of the learning problem (enhanced with notation from the Deep Learning book)\nThe description below is taken from Vadimir Vapnik\u0026rsquo;s classic book Statistical Learing Theory, albeit with some enhancements on terminology to make it more in line with this book.\nThe generator is a source of situations that determines the environment in which the target function (he calls it supervisor) and the learning algorithm act. Here we consider the simplest environment: the data generator generates the vectors $\\mathbf{x} \\in \\mathcal{X}$ independently and identically distributed (i.i.d.) according to some unknown (but fixed) probability distribution function $p(x)$.\nThese vectors are inputs to the target function (or operator); the target operator returns the output values $\\mathbf{y}$. The target operator which transforms the vectors $\\mathbf{x}$ into values y, is unknown but we know that it exists and does not change.\nThe learning algorithm observes the training dataset,\n$${ (\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_m, y_m) }$$\nwhich contain input vectors $\\mathbf{x}$ and the target response $\\mathbf{y}$. During this period, the learning algorithm constructs some operator which will he used for prediction of the supervisor\u0026rsquo;s answer $y_i$ on any specific vector $\\mathbf{x}_i$ generated by the generator. The goal of the learning algorithm is to construct an appropriate approximation of the target function - we will call this a hypothesis. The hypothesis can be iteratively constructed so the final hypothesis is the one that is used to produce the label $\\hat{y}$.\nTo be a mathematically correct, this general scheme of learning from examples needs some clarification. First of all, we have to describe what kind of operators are used by the target function. In this book. we suppose that the target function returns the output $\\mathbf{y}$ on the vector $\\mathbf{x}$ according to a conditional distribution function $p(\\mathbf{y} | \\mathbf{x})$ (this includes the case when the supervisor uses some function $\\mathbf{y} = f(\\mathbf{x}))$.\nThe learning algorithm observes the training set which is drawn randomly and independently according to a joint distribution function $p(\\mathbf{x} , \\mathbf{y}) = p(\\mathbf{x}) p(\\mathbf{y} | \\mathbf{x})$. Recall that we do not know this function but we do know that it exists. Using this training set, the learning algorithm constructs an approximation to the unknown function. The ability to correctly predict / classify when observing the test set, is called generalization.\nA couple of examples of supervised learning are shown below:\nExamples from the MNIST training dataset used for classification\nBirdseye view of home prices - Zillow predicts prices for similar homes in the same market. This is a regression problem.\nUnsupervised Learning In unsupervised learning, we present a training set ${ \\mathbf{x}_1, \\dots, \\mathbf{x}_m }$ without labels. The most common unsupervised learning method is clustering. We construct a partition of the data into a number of $K$ clusters, such that a suitably chosen loss function is minimized for a different set of input data (test).\nClustering showing two classes and the exemplars per class\nSemi-supervised Learning and Active Learning Semi-supervised learning stands between the supervised and unsupervised methods. One of the hottest methods in this category is the so called Active learning. In many practical settings we simply cannot afford to label /annotate all $\\mathbf x$ for very large $m$, and we need to select the ones that greedily result into the biggest performance metric gain (e.g. accuracy).\nReinforcement Learning In reinforcement learning, in this course we will treat RL in detail later, a teacher is not providing a label (as in supervised learning) but rather a reward that judges whether the agent\u0026rsquo;s action results on favorable environment states. In reinforcement learning we can learn end to end optimal mappings from perceptions to actions.\n"});index.add({'id':18,'href':'/cs-gy-6613-spring-2020/docs/lectures/regression/','title':"Lecture 2b - Regression",'content':""});index.add({'id':19,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/','title':"Lecture 2c - Linear Classification",'content':"Linear Classification This section captures the main principles of linear classification which is a fundamental ability for the perception system of an AI agent.\nClassification Principle There are three broad classes of methods for determining the parameters $\\mathbf{w}$ of a linear classifier:\n  Discriminative Models, which form a discriminant function that maps directly test data $\\mathbf{x}$ to classes $\\mathcal{C}_k$. In this case, probabilities play no role. Examples include the Perceptron and Support Vector Machines (SVMs).\n  Probabilistic Discrimitative Models, First solve the inference problem of determining the posterior class probabilities $p(\\mathcal{C}_k|\\mathbf{x})$ and then subsequently assign each new $\\mathbf{x}$ to one of the classes. Approaches that model the posterior probabilities directly are called discriminative models. Examples of discriminative training of linear classifiers include:\n Logistic regression—maximum likelihood estimation of $\\mathbf{w}$ assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.    Probabilistic Generative Models, which infer the posterior $p(\\mathcal{C}_k|\\mathbf{x})$ using Bayessian approach and we therefore generate the class-conditional density $p(\\mathbf{x}|\\mathcal{C}_k)$ and the prior $p(\\mathcal{C}_k)$. Examples of such algorithms include:\n Linear Discriminant Analysis (or Fisher\u0026rsquo;s linear discriminant) (LDA)—assumes Gaussian conditional density models Naive Bayes classifier with multinomial or multivariate Bernoulli event models.    Discriminative Models Synthetic Dataset with multiple discriminant functions\n  The learning algorithm is asked to assign the input vector $\\mathbf{x} \\in \\mathbb{R}^n$ to one of the $k$ classes. The learning algorithm will need to produce a function $\\mathbf{y}=g(\\mathbf{x})$ also called discriminant function.\n  In binary classification, the target variable (label) $y$ belongs to either of the two classes ${\\mathcal{C}_1, \\mathcal{C}_2}$ - for multi-class it is taking a value out of a finite set of classes. Contrast this to regression where $y$ is taking any value out of a infinite possible set of values ($y \\in \\mathbb{R}$). For example, we can choose convenient labels $y=0$ for class $\\mathcal{C}_1$, and $y=1$ for class $\\mathcal{C}_2$.\n  The Bayesian setting is now very handy, $ p(\\mathcal{C}_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{p(\\mathbf{x})}$ where $p(\\mathcal{C}_k)$ is the prior probability for the corresponding class. The equation above is similar to what we have seen in regression, and can be used to update the posterior probability $p(\\mathcal{C}_k|\\mathbf{x})$ given the likelihood function, prior and evidence. As an example, we look at the CT-scan of a patient and obtain the posterior probability based on the formula above and can now diagnose the patient as a cancer free if $p(\\mathcal{C}_0|\\mathbf{x}) \u0026gt; p(\\mathcal{C}_1|\\mathbf{x})$ where $\\mathcal{C}_0$ is the cancer free class.\n  Joint probabilities involved in binary classification. By sweeping the discrimination function (which in this example is the threshold $\\hat{x}$) left or right we are adjusting the areas shown as red blue and green that are the main determinants of classifier performance.\nReceiver Operating Curve and Classification Metrics Obviously the criticality of estimating the right posterior for the treatment of the patient is very high - we have therefore developed metrics that gauge such detection. It is extremely important to understand the Receiver Operating Curve that gives raise to many classification quality metrics. Where the name comes from? The top of this page is decorated by a photo of one of the hundreds of RADAR towers installed in England just before the WWII. The job of the RADAR was to detect incoming Nazi air bombings. RADAR was one of the key technologies that won the war (the other was cryptography).\nLet us consider a two-class prediction problem (binary classification), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p.\nTo get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:\nJoint probabilities and ROC Curve (Wikipedia)\nConfusion Matrix (Wikipedia)\nFor an instructional example of determining the confusion matrix of classification models using scikit-learn see the Iris case study.\n"});index.add({'id':20,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/','title':"Lecture 3 - Deep Neural Networks",'content':"Deep Neural Networks DNNs are the implementation of connectionism, the philosophy that calls for algorithms that perform function approximations to be constructed by an interconnection of elementary circuits called neurons. There are two main benefits that DNNs brought to the table, on top of their superior perfomance in large datasets.\nAutomated Feature Construction (Representations) Given a synthetic dataset like this, there is a thought process that a data scientist must undergo to transform the input into a suitable representation e.g. $(x,y)=(r\\cos\\theta,r\\sin\\theta)$ for a linear classifier to be able to provide a solution.\nThe DNN (even as small as a single neuron) will find such representation automatically.\nHierarchical representations On top of the automated feature construction advantage, that even shallow networks can provide, features can be represented hierarchically as shown below.\nIn the next sections, we provides some key points on the following questions:\n What DNNs look like How they work (backpropagation)  We will use a combination of material from Ian Goodfellow\u0026rsquo;s book chapter 6 and CS331n. Another excellent resource is Nielsen\u0026rsquo;s tutorial treatment of the subject.\nNOTE: You can use this to depict several DNN architectures - the fully connected ones can be used to draw rudimentary NNs and pencil them with back propagation equations.\n Architecture Feedforward networks consist of elementary units that resemble the perceptron. These units are stacked up into layers.\nThere are multiple layers:\n The input layer One or more hidden layers The output layer  A typical DNN consists a trivial placeholder layer that feeds the network with input data $\\mathbf x$ via an input layer. One or more hidden layers that that employ one ore more activation functions and output layer that usually takes the shape for classification problems of a softmax function.\nActivation Functions There are several possible but we limit the discussion just three here.\n  The perceptron activation function which we have seen here:\n$$g(a) = \\begin{cases}1.0 \u0026amp;\\text{if } a \\geq 0 \\\\ -1.0 \u0026amp;\\text{if } a \u0026lt; 0 \\end{cases}$$\n  The sigmoid activation function that we have also seen during logistic regression.\n$$g(a) = \\sigma(a) = \\frac{1}{1+e^{-a}} \\hspace{0.3in} \\sigma(a) \\in (0,1)$$\nTowards either end of the sigmoid function, the $\\sigma(a)$ values tend to respond much less to changes in a vanishing gradients. The neuron refuses to learn further or is drastically slow.\n  The Rectified Linear Unit activation function - very popular in Deep Learning.\nThe RELU is very inexpensive to compute compared to sigmoid and it offers the following benefit that has to do with sparsity: Imagine an MLP with random initialized weights to zero mean ( or normalised ). Almost 50% of the network yields 0 activation because of the characteristic of RELU. This means a fewer neurons are firing (sparse activation) making the the network lighter and more efficient. On the other hand for negative $a$, the gradient can go towards 0 and the weights will not get adjusted during descent.\n  Softmax Output units The softmax output unit is a generalization of the sigmoid for problems with more than two classes.\n$$\\text{softmax}(\\mathbf z)_i = \\arg \\max_i \\frac{\\exp (z_i)}{\\sum_i \\exp(z_i)}$$\nwhere $i$ is over the number of inputs of the softmax function.\nFrom a neuroscientiﬁc point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it: the softmax outputs always sum to 1 so an increase in the value of one unit necessarily corresponds to a decrease in the value of others. This is analogous to the lateral inhibition that is believed to exist between nearby neurons in the cortex. At the extreme (when the diﬀerence between the maximal and the others is large in magnitude) it becomes a form of winner-take-all(one of the outputs is nearly 1, and the others are nearly 0).\n"});index.add({'id':21,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-intro/','title':"Introduction to Backpropagation",'content':"Introduction to Backpropagation The backpropagation algorithm brought back from the winter neural networks as it made feasible to train very deep architectures by dramatically improving the efficiency of calculating the gradient of the loss with respect to all the network parameters.\nIn this section we will go over the calculation of gradient using an example function and its associated computational graph. The example does not have anything to do with DNNs but that is exactly the point. The essence of backpropagation was known far earlier than its application in DNN.\nFor students that need a refresher on derivatives please go through Khan Academy\u0026rsquo;s lessons on partial derivatives and gradients.\nCalculating the Gradient of a Function Our goal is to compute the components of the gradient of the function $\\nabla f = [ \\partial f / \\partial x , \\partial f / \\partial y ]^T$ where,\n$$f(x, y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$$\nThe computational graph of this function is shown below. Its instructive to print this graph and pencil in all calculations for both this example and others in the backpropagation section. You may need to review derivative tables from Calculus e.g. http://cs231n.stanford.edu/handouts/derivatives.pdf\nForward Pass In the forward pass, the algorithm works bottom up (or left to right depending how the computational graph is represented) and calculates the values of all \u0026ldquo;gates\u0026rdquo; (gates are the elementary functions that synthesize the function) of the graph and stores their values into variables as they will be used by the backwards pass. There are eight values stored in this specific example.\n1 2 3 4 5 6 7 8 9 10  sigy = 1.0 / (1 + exp(-y)) #(1) num = x + sigy # numerator #(2) sigx = 1.0 / (1 + exp(-x)) #(3) xpy = x + y #(4) xpysqr = xpy**2 #(5) den = sigx + xpysqr # denominator #(6) invden = 1.0 / den #(7) f = num * invden #(8)   Backwards Pass In the backwards pass, we reverse direction and start at the top or rightmost node (the stored variables) of the graph and compute the input (in the reverse direction) derivative of input of each gate using the template depicted below:\nBackpropagation template\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # gradient at the top df = 1.0 # backprop f = num * invden dnum = 1.0 * invden # gradient on numerator #(8) dinvden = 1.0 * num #(8) # backprop invden = 1.0 / den  dden = (-1.0 / (den**2)) * dinvden #(7) # backprop den = sigx + xpysqr dsigx = (1) * dden #(6) dxpysqr = (1) * dden #(6) # backprop xpysqr = xpy**2 dxpy = (2 * xpy) * dxpysqr #(5) # backprop xpy = x + y dx = (1) * dxpy #(4) dy = (1) * dxpy #(4) # backprop sigx = 1.0 / (1 + exp(-x)) dx += ((1 - sigx) * sigx) * dsigx # ATTENTION !! # backprop num = x + sigy dx += (1) * dnum #(2) dsigy = (1) * dnum #(2) # backprop sigy = 1.0 / (1 + exp(-y)) dy += ((1 - sigy) * sigy) * dsigy #(1)   As the previous example indicated, the essence of backpropagation algorithm is that local gradients can be calculated symbolically (using calculus) as they only depend on the simple gate structures and these gradients act as valves in the gradient flow that starts from the top of the network (the value of the flow there is always 1.0) and propagates to the bottom of the graph. Think about it as valves controlling the water flowing down a river delta. Individual flows may also merge like in the example above.\nThis example allowed us to understand how the gates control the flow. We have met add, split, multiply and non-linear gates.\nThe add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged).\nThe multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly non intuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^Tx_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples $x_i$ by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.\nIn neural networks we will also meet the max gate (ReLU) and its instructive to outline here its effect on the flow.\nThe max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values.\n"});index.add({'id':22,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-dnn/','title':"Backpropagation in Deep Neural Networks",'content':"Backpropagation in Deep Neural Networks Following the introductory section, we have persuaded ourselves that backpropagation is a procedure that involves the repetitive application of the chain rule, let us look more specifically its application to neural networks and the gates that we usually meet there. In DNNs we are dealing with vectors, matrices and in general tensors and therefore its required to review first how we can expand on the template above for these data types.\nDNN Gates In the following we heavily borrow from this text. The basic building block of vectorized gradients is the Jacobian Matrix. In the introductory section we dealt with functions $f: \\mathbb{R}^2 \\to \\mathbb{R}$. Suppose that we have a more complicated function $\\bm f: \\mathbb{R}^n \\to \\mathbb{R}^m$ that maps a vector of length $n$ to a vector of length $m$:\n$$\\bm f(\\bm x) = [f_1(x_1, \u0026hellip;, x_n), f_2(x_1, \u0026hellip;, x_n), \u0026hellip;, f_m(x_1, \u0026hellip;, x_n)]$$.\nThen its Jacobian is:\n$$\\frac{\\partial \\bm f}{\\partial \\bm x} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\dots \u0026amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots \u0026amp; \\dots \u0026amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\dots \u0026amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$$\nThe Jacobian matrix will be useful for us because we can apply the chain rule to a vector-valued function just by multiplying Jacobians.\nAs a little illustration of this, suppose we have a function $f(\\mathbf x) = [f_1(x), f_2(x)]$ taking a scalar to a vector of size 2 and a function $g(\\mathbf y) = [g_1(y_1, y_2), g_2(y_1, y_2)]$ taking a vector of size two to a vector of size two. Now let\u0026rsquo;s compose them to get $g(x) = [g_1(f_1(x), f_2(x)), g_2(f_1(x), f_2(x))]$. Using the regular chain rule, we can compute the derivative of $g$ as the Jacobian\n$$\\frac{\\partial g}{\\partial x} = \\begin{bmatrix} \\frac{\\partial}{\\partial x}g_1(f_1(x), f_2(x)) \\\\ \\frac{\\partial} {\\partial x}g_2(f_1(x), f_2(x)) \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial{g_1}}{\\partial f_1}\\frac{\\partial{f_1}}{\\partial x} + \\frac{\\partial{g_1}}{\\partial f_2}\\frac{\\partial{f_2}}{\\partial x} \\\\ \\frac{\\partial{g_2}}{\\partial f_1}\\frac{\\partial{f_1}}{\\partial x} + \\frac{\\partial{g_2}}{\\partial f_2}\\frac{\\partial{f_2}}{\\partial x} \\end{bmatrix}$$\nAnd we see this is the same as multiplying the two Jacobians:\n$$\\frac{\\partial{g}}{\\partial x} = \\frac{\\partial{ g}}{\\partial f}\\frac{\\partial{f}}{\\partial x} = \\begin{bmatrix} \\frac{\\partial{g_1}}{\\partial f_1} \u0026amp; \\frac{\\partial{g_1}}{\\partial f_2} \\\\ \\frac{\\partial{g_2}}{\\partial f_1} \u0026amp; \\frac{\\partial{g_2}}{\\partial f_2} \\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial{f_1}}{\\partial x} \\\\ \\frac{\\partial{f_2}}{\\partial x} \\\\ \\end{bmatrix}$$\nThis is also another instructive summary that help us understand how to calculate the local gradients involved and the gate templates (identities) summarized below that are routinely found in neural network backpropagation calculations. Assume that with $\\mathbf W \\in \\mathbb{R}^{n \\times m}, \\mathbf x \\in \\mathbb{R}^m$.\nTables of Gates and Gradients used in the backpropagation of deep neural networks\n   Gate Solution     $\\mathbf z = \\mathbf W \\mathbf x$ $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\mathbf W$   $\\mathbf z = \\mathbf x \\mathbf W$ $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\mathbf W^T$   $\\mathbf z = \\mathbf x$ $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\mathbf I$   $\\mathbf z = f(\\mathbf x)$ element-wise $\\frac{\\partial \\mathbf z}{\\partial \\mathbf x} = \\text{Diag}[ f\u0026rsquo;(\\mathbf x) ]$   $\\mathbf z = \\mathbf W \\mathbf x$, $\\mathbf \\delta = \\frac{\\partial L}{\\partial \\mathbf z}$ $\\frac{\\partial L}{\\partial \\mathbf W} = \\mathbf \\delta^T \\mathbf x^T$   $\\mathbf z = \\mathbf x \\mathbf W$, $\\mathbf \\delta = \\frac{\\partial L}{\\partial \\mathbf z}$ $\\frac{\\partial L}{\\partial \\mathbf W} = \\mathbf x^T \\mathbf \\delta$   $\\mathbf z = \\mathbf W \\mathbf x$, $\\hat \\mathbf y = \\mathtt{softmax}(\\mathbf z)$, $L=CE(\\mathbf y , \\hat \\mathbf y )$ $\\frac{\\partial L}{\\partial \\mathbf z} = \\hat \\mathbf y - \\mathbf y$    During the lecture we will go through an NN example on the whiteboard that will use these gate gradients for the estimation of the gradient of the loss with respect to its parameters using backpropagation.\nBackprop behavior during training As documented here and in Geron\u0026rsquo;s textbook, you also need to be watchful of the effects of the various non-linear gates on the gradient flow.\nFor sigmoid gate, if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, $z*(1-z)$, which is **local** gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.\nFor ReLU gates, if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead.\nTensorflow can create a computational graph from the DNN model specification (python). These graphs can be visualized on the web UI with Tensorboard. Use the playground when you first learn about DNNs to understand the principles but dive into the Fashion MNIST using Tensorflow use case to understand the Tensorflow mechanics and how to debug Tensorflow python scripts both syntactically and logically. Logical debugging should happen using Tensorboard visualizations. Similarly with Pytorch if this is your choice.\n  "});index.add({'id':23,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/regularization/','title':"Regularization in Deep Neural Networks",'content':""});index.add({'id':24,'href':'/cs-gy-6613-spring-2020/docs/lectures/dnn/fashion-mnist-case-study/','title':"Fashion MNIST Case Study",'content':"Fashion MNIST Case Study This notebook is a case study on MNIST Fashion dataset that due to the almost perfect classification of MNIST is now the new point of reference dataset to learn and try ML algorithms on.\nClick on the button \u0026ldquo;Run in Google Colab\u0026rdquo; to run the notebook and step through the code.\n"});index.add({'id':25,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/','title':"Lecture 4a - Convolutional Neural Networks",'content':""});index.add({'id':26,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-intro/','title':"Introduction to Convolutional Neural Networks",'content':"Introduction to Convolutional Neural Networks As elaborated here, humans build up a more schematic version of the environment across eye fixations than was previously thought. This schematic version of the environment is typically known as scene gist. It contains conceptual information about the scene’s basic category – is it natural, human-made, a cityscape? – and general layout, maybe limited to a few objects and/or features. This schematic version of the environment is a far cry from the “picture in the head” scenario. But it’s this schematic information that guides us from one eye fixation to the next, during which more detailed information can be sampled. In the picture above, the brain will first detect the cityscape schematic and then process one of the scene fixations - e.g. the yellow cab.\nAs we will see shortly images-based datasets are almost exclusively used for instructive purposes in CNNs. The reason is simple. With images we are able to \u0026ldquo;see\u0026rdquo; the effects of a number of important algorithms that are related to classification, object detection etc.\nFocusing at the \u0026ldquo;fixation\u0026rdquo; stage of human vision, CNNs are biologically inspired from the structure that the neuroscientists David Hubel and Torsten Wiesel saw in the so called V1 region of the brain - the region at the back of our head that is responsible for the processing of visual sensory input signals coming from the eye\u0026rsquo;s retina.\nColor images as functions A grayscale picture can be seen as a function $f(x,y)$ of light intensities, where x and y are the rows and columns of pixels in the image.\nUsually we have border constraints in the range of the input pixels e.g. $x \\in [a,b], y \\in [c,d]$ but also in the output intensity values (typically 8-bit encoding is assumed that limits the values to [0, 255]).\nThe city scape color image above can also be seen as a vector function:\n$$f(x,y)= \\begin{bmatrix} r(x,y) \\ g(x,y) \\ b(x,y) \\end{bmatrix}$$\nwith its elements capturing the channels Red, Green and Blue functions, the mixture (superposition) of which can generate the pixel color of the original image.\nPlease note the convention to refer to the columns $j$ of the matrix that represents each function with $x$ and to the rows $i$ with $y$. This may cause some confusion at first.\nThe Convolution \u0026amp; Cross-Correlation Operation The key operation performed in CNN layers is that of 2D convolution. In fact in practice they are 4D convolutions as we try to learn many filters and we also consider many input images (mini-batch) in the iteration of our SGD optimizer.\nWe cover first the 1-dimensional case.\n1D Convolution By definition the convolution between two functions in one dimension is given by:\n$$(a * b)[n] = \\sum_{m=-\\inf}^{\\inf} a[m]b[n-m]$$\n1D-convolution example. The operation effectively slides the flipped version of $b$ across $a$\nThe result can be calculated as follows:\n$c[0] = \\sum_m a[m]b[-m] = 0 * 0.25 + 0 * 0.5 + 1 * 1 + 0.5 * 0 + 1 * 0 + 1 * 0 = 1$\n$c[1] = \\sum_m a[m]b[1-m] = 0 * 0.25 + 1 * 0.5 + 0.5 * 1 + 1 * 0 + 1 * 0 = 1$\n$c[2] = \\sum_m a[m]b[2-m] = 1 * 0.25 + 0.5 * 0.5 + 1 * 1 + 1 * 0 + 1 * 0 = 1.5$\n$c[3] = \\sum_m a[m]b[3-m] = 1 * 0 + 0.5 * 0.25 + 1 * 0.5 + 1 * 1 = 1.625$\nand so on.\nConvolution and cross-correlation are very close - see this 1D example (source to persuade yourself that this is the case.\nConvolution and cross-correlation are very similar quantities\n2D Convolution In 2D the same principle applies. First, the convolution operation in some frameworks is the flipped version - this is perfectly fine as the convolution operation is commutative.\n$$S(i,j) = \\sum_m \\sum_n x(m, n) h(i-m,j-n) = \\sum_m \\sum_n x(i-m, j-n)h(m,n)$$\nwhere $x$ is the input of the convolution and $h$ is the kernel or filter typically of smaller spatial dimensions.\nMany ML frameworks don\u0026rsquo;t even implemented convolution but they do the very similar cross-correlation operation and they sometimes call it convolution to add to the confusion. Tensorflow implements the cross-correlation operation under the hood.\n$$S(i,j) = \\sum_u \\sum_v x(i+u, j+v)h(u,v)$$\nIf you learn the flipped version of the kernel or not is irrelevant to the task of predicting the right label. Therefore you should not be concerned with the framework implementation details, the thing that is important for you to grasp is the essence of the operation. Lets look some effects that convolution has on input signals such as images.\nEffects of 2D filtering operations Moving Average Lets go through the simplest possible 2D filtering operation as shown below. Note that these implement the convolution.\nCompleted 2D MA Filtering\nThe MA filter causes \u0026ldquo;box\u0026rdquo; blurring of input images\n2D Gaussian Using a Gaussian Blur filter before edge detection aims to reduce the level of noise in the image, which improves the result of the susually subsequent edge-detection algorithms. We will meet again this 2D Gaussian filter in the object detection section, where it is used to help in the initial segmentation in RCNN architectures.\nGaussian Filter\nBlurring is evident in this picture\nBlurring is used to improve edge-detection\nThe above filtering operations are obviously deterministic. We are given the filter but in CNNs as we will see in the next section we are learning such filters. To keep the terminology aligned with the dense neural networks layers we will be denoting the filter with $\\mathbf w$ - the weights that need to be learned through the training process.\n"});index.add({'id':27,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-arch/','title':"CNN Architectures",'content':"CNN Architectures Convolutional Layer In the convolutional layer the first operation a 3D image with its two spatial dimensions and its third dimension due to the primary colors, typically Red Green and Blue is at the input layer, is convolved with a 3D structure called the filter shown below.\nEach filter is composed of kernels - source\nThe filter slides through the picture and the amount by which it slides is referred to as the stride $s$. The stride is a hyperparameter. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\nIt\u0026rsquo;s common to have more than one filter. Different filters pick up different qualities of the receptive field. Apart from the stride, the spatial dimensions of the filter (height, width, num_kernels) the number of filters is another hyperparameter of a convolutional layer.\nThis means that typically we are dealing with volumes (3D tensors) and of course if someone adds the fact that we do processing in minibatches we are typically dealing with 4D tensors that contain input feature maps. Lets look at a single feature map visualization below of the convolution operation.\nConvolutional layer with a single feature map. We can see the strides $(s_h, s_w)$, the zero padding as well as the receptive field in the produced feature map.\nIn the figure below the authors of this paper have also animated the operation. Blue maps are inputs, and cyan maps are outputs.\n       No padding, no strides Arbitrary padding, no strides Half padding, no strides Full padding, no strides         No padding, strides Padding, strides Padding, strides (odd)    In general though in practice we are dealing with volumes due to the multiple feature maps \u0026amp; kernels involved. Its important to understand the figure below. Its a direct extension to the single feature map figure above. The difference is that each neuron in each feature map of layer $l$ is connected to all neurons of the corresponding receptive field of layer $l-1$ just as before but now these connections extend to all feature maps of layer $l-1$. in other words we connect each neuron in the feature map of layer $l$ to the corresponding receptive volume (3D array) of neurons in the layer below.\nIn the class we will go through the example below.\nThere are two steps involved. Notice that the number of input feature maps is $M_{l-1} = 2$, while the number of output feature maps is $M_{l}=3$. We therefore have three filters of spatial dimension $[3x3]$ and depth dimension of 2. In the first step each of the three filters generates a correlation result for each of the 2 input feature maps.\n$z(i,j) = \\sum_u^{height} \\sum_v^{width} x(i+u, j+v) w(u,v)$\nIn the second step we sum over the correlations for each of the three filters separately. Equivalently is like taking a volume cross correlation and extend the equation above accordingly.\n$z(i,j,k_l) = \\sum_u^{height} \\sum_v^{width} \\sum_{k_{l-1}=1}^{M_i} x(i+u, j+v, k_{l-1}) w(u, v, k_{l-1}, k_l)$\nThe figure below illustrates the input feature map to output feature map mapping expressed directly i.e. without the intermediate step of the example above.\nConvolutional layers with multiple feature maps. We can see the receptive field of each column of neurons of the next layer. Each column is produced by performing multiple convolutions (or cross correlation operations) between the volume below and each of the filters.\nIn each layer we can have in other words, as was shown in the example above, input and output feature maps of different depths.\n2D convolution that produces a feature map with different depth than the input feature map\nZero Padding Each feature map \u0026ldquo;pixel\u0026rdquo; that results from the above convolution is followed by a RELU non-linearity i.e. RELU is applied element-wise. Few words about padding. There are two types: same padding where we add zeros at the edges of the picture and valid padding where we dont. The reason we pad with zeros is to maintain the original spatial dimensions from one convolution layer to the next. If we dont very soon we can end up with deep architectures with just a one \u0026ldquo;pixel\u0026rdquo;.\nLets see a complete animated example that includes padding. You can press the toggle movement button to stop the animation and do the calculations with pencil and paper.\n source: CS231n The output spatial dimension (assuming square) is in general given by $⌊ \\frac{i+2p-k}{s} ⌋ + 1$, where $p$ is the amount of passing, $k$ is the square kernel size, $s$ is the stride. In the animation above, $p=1, k=3, s = 2$.\nWhat the convolution / operation offers There are two main consequences of the convolution operation: sparsity and parameter sharing. With the later we get as a byproduct equivariance to translation. These are explained next.\nSparsity In DNNs, every output unit interacts with every input unit. Convolutional networks, however, typically have sparse interactions(also referred to as sparse connectivity or sparse weights). This is accomplished by making the kernel smaller than the input as shown in the figure above. For example,when processing an image, the input image might have thousands or millions of pixels, but we can detect small, meaningful features such as edges with kernels that occupy only tens or hundreds of pixels.\nSparse connectivity, viewed from below. We highlight one input unit,$x_3$, and highlight the output units in that are aﬀected by this unit. (Top) When is formed by convolution with a kernel of width 3, only three outputs are aﬀected by x. (Bottom)When is formed by matrix multiplication, connectivity is no longer sparse, so all the outputs are aﬀected by $x_3$.\nParameter sharing In CNNs, each member of the kernel is used at every feasible position of the input. The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for every location, we learn only one set.\nParameter sharing. Black arrows indicate the connections that use a particular parameter in two diﬀerent models. (Top)The black arrows indicate uses of the central element of a 3-element kernel in a convolutional model. Because of parameter sharing, this single parameter is used at all input locations. (Bottom)The single black arrow indicates the use of the central element of the weight matrix in a fully connected model. This model has no parameter sharing, so the parameter is used only once.\nThe particular form of parameter sharing causes the layer to have a property called equivariance to translation. This means that a translation of input features results in an equivalent translation of outputs. So if your pattern 0,3,2,0,0 on the input results in 0,1,0,0 in the output, then the pattern 0,0,3,2,0 might lead to 0,0,1,0\nAs explained here this should not be confused with invariance to translation. The later means that a translation of input features does not change the outputs at all. So if your pattern 0,3,2,0,0 on the input results in 0,1,0 in the output, then the pattern 0,0,3,2,0 would also lead to 0,1,0.\nFor feature maps in convolutional networks to be useful, they typically need both equivariance and invariance in some balance. The equivariance allows the network to generalise edge, texture, shape detection in different locations. The invariance allows precise location of the detected features to matter less. These are two complementary types of generalization for many image processing tasks.\nIn a nutshell in images, these properties ensure that the CNN that is trained to detect an object, can do its job irrespective on where the object is located in the image.\nDimensionality Reduction Pooling Layer Pooling was introduced to reduce redundancy of representation and reduce the number of parameters, recognizing that precise location is not important for object detection.\nThe pooling function is a form of non-linear function that further modifies the result of the RELU result. The pooling function accepts as input pixel values surrounding (a rectangular region) a feature map location (i,j) and returns one of the following\n the maximum, the average or distance weighted average, the L2 norm.  A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. You may alternatively pool over the depth dimension, in which case the image’s spatial dimensions (height and width) remain unchanged, but the number of channels is reduced.\nDespite receiving ample treatment in Ians Goodfellows\u0026rsquo; book, pooling has fallen out of favor. Some reasons are:\n Datasets are so big that we\u0026rsquo;re more concerned about under-fitting. Dropout is a much better regularizer. Pooling results in a loss of information - think about the max-pooling operation as an example shown in the figure below.  Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\nTo further understand the latest reservations against pooling in CNNs, see this summary of Hinton\u0026rsquo;s capsules concept. Capsules are not in scope for this course and by extension for the final test.\n1x1 Convolutional layer The 1x1 convolution layer is met in many network architectures (e.g. GoogleNet) and offers a number of modeling capabilities. Spatially, the 1x1 convolution is equivalent to a single number multiplication of each spatial position of the input feature map (if we ignore the non-linearity) as shown below. This means that leaves the spatial dimensions of the input feature maps unchanged unlike the pooling layer.\n1x1 convolution of a single feature map is just scaling - the 1x1 convolution is justified only when we have multiple feature maps at the input.\nThe most straightforward way to look at this layer is as a cross feature map pooling layer. When we have multiple input feature maps $M_{l-1}$ and 1x1 filters 1x1x$M_{l-1}$ (note the depth of the filter must match the number of the input feature maps) then we form a dot product between the feature maps at the spatial location $(i,j)$ of the 1x1 filter followed by a non-linearity (ReLU). This operation is in other words the same operation of a fully connected single layer neural network whose neurons are those spanned by the single column at the $(i,j)$ coordinate. This layer will produce a single output at each visited $(i,j)$ coordinate.\nThis idea can be expanded to multiple layers as described in this paper.\nWhen we have multiple $M_l$ layers of size 1 x 1 x $M_{l-1}$ then, effectively, we produce multiple feature maps one for each 1x1 layer and this is a good way to reduce the number of feature maps at the output of this layer, with benefits in computational complexity of the deep network as a whole.\nKnown CNN Architectures A summary of well known CNN networks are here article with this update as a reference. This summary will be important to you as a starting point to develop your own understanding of very well known CNNs and after you read the corresponding papers in arxiv you will be able to recall key design patterns and why those patterns came to be.\n"});index.add({'id':28,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-sizing/','title':"CNN Sizing",'content':"CNN Sizing Sizing is an exercise that will help you how to specify hyperparameters in tf.keras such as the height, width, depth of filters, feature map sizes etc. Sizing is needed so that you can stitch all the layers together correctly.\nSizing Example We will use an toy network for such exercise.\nThe example CNN architecture above has the following layers:\n INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B. CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters. RELU layer will apply an elementwise activation function, such as the $\\max(0,x)$ thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]). POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12]. FC (i.e. fully-connected) layer, also known as dense, will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10 dataset. As you recall in FC layers each neuron in this layer will be connected to all the neurons in the previous volume.  The impact of padding on the sizing of the produced feature map is shown in the following numerical example. The example is for [28x28x3] input layer but results can be extrapolated for [32x32x3]\nNumber of parameters and memory "});index.add({'id':29,'href':'/cs-gy-6613-spring-2020/docs/lectures/cnn/visual-attention/','title':"Attention Mechanisms in CNNs",'content':""});index.add({'id':30,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/scene-understanding-intro/','title':"Introduction to Scene Understanding",'content':"Introduction to Scene Understanding In the previous chapters we have treated the perception subsystem mainly from starting the first principles that govern supervised learning to algorithms that enable classical as well as deep learning machines. Now we are synthesizing these algorithms to pipelines that can potentially enable the holly grail of perception - our understanding of the scene. As discussed in the introduction to computer vision, humans has a unique to interpret scenes based on their ability to infer (reason) what they dont see. This is the reason why the scene understanding involves far more than just perception. In this chapter we will cover algorithms that allow us to:\n Detect objects in an image. Object detection is demonstrated in this short video clip that shows the end result of the algorithm.   Its important to understand the difference between classification and object detection as shown below.\nDifference between classification and detection\nIn classification we are given images (we can consider video clips as a sequence of images) and we are asked to produce the array of labels assigned to objects that are present in the frame. Typically in many datasets there is only one class and the images are cropped around the object. In localization, in addition to classification we are interested in locating (using for example a bounding box) each class in the frame. In object detection we are localizing multiple objects (some objects can be of the same class.) Localization is a regression problem fundamentally. Mathematically we have,\n$$y = p_{data}(x)$$\nWe try to come up with a function approximation to the true function $p_{data}$ that maps the image $x$ to the location of the bounding box $y$. We can uniquely represent the bounding box by the (x,y) coordinates of its upper left corner and its width and height $[x,y,w,h]$. Being a regression problem, as $y$ is a floating point vector, we can use well known loss functions e.g. CE $≡$ MSE where the error is the Euclidean distance between the coordinates of the true bounding box and the estimated bounding box. However, the regression approach does not work well in practice and has been superceded by the algorithms described later in this chapter.\nAssign semantic labels to each pixel in this image.  Sementic Segmentation in medical, robotic and sports analytics applications\nBoth of these abilities enable the reflexive part of perception where the inference ends up being a classification or regression or search problem and in practice, depending on the algorithm, it can range from few ms to 100s of ms. Both of these reflexive inferences are essential parts of many mission critical almost real time applications such as robotics e.g. self driving cars.\nThere are other abilities that we need for scene understanding that don\u0026rsquo;t cover until later in this book. Our ability to recognize the attribute of uniqueness in an object and assign a symbol to it, is fundamental to our ability to reason very quickly at the symbolic level. At that level we can use a whole portfolio of symbolic inference algorithms developed over the last few decades. But before we reach this level we need to solve the supervised learning problem for the relatively narrow task of bounding and coloring objects. This needs annotated data and knowing what kind of data we have at our disposal is an essential skill.\nDatasets for scene understanding tasks COCO Typical example for Detection and Image Captioning Tasks\nAfter its publication by Microsoft, the COCO dataset has become the reference dataset to train models in perception tasks and it is constantly evolving through yearly competitions. The competitions are challenging as compared to earlier ones (e.g. VOC) (see performance section) since many objects are small. COCO\u0026rsquo;s 330K images are annotated with\n 80 object classes. These are the so-called thing classes (person, car, elephant, \u0026hellip;). 91 stuff classes. These are the co-called stuff classes (sky, grass, wall, \u0026hellip;). Stuff classes cover the majority of the pixels in COCO (~66%.). Stuff classes are important as they allow to explain important aspects of an image, including scene type, which thing classes are likely to be present and their location (through contextual reasoning), physical attributes, material types and geometric properties of the scene. 5 captions per image Keypoints for the \u0026ldquo;person\u0026rdquo; class  Common perception tasks that the dataset can be used for, include:\n Detection Task: Object detection and semantic segmentation of thing classes. Stuff Segmentation Task: Semantic segmentation of stuff classes. Keypoints Task: Localization of person\u0026rsquo;s keypoints (sparse skeletal points). DensePose Task: Localization of people\u0026rsquo;s dense keypoints, mapping all human pixels to a 3D surface of the human body. Panoptic Segmentation Task: Scene segmentation, unifying semantic and instance segmentation tasks. Task is across thing and stuff classes. Image Captioning Task: Describing with natural language text the image. This task ended in 2015. Image captioning is very important though and other datasets exists to supplement the curated COCO captions.  Even in a world with so much data, the curated available datasets that can be used to train models are by no means enough to solve AI problems in any domain. First, datasets are geared towards competitions that supposedly can advance the science but in many instances deaderboards become \u0026ldquo;academic exercises\u0026rdquo; where 0.1% mean accuracy improvement can win the competition but definitely does not progress AI. The double digit improvements can and these discoveries create clusters of implementations and publications around them that fine tune them. One of these discoveries is the RCNN architecture described in the object detection section that advanced the accuracy metric by almost 30%. Secondly, the scene understanding problems that AI engineers will face in the field, e.g. in industrial automation or drug discovery, involve domain specific classes of objects. Although we cant directly use curated datasets, engineers can transfer learning, worthy of a chapter by itself, where a dataset is used to train a model for a given task whose weights can be reused to train a model for fairly similar task.\nDetection/Segmentation Task Evaluation Metrics The evaluation metrics for detection with bounding boxes and segmentation masks are identical in all respects except for the IoU computation (which is performed over boxes or masks, respectively). Therefore we omit any evaluation discussion in the semantic segmentation chapter.\nTo understand the calculation of mAP see this write up.\n"});index.add({'id':31,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/','title':"Lecture 4b - Sequences and Recurrent Neural Networks (RNN)",'content':"Sequences and Recurrent Neural Networks (RNN) Sequences Data streams are everywhere in various applications. For example, weather station sensor data arrive in streams indexed by time, in financial trading data - one can think of many others. We are interested to fit sequenced data with a model and in order to do so we need a hypothesis set that we can draw from that is rich enough for the task at hand.\nDynamical systems are such rich models. In the following we use $t$ as the index variable and the notation $s_{1:\\tau}$ means the sequence from 1 to $\\tau$, without this implying any time semantics for the index $t$. In a dynamical system, its _recurrent_ state evolution can be represented as:\n$$\\mathbf{s}_t = f_t(\\mathbf{s}_{t-1}, \\mathbf{a}_t ; \\bm \\theta_t)$$\nwhere $\\bm s$ is the evolving state, $\\bm a$ is an external action or control and $\\bm \\theta$ is a set of parameters that specify the state evolution model $f$. This innocent looking equation can capture quite a lot of complexity.\n The state space which is the set of states can depend on $t$. The action space similarly can depend on $t$ Finally, the function that maps previous states and actions to a new state can also depend on $t$  So the dynamical system above has indeed offer a very profound modeling flexibility.\nRNN Architecture The RNN architecture is a constrained implementation of the above dynamical system\n$$\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{x}_t ; \\bm \\theta)$$\nRNNs implement the same function (parametrized by $\\bm \\theta$) across the sequence $1:\\tau$. Effectively there is no dependency on $t$ of the parameters $\\bm \\theta$ and this means that the network shares parameters across the sequence. We have seen parameter sharing in CNNs as well but if you recall the sharing was over the relatively small span of the filter. But the most striking difference between CNNs and RNNs is in recursion itself. The state is latent and is denoted with $\\bm h$ to match the notation we used earlier for the hidden layers.\nRecursive state representation in RNNs\nThe weights $\\bm h$ in CNNs were not a function of previous weights and this means that they cannot remember previous hidden states in the classification or regression task they try to solve. This is perhaps the most distinguishing element of the RNN architecture - its ability to remember via the hidden state who is dimensioned according to the task at hand. There is a way using sliding windows to allow DNNs to remember past inputs as shown in the figure below for an NLP application.\nDNNs can create models from sequential data (such as the language modeling use case shown here). At each step $t$ the network with a sliding window span of $\\tau=3$ that acts as memory, will concatenate the word embeddings and use a hidden layer $\\bm h$ to predict the the next element in the sequence. However, notice that (a) the span is limited and fixed (b) words such as \u0026ldquo;the ground\u0026rdquo; will appear in multiple sliding windows forcing the network to learn two different patterns for this constituent (\u0026ldquo;in the ground\u0026rdquo;, \u0026ldquo;the ground there\u0026rdquo;).\nThere are many RNN architectures and in this course will suffice to go over just two to understand what they offer in terms of their representational capacity. One significant factor that separates the architectures is the way they perform the hidden state calculation at each $t$. This is shown in the next figure.\nDifferentiating Architectures (a) DNN, (b) Simple RNN, (c) LTSM, (d) GRU\n"});index.add({'id':32,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/','title':"Lecture 5 - Scene Understanding",'content':""});index.add({'id':33,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/feature-extraction-resnet/','title':"Feature Extraction via Residual Networks",'content':"Feature Extraction via Residual Networks The ResNet Architecture 34 layers deep ResNet architecture (3rd column) vs earlier architectures\nResNets or residual networks, introduced the concept of the residual. This can be understood looking at a small residual network of three stages. The striking difference between ResNets and earlier architectures quoted in the linked articles in http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-arch/ are the skip connections. Shortcut connections are those skipping one or more layers. The shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers. Identity shortcut connections add neither extra parameter nor computational complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries without modifying the solvers.\nHinton showed that dropping out individual neurons during training leads to a network that is equivalent to averaging over an ensemble of exponentially many networks. Entire layers can be removed from plain residual networks without impacting performance, indicating that they do not strongly depend on each other.\nEach layer consists of a residual module $f_i$ and a skip connection bypassing $f_i$. Since layers in residual networks can comprise multiple convolutional layers, we refer to them as residual blocks. With $y_{i-1}$ as is input, the output of the i-th block is recursively defined as\n$y_i = f_i(y_{i−1}) + y_{i−1}$\nwhere $f_i(x)$ is some sequence of convolutions, batch normalization, and Rectified Linear Units (ReLU) as nonlinearities. In the figure above we have three blocks. Each $f_i(x)$ is defined by\n$f_i(x) = W_i^{(1)} * \\sigma(B (W_i^{(2)} * \\sigma(B(x))))$\nwhere $W_i^{(1)}$ and $W_i^{(2)}$ are weight matrices, · denotes convolution, $B(x)$ is batch normalization and $\\sigma(x) ≡ max(x, 0)$. Other formulations are typically composed of the same operations, but may differ in their order.\nDuring the lecture we will go through this paper analysis of the unrolled network to understand the behavior of ResNets that are inherently scalable networks.\nResNets introduced below - are commonly used as feature extractors for object detection. They are not the only ones but these networks are the obvious / typical choice today and they can also be used in real time video streaming applications achieving significant throughput e.g. 20 frames per second.\n "});index.add({'id':34,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/simple-rnn/','title':"Simple RNNs and their Backpropagation",'content':"Simple RNN Simple RNN with recurrences between hidden units. This architecture can compute any computable function and therefore is a Universal Turing Machine. Your laptops and smartphones are descendants of UTM.\nNotice how the path from input $\\bm x_{t-1}$ affects the label $\\bm y_{t}$ and also the conditional independence between $\\bm y$ given $\\bm x$. Please note that this is not a computational graph rather one way to represent the hidden state transfer between recurrences.\nForward Propagation This network maps the input sequence to a sequence of the same length and implements the following forward pass:\n$$\\bm a_t = \\bm W \\bm h _{t-1} + \\bm U \\bm x_t + \\bm b$$\n$$\\bm h_t = \\tanh(\\bm a_t)$$\n$$\\bm o_t = \\bm V \\bm h_t + \\bm c$$\n$$\\hat \\bm y_t = \\mathtt{softmax}(\\bm o_t)$$\n$$L(\\bm x_1, \\dots , \\bm x_{\\tau}, \\bm y_1, \\dots , \\bm y_{\\tau}) = D_{KL}[\\hat p_{data}(\\bm y | \\bm x) || p_{model}(\\bm y | \\bm x; \\bm w)]$$\n$$= - E_{\\bm y | \\bm x ≋ \\hat{p}_{data}} \\log p_{model}(\\bm y | \\bm x ; \\bm w) = - \\sum_t \\log p_{model}(y_t | \\bm x_1, \\dots, \\bm x_t ; \\bm w)$$\nNotice that RNNs can model very generic distributions $\\log p_{model}(\\bm x, \\bm y ; \\bm w)$. The simple RNN architecture above, effectively models the posterior distribution $\\log p_{model}(\\bm y | \\bm x ; \\bm w)$ and based on a conditional independence assumption it factorizes into $\\sum_t \\log p_{model}(y_t | \\bm x_1, \\dots, \\bm x_t ; \\bm w)$.\nNote that by connecting the $\\bm y_{t-1}$ to $\\bm h_t$ via a matrix e.g. $\\bm R$ we can avoid this simplifying assumption and be able to model an arbitrary distribution $\\log p_{model}(\\bm y | \\bm x ; \\bm w)$. In other words just like in the other DNN architectures, connectivity directly affects the representational capacity of the hypothesis set.\nIn many instances we have problems where it only matters the label $y_\\tau$ at the end of the sequence. Lets say that you are classifying speech or video inside the cabin of a car to detect the psychological state of the driver. The same architecture shown above can also represent such problems - the only difference is the only the $\\bm o_\\tau$, $L_\\tau$ and $y_\\tau$ will be considered.\nLets see an example to understand better the forward propagation equations.\nExample sentence as input to the RNN\nIn the figure above you have a hypothetical document (a sentence) that is broken into what in natural language processing called tokens. Lets say that a token is a word in this case. In the simpler case where we need a classification of the whole document, given that $\\tau=6$, we are going to receive at t=1, the first token $\\bm x_1$ and with an input hidden state $\\bm h_0 = 0$ we will calculate the forward equations for $\\bm h_1$, ignoring the output $\\bm o_1$ and repeat the unrolling when the next input $\\bm x_2$ comes in until we reach the end of sentence token $\\bm x_6$ which in this case will calculate the output and loss\n$$- \\log p_{model} (y_6|\\bm x_1, \\dots , \\bm x_6; \\bm w)$$\nwhere $\\bm w = \\{ \\bm W, \\bm U, \\bm V, \\bm b, \\bm c \\}$.\nBack-Propagation Through Time (BPTT) Lets now see how the backward propagation would work.\nUnderstanding RNN memory through BPTT procedure\nBackpropagation is similar to that of feed-forward (FF) networks simply because the unrolled architecture resembles a FF one. But there is an important difference and we explain this using the above computational graph for the unrolled recurrences $t$ and $t-1$. During computation of the variable $\\bm h_t$ we use the value of the variable $\\bm h_{t-1}$ calculated in the previous recurrence. So when we apply the chain rule in the backward phase of BP, for all nodes that involve the such variables with recurrent dependencies, the end result is that _non local_ gradients from previous backpropagation steps ($t$ in the figure) appear. This is effectively why we say that simple RNNs feature _memory_. This is in contrast to the FF network case where during BP only local to each gate gradients where involved as we have seen in the the DNN chapter.\nThe key point to notice in the backpropagation in recurrence $t-1$ is the junction between $\\tanh$ and $\\bm V \\bm h_{t-1}$. This junction brings in the gradient $\\nabla_{\\bm h_{t-1}}L_t$ from the backpropagation of the $\\bm W h_{t-1}$ node in recurrence $t$ and just because its a junction, it is added to the backpropagated gradient from above in the current recurrence $t-1$ i.e.\n$$\\nabla_{\\bm h_{t-1}}L_{t-1} += \\nabla_{\\bm h_{t-1}}L_t $$\nIan Goodfellow\u0026rsquo;s book section 10.2.2 provides the exact equations - please note that you need to know the intuition behind computational graphs for RNNs.\nVanishing or exploding gradients In the figure below we have drafted a conceptual version of what is happening with recurrences over time. Its called an infinite impulse response filter for reasons that will be apparent shortly.\nInfinite Impulse Response (IIR) filter with weight $w$\nWith $D$ denoting a unit delay, the recurrence formula for this system is:\n$$h_t = w h_{t-1} + x_t$$\nwhere $w$is a weight (a scalar). Lets consider what happens when an impulse, $x_t = \\delta_t$ is fed at the input of this system with $w=-0.9$.\n$$h_0 = -0.9 h_{-1} + \\delta_0 = 1$$ $$h_1 = -0.9 h_{0} + \\delta_1 = -0.9$$ $$h_2 = -0.9 h_{1} + \\delta_2 = 0.81$$ $$h_3 = -0.9 h_{2} + \\delta_3 = -0.729$$\nWith $w=-0.9$, the h_t (called impulse response) follows a decaying exponential envelope while obviously with $w \u0026gt; 1.0$ it would follow an exponentially increasing envelope. Such recurrences if continue will result in vanishing or exploding responses long after the impulse showed up in the input $t=0$.\nIn a similar fashion, the RNN hidden state recurrence, in the backwards pass of backpropagation that extends from the $t=\\tau$ to $t=1$ can make the gradient, when $\\tau$ is large, either vanish or explode. Instead of a scalar $\\w$ we have matrices $\\bm W$ involved instead of $h$ we have gradients $\\nabla_{\\bm h_{t}}L_{t}$. This is discussed in this paper.\nUsing this primitive IIR filter as an example, we can see that the weight plays a crucial role in the impulse response. This is further discussed in the LSTM section.\n"});index.add({'id':35,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/object-detection/','title':"Object Detection",'content':"Object Detection In the introductory section, we have seen examples of what object detection is. In this section we will treat the detection pipeline itself, summarized below:\nObject detection pipeline.\nWork on object detection spans 20 years and is impossible to cover every algorithmic approach in this section - the interested reader can trace these developments by reading in this paper.\nAs expected, since 2014, deep learning has surpassed classical ML in the detection competitions - we therefore focus only on such architectures. More specifically we will be focusing on the so called two stage detectors that employ two key ingredients:\n Recognition using regions that we will explain shortly. CNNs that we covered earlier.  Object detection, involves three main stages: the feature extraction stage, the classification stage and the detection or localization stage. In the literature the feature and classification stages are counted as one, called the classification stage and people refer to such architecture as _two stage.\nWe also need to insert an additional requirement: to be able to detect objects in almost real time (20 frames per second) - a significant subset of what we call mission critical applications require it. Therefore will focus a specific family that is considered to be the canonical CNN architecture for detection - the family of Region CNNs.\nRegion CNN (RCNN) Region Proposals We can think about the detection problem as a classification problem of all possible portions (windows/masks) of the input image since an object can be located at any position and scale in the image. It is natural to search therefore everywhere and an obvious method to generate region proposals, is to slide various width-height ratio windows slide around the image and using a metric to declare that the window contains one or more blob of pixels. Obviously, such method is computationally infeasible and we need think of how to reduce this number by having some means of gauging where to look in the image.\nRCNN can accommodate a number of efficient algorithms that can produce region proposals. The baseline RCNN employs instead selective search via hierarchical grouping.\nThe algorithm contains another algorithm that segments the image into initial regions.\nGraph-based segmentation We perform segmentation in the image using an efficient graph-based algorithm to obtain the set $R=\\{r_1, \\dots, r_n \\}$ of initial regions. The segmentation algorithm starts by formulating the image as a graph.\nLet G = (V, E) be an undirected graph with vertices $v_i \\in V$ , the set of elements to be segmented, and edges $(v_i, v_j) ∈ E$ corresponding to pairs of neighboring vertices. Each edge has a corresponding weight $w((v_i, v_j ))$, which is a non-negative measure of the dissimilarity between neighboring elements $v_i$ and $v_j$. In the case of image segmentation, the elements in V are pixels and the weight of an edge is some measure of the dissimilarity between the two pixels connected by that edge (e.g., the difference in intensity, color, motion, location or some other local attribute).\nIn the graph-based approach, a segmentation $S$ is a partition of $V$ into components such that each component (or region) $C ∈ S$ corresponds to a connected component in a graph $G\u0026rsquo; = (V, E\u0026rsquo;)$, where $E\u0026rsquo; ⊆ E$. In other words, any segmentation is induced by a subset of the edges in $E$. There are different ways to measure the quality of a segmentation but in general we want the elements in a component to be similar, and elements in different components to be dissimilar. This means that edges between two vertices in the same component should have relatively low weights, and edges between vertices in different components should have higher weights.\nFor example, a pixel $p_i$ corresponds to a vertex $v_i$ and it has 8 neighboring pixels. We can define the weight to be the absolute value of the dissimilarity between the pixel intensity $I(p_i)$ and its neighbors.\n$$w((v_i, v_j )) = |I(p_i) − I(p_j )|$$\nBefore we compute the weights we use the 2D Gaussian kernel / filter we met in the introductory section to CNNs - the end result being a smoothing of the image that helps with noise reduction. For color images we run the algorithm for each of the three channels.\nThere is one runtime parameter for the algorithm, which is the value of $k$ that is used to compute the threshold function $\\tau$ . Recall we use the function $τ(C) =k/|C|$ where $|C|$ is the number of elements in C. Thus k effectively sets a scale of observation, in that a larger k causes a preference for larger components.\nThe graph algorithm can also accommodate dissimilarity weights between neighbors at a feature space rather at pixel level based on a Euclidean distance metric with other distance functions possible.\nGraph representation of the segmentation problem\nNotice that the initial segments may be many and do not necessarily accurately represent distinct objects as shown below:\nResult of the initial segments produced by the graph-based algorithm, $k=300$\nGrouping After the initial regions are produced, we use a greedy algorithm to iteratively group regions together. This is what gives the hierarchical in the name of the algorithm.\nFirst the similarities between all neighboring regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbors. The process of grouping the most similar regions is repeated until the whole image becomes a single region.\nFor the similarity $s(r_i ,rj)$ between region $r_i$ and $r_j$ we apply a variety of complementary measures under the constraint that they are fast to compute. In effect, this means that the similarities should be based on features that can be propagated through the hierarchy, i.e. when merging region $r_i$ and $r_j$ into $r_t$, the features of region $r_t$ need to be calculated from the features of $r_i$ and $r_j$ without accessing the image pixels. Such feature similarities include color, texture, size, fill - we create a binary sum of such individual similarities.\n$$ s(r_i ,r_j) = a_1 s_{color}(r_i ,r_j)+a_2 s_{texture}(r_i ,r_j) + a_3 s_{size}(r_i ,r_j) + a_4 s_{fill}(r_i ,r_j) $$\nwhere $a_i ∈ {0,1}$ denotes if the similarity measure is used or not.\nThe end result of the grouping is a hierarchy of regions ranked according to creation time. As earlier created regions may end up being the largest some permutation in the ranking is applied.\nExample of hierarchical grouping\nThe above selective search strategy is diversified (1) by using a variety of colour spaces with different invariance properties, (2) by using different similarity measures $s(r_i, r_j)$, and (3) by varying our initial regions. Each strategy results in a different hierarchy and after a permutation that randomizes the ranking the final list of regions is produced. For RCNN we use 2000 such region proposals.\nFinal proposals - in reality we have 2000 of such regions.\nCNN Features and SVM Classification Each of these proposals can be fed into a CNN (e.g. ResNet). Since regions are of various rectangular shapes, we warp the regions to a fixed size (CNNs can process fixed input tensors) of 227 x 227 pixels and the CNN produces a 4096-dim feature vector from each of the regions. Note that this representation of each region by 4096 elements is considered quite compact and more importantly the features are shared across classes.\nUsing these features we use a binary SVM classifier that produces a positive or negative on whether this feature contains the class of interest or not. We train a separate binary SVM classifier per class (binary classification).\nThe sharing of features allows us to parctically aggregate into a matrix all features of all regions (2000 x 4096) we well as aggregate the SVM weights into another matrix (4096 x K), where K is the number of classes we have trained for, and do this combined operation via a matrix-matrix product.\nAfter the scoring of each proposed region by the SVM we apply a greedy Non-Maximum Suppression (NMS) algorithm for each class independently, that rejects a region if it has an Intersection over Union (IoU) metric higher than a threshold with a higher scoring region. This threshold was empirically determined to be 0.3 for the task outlined in the paper. But it is expected to be a hyperparameter in practice.\nFinally, a bounding box regressor, predicts the location of the bounding boxes using the proposal boxes and nearby ground truth box data so that we can adjust the final detection box and avoid situations that whole objects are detected but partially included in the detection box.\nThe whole process is partially shown below and the solution is called Region-CNN (RCNN).\nRCNN pipeline\nRCNN detection result after processing multiple region proposals\nTraining RCNN training is complex as involves a multi-stage pipeline.\n R-CNN first fine pretrains a CNN on a classification task without using any bounding box ground truth. It then adapts the trained CNN by replacing the classification layer with another randomly initialized layer of span K+1, where K are the number of classes in the domain of interest (+1 is due to the fact that we consider the background). We continue training with SGD using (a) as inputs the warped images as determined by the region proposals (b) using this time the ground truth bounding boxes and declaring each region proposal with IoU ? 0.5 relative to the ground truth bounding box as true positive. Then, it fits SVMs to CNN features. These SVMs act as object detectors, replacing the softmax classifier learnt by fine-tuning.  .\nFast RCNN Fast-RCNN is the second generation RCNN that aimed to accelerate RCNN. Apart from the complex training of RCNN, its inference involved a forward pass for each of the 2000 proposals.\nFast RCNN Architecture\nA Fast RCNN network takes as input an entire image and a set of proposals $R$. The set of proposals is produced by the selective search alg used in RCNN and its similarly around 2000 per image.\n  The network first processes the whole image with a CNN having several convolutional (experiments were done for 5-13 layers) and 5 max pooling layers to produce a feature map. The selective search met in RCNN, produces region proposals and for each proposal, a region of interest (RoI) pooling layer (see below) extracts a fixed-length feature vector from the feature map. This is in contrast to the RCNN that fed the different proposals to the CNN. Now we only have one feature map and we elect regions of interest from that.\n  Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.\n  The key element in the architecture is the RoI pooling layer.\nWhat is an RoI? An RoI is a rectangular window into a feature map. Each RoI is defined by a four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w).\nThe RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g.7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI. RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling\nAs you may have noticed we have replaced the SVN and the training can happen end to end starting from a pretrained CNN and using a multi-task (classification, regression) loss function. NMS is maintained just like in RCNN to produce the final box.\nFaster RCNN With Faster RCNN, we are not making changes to Fast RCNN detector itself bur rather to the input of the CNN. The selective search algorithm that is considered slow and computationally expensive is replaced with a neural network called the Region Proposal Network (RPN) that as the name implies proposes regions.\nFaster RCNN Architecture - the RPN tells the Fast RCNN detector where to attend to\nTherefore, in this architecture there is one CNN network that does not only produces a global feature map but also it produces proposals from the feature map itself rather than the original image.\nRegion Proposals as generated by the RPN network\nIt is doing so by sliding a window n x n over the feature map. At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal.\nThe k proposals are parameterized relative to k reference boxes, which we call anchor boxes. The size can be changed but the original paper used anchor size of (128 × 128, 256 × 256, 512 × 512) and three aspect ratios (1:1, 1:2 and 2:1). An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio. By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a feature map of a size W × H (typically ∼2,400), there are $W xH x k$ anchors in total.\nThe RPN network produces a classification score i.e. how confident we are that there is an object for each of the anchor boxes as well as the regression on the anchor box coordinates.\nFinally, the following curve (focus on COCO) presents the relative performance between "});index.add({'id':36,'href':'/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/semantic-segmentation/','title':"Semantic Segmentation",'content':""});index.add({'id':37,'href':'/cs-gy-6613-spring-2020/docs/lectures/rnn/lstm/','title':"The Long Short-Term Memory (LSTM) Cell Architecture",'content':"The Long Short-Term Memory (LSTM) Cell Architecture In the simple RNN we have seen the problem of exploding or vanishing gradients when the span of back-propagation is large (large $\\tau$). Using the conceptual IIR filter, that ultimately integrates the input signal, we have seen that in order to avoid an exploding or vanishing impulse response, we need to control $w$. This is exactly what is being done in an evolutionary RNN architectures that we will treat in this section called gated RNNs.\nThe way we control $w$ is to have another system produce it for the task at hand. In the best known gated RNN architecture, the IIR recurrence and everything that controls it, is contained in the LSTM cell. The LSTM cell adjusts $w$ depending on the input sequence context and this means that (a) there is an internal memory to the cell, we call this the cell state and (b) $w$ will fluctuate depending on $x$. We employ another hidden unit to learn the context and, based on that, set the right $w$. This unit is called the forget gate: since by making $w$ equal to zero, $h_t$ stops propagating aka it forgets the previous hidden state. We employ a couple of other gates as well: the input gate and the output gate as shown in the diagram below.\nLSTM Cell: The cell is divided into three areas: input (green), cell state (blue) and output (red). The $i$ index (see description below) has been supressed for clarity\nThe cell is divided into three areas: input (green), cell state (blue) and output (red)\nIn each area there is a corresponding gate (filled node) - these are the input gate, forget gate, output gate for the input, cell state and output regions respectively. The gates controls the flow of information that goes through these areas via element-wise multipliers. The two inputs to the cells are the concatenation of $\\bm x_t$ and $\\bm h_{t-1}$ and the cell state from the previous recurrence $\\bm s_{t-1}$.\nThe Cell State Starting at the heart of the LSTM cell, to describe the update of we will use two indices: one for the unfolding sequence index $t$ and the other for the cell index $i$. We use the additional index to allow the current cell at step $t$ to use or forget inputs and hidden states from other cells.\n$$s_t(i) = f_t(i) s_{t-1}(i) + g_t(i) \\sigma \\Big( \\bm W^T(i) \\bm h_{t-1}(i) + \\bm U^T(i) \\bm x_t(i) + \\bm b(i) \\Big)$$\nThe parameters $\\theta_{in} = \\{ \\bm W, \\bm U, \\bm b \\}$ are the recurrent weights, input weights and bias at the input of the LSTM cell.\nThe forget gate calculates the forgetting factor,\n$$f_t(i) =\\sigma \\Big( \\bm W_f^T(i) \\bm h_{t-1}(i) + \\bm U_f^T(i) \\bm x_t(i) + \\bm b_f(i) \\Big) $$\nInput The input gate protects the cell state contents from perturbations by irrelevant to the context inputs. Quantitatively, input gate calculates the factor,\n$$g_t(i) =\\sigma \\Big( \\bm W_g^T(i) \\bm h_{t-1}(i) + \\bm U_g^T(i) \\bm x_t(i) + \\bm b_g(i) \\Big) $$\nThe gate with its sigmoid function adjusts the value of each element produced by the input neural network.\nOutput The output gate protects the subsequent cells from perturbations by irrelevant to their context cell state. Quantitatively,\n$$h_t(i) = q_t(i) \\tanh(s_t(i))$$\nwhere $q_t(i)$ is the output factor\n$$q_t(i) =\\sigma \\Big( \\bm W_o^T(i) \\bm h_{t-1}(i) + \\bm U_o^T(i) \\bm x_t(i) + \\bm b_o(i) \\Big) $$\nNotice that if you omit the gates we are back to the simple RNN architecture. You can expect backpropagation to work similarly in LSTM albeit with more complicated expressions. Some more diagraming and annimation dont hurt to understand LSTMs. See 1 and 2.\n"});index.add({'id':38,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/','title':"Lecture 6 - Probabilistic Graphical Models",'content':""});index.add({'id':39,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/pgm-intro/','title':"Introduction to Probabilistic Reasoning",'content':"Introduction to Probabilistic Reasoning In the scene understanding chapter we started putting together the perception pipelines that resulted in us knowing where are the objects of interest in the image coordinate frame but we stopped short of any advanced methods that can lead to what we humans call understanding. We hinted that assigning an object an symbolic identity is an essential ability that allows an embodied AI agent to reason about the scene using symbolic reasoning approaches researched by the AI community over many decades.\nPositioning of probabilistic reasoning subsystem\nStaying in this trajectory, we introduce the topic of reasoning via a probabilistic lens. We argue that enhancing the environment state as determined by the perception subsystem, includes another subsystem that we will call probabilistic reasoning subsystem that allow us to:\n infer the hidden state of the environment and learn the state that the agents internally maintains via appropriate to the task representations.  In a subsequent chapter we will enhance the model to include past actions rather than just past percepts. Let us now start with the rule that is the cornerstone of probabilistic modeling and reasoning.\nThe Bayes Rule Thomas Bayes (1701-1761)\nLet $\\mathbf{\\theta}$ denote the unknown parameters, $D$ denote the dataset and $\\mathcal{H}$ denote the hypothesis space that we met in the learning problem chapter. The Bayes rule states:\n$$ P(\\mathbf{\\theta} | D, \\mathcal{H}) = \\frac{P( D | \\mathbf{\\theta}, \\mathcal{H}) P(\\mathbf{\\theta} | \\mathcal{H}) }{ P(D|\\mathcal{H})} $$\nThe Bayesian framework allows the introduction of priors $p(\\theta | \\mathcal{H})$ from a wide variety of sources: experts, other data, past posteriors, etc. It allows us to calculate the posterior distribution from the likelihood function and this prior subject to a normalizing constant. We will call belief the internal to the agent posterior probability estimate of a random variable as calculated via the Bayes rule. For example,a medical patient is exhibiting symptoms x, y and z. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor (the expert) has beliefs about the underlying disease, but a second doctor may have slightly different beliefs.\nProbabilistic Graphical Models Let us now look at a representation, the probabilistic graphical model (pgm) (also called Bayesian network when the priors are captured) that can be used to capture the structure of such beliefs and in general capture dependencies between the random variables involved in the modeling of a problem. We can use such representations to efficiently compute such beliefs and in general conditional probabilities. For now we will limit the modeling horizon to just one snapshot in time - later we will expand to capture problems that include time $t$ as a variable.\nBy convention we represent in PGMsas directed graphs, with nodes being the random variables involved in the model and directed edges indicating a parent child relationship, with the arrow pointing to a child, representing that the child nodes are probabilistically conditioned on the parent(s).\nIn a hypothetical example of a joint distribution with $K=7$ random variables,\nPGM governing the joint distribution $p(x_1, x_2, \u0026hellip;, x_7)=p(x_1)p(x_2)p(x_3)p(x_4|x_1, x_2, x_3)p(x_5|x_1, x_3) p(x_6|x_4)p(x_7|x_4, x_5)$\nIn general,\n$$p(\\mathbf x)= \\prod_{k=1}^K p(x_k | \\mathtt{pa}_k)$$\nwhere $\\mathtt{pa}_k$ is the set of parents of $x_k$.\nNote that we have assumed that our model does not have variables involved in directed cycles and therefore we call such graphs Directed Acyclic Graphs (DAGs).\n"});index.add({'id':40,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/bayesian-inference/','title':"Inference in Graphical Models",'content':"Inference in Graphical Models Bayesian Linear Regression The PGM representation should not feel foreign - lets consider the simplest possible example of a graphical model and see how it connects to concepts we have seen before. Any joint distribution $p(\\bm x, y)$ can be decomposed using the product rule (we drop the data qualifier)\n$$p(\\bm x, y) = p(\\bm x) p(y|\\bm x)$$\nand such distribution can be represented via the simple PGM graph (a) below.\nSimplest possible PGM example\nWe introduce now a graphical notation where we shade, nodes that we consider observed. Let us know assume that we observe $y$ as shown in (b). We can view the marginal $p(\\bm x)$ as a prior over $x$ and and we can infer the posterior distribution using the Bayes rule\n$$p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$$\nwhere using the sum rule we know $p(y) = \\sum_{x\u0026rsquo;} p(y|x\u0026rsquo;) p(x\u0026rsquo;)$. This is a very innocent but very powerful concept.\nTo see why lets consider an online learning problem where the underlying target function is $p_{data}(x, \\mathbf a) = a_0 + a_1 x + n$ - this is the equation of the line. In this example its parametrized with $a_0=-0.3, a_1=0.5$ and $n \\in \\mathcal N(0, \\sigma=0.2)$. To match the simple inference exercise that we just saw, we draw the equivalent PGM\nBayesian Linear Regression example - please replace $t$ with $y$ to match earlier notation in these notes\nThe Bayesian update of the posterior can be intuitively understood using a graphical example of our model of the form: $$g(x,\\mathbf{w})= w_0 + w_1 x$$ (our hypothesis). The reason why we pick this example is illustrative as the model has just two parameters and is amendable to visualization. The update needs a prior distribution over $\\mathbf w$ and a likelihood function. As prior we assume a spherical Gaussian\n$$p(\\mathbf w | \\alpha) = \\mathcal N(\\mathbf w | \\mathbf 0, \\alpha^{-1} \\mathbf I)$$\nwith $\\alpha = 0.2$. We starts in row 1 with this prior and at this point there are no data and the likelihood is undefined while every possible linear (line) hypothesis is feasible as represented by the red lines. In row 2, a data point arrives and the the Bayesian update takes place: the previous row posterior becomes the prior and is multiplied by the current likelihood function. The likelihood function and the form of the math behind the update are as shown in Bishop\u0026rsquo;s book in section 3.3. Here we focus on a pictorial view of what is the update is all about and how the estimate of the posterior distribution $p(\\mathbf w | \\mathbf y)$ ultimately (as the iterations increase) it will be ideally centered to the ground truth ($\\bm a$).\nInstructive example of Bayesian learning as data points are streamed into the learner. Notice the dramatic improvement in the posterior the moment the 2nd data point arrives. Why is that?\nBayesian vs Maximum Likelihood In the linear regression section we have seen a simple supervised learning problem that is specified via a joint distribution $\\hat{p}_{data}(\\bm x, y)$ and are asked to fit the model parameterized by the weights $\\mathbf w$ using ML. Its important to view pictorially perhaps the most important effect of Bayesian update:\n In ML the $\\mathbf{w}$ is treated as a known quantity with an estimate $\\hat{\\mathbf{w}}$ that has a mean and variance resulting from the distribution of $y$. In the Bayesian setting, we are integrating over the distribution of $\\mathbf{w}$ given the data i.e. we are not making a point estimate of $\\mathbf{w}$ but we marginalize out $\\mathbf{w}$. $$p(\\mathbf{w}|y) = \\frac{p(y|\\mathbf{w}) p(\\mathbf{w})}{\\int p(y|\\mathbf{w}) p(\\mathbf{w}) d\\mathbf{w}}$$  We get at the end a posterior (predictive) distribution rather than a point estimate. As such it can capture the effects of sparse data producing more uncertainty via its covariance in areas where there are no data as shown in the following example which is exactly the same sinusoidal dataset fit with Bayesian updates and Gaussian basis functions.    ML frameworks have been enhanced recently to deal with Bayesian approaches and approximations that make such approaches feasible for both classical and deep learning. TF.Probability and PyTorch Pyro are examples of such enhancements.\n This example is instructive beyond the habit of having coin flip examples in every textbook in probability theory and statistics. It is useful to understand the conjugate prior distribution being discussed in Bishop's section 2.1.1 and Figure 3 that the code above replicates. Armed with this understanding, we can now treat the Bayesian update for linear regression as described in the [linear regression section](/docs/lectures/regression/linear-regression). -- "});index.add({'id':41,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/recursive-state-estimation/','title':"Recursive State Estimation",'content':"Recursive State Estimation In Inference in Graphical Models section we have seen how sequential data belonging to just two evidential variables (captured via $p(x,y)$) can be treated by probabilistic models to infer (reason) about values of the posterior. Now we will expand on two fronts:\n  Introduce the concept of state $s$ that encapsulates multiple random variables and consider dynamical systems with non-trivial non-linear dynamics (state transition models) common in robotics, medical diagnosis and many other fields.\n  Introduce the time index $t$ explicitly in the aforementioned state evolution as represented via a graphical model.\n  The perception subsystem, that processes sensor data produces noisy estimates (object detections etc.) - these can be attributed to the algorithmic elements of the subsystem or to imperfections in the sensors themselves. The model that captures the perception pipeline from sensors to estimates will be called measurement model or sensor model. So in summary we have two abstractions / models that we need to be concerned about: the transition model of the environment state and the sensor model.\nSuch expansion, will allow us to form using the Bayes rule, perhaps one of the most important contributions to the probabilistic modeling of dynamical systems: the recursive state estimator also known as Bayes filter that affords the agent the ability to maintain an internal belief of the current state of the environment.\nBayes Filter We are introducing this algorithm, by considering a embodied agent (a robot) that moves in an environment characterized by the so called Environment Type 4 in the taxonomy presented in the agent chapter.\nAgent belief and environment interactions\nThe state of such environment contain variables that capture dynamics such as pose (6D) that includes location and orientation, agent velocity, other objects poses, etc., as well as static state variables such as location of obstacles, walls etc. Most practical algorithms for state estimation assume that the stochastically evolving environment is not affected from state variables prior to $s_t$. This is the Markovian assumption and is key in making such algorithms tractable. Note that the assumption does not constraint the actual time internal that it is impactful for the future as we are free to define anyway we want the state $s_t$. It may for example use a super-state that consists of two states in corresponding time intervals $s_t=[s_{t-1}, s_t]$. We call this the Markov order - in this case the order is two. In the figure below you can see the PGM that corresponds to the Markov assumption.\nDynamic Bayesian Network that characterizes the Markov evolution of states, measurements and controls - in the text we use for states the letter $s$ instead of $x$ and for actions the letter $a$ instead of $u$.\nThe above graph decomposes as follows:\n$$p(z_t|s_{0:t}, z_{1:t}, a_{1:t})=p(z_t|s_t)$$ $$p(s_t|s_{1:t-1}, z_{1:t}, a_{1:t})=p(s_t|s_{t-1}, a_t)$$\nIn the following we will use $z_{t_1:t_2}$ to represent sensing estimates of the perception subsystem acquired between $t_1$ and $t_2$. The measurement or sensor model is given by the conditional probability distribution $p(z_t|x_t)$. Note a couple of important points: as measurements arrive over time, the knowledge of the agent increases and there may not be dependency on time for the measurement model.\nWe will also use the conditional probability to represent the state transition $p(s_t | s_{t-1}, a_t)$ where $a_t$ is the control action variable that the agent executes causing a state change in the environment. By convention, we execute first a control action $a_1$ and then measure $z_1$.\nThe belief is the posterior distribution over the state $s_t$ conditioned on all past measurements and actions.\n$$\\mathtt{bel}(s_t) = p(s_t | z_{1:t}, a_{1:t})$$\nIt would also be useful to define the belief just after we took action $a_t$ but before considering the measurement $z_t$\n$$\\mathtt{\\hat{bel}}(s_t) = p(s_t | z_{1:t-1}, a_{1:t})$$\nThe Bayes filter is a recursive algorithm that involves two steps:\n(a) the prediction step that estimates the belief $\\mathtt{\\hat{bel}}(s_t)$ from the belief of the previous recursion $\\mathtt{bel}(s_{t-1})$\n(b) the _measurement update_ step that that weighs the belief $\\mathtt{\\hat{bel}}(s_t)$ with the probability that measurement $z_t$ was observed.\nBayes Filter\n $\\mathtt{bel}(s_t)$ = bayes_filter($\\mathtt{bel}(s_{t-1}), a_t, z_t)$\nfor all $s_t$ do:\n$→ \\mathtt{\\hat{bel}}(s_t) = \\int p(s_t | a_t, s_{t-1}) \\mathtt{bel}(s_{t-1}) ds_{t-1}$ (prediction)\n$→ \\mathtt{bel}(s_t) = \\eta p(z_t | s_t) \\mathtt{\\hat{bel}}(s_t)$ (measurement update)\nendfor\n To illustrate how the Bayes filter is useful, lets look at a practical example. This example was borrowed from Sebastian Thrun\u0026rsquo;s book, \u0026ldquo;Probabilistic Robotics\u0026rdquo;, MIT Press, 2006.\nDoor state estimation The problem we are considering is estimating the state of a door using an agent (robot) equipped with a monocular camera.\nFor simplicity lets assume that the door can be in any of two possible states (open or closed) and that the agent does not know the initial state of the door. Therefore initially, its beliefs are:\n$$\\mathtt{bel}(s_0=open) = 0.5$$ $$\\mathtt{bel}(s_0=closed) = 0.5$$\nMeasurement Model No real agent has ideal sensing abilities so the sensor or measurement model is noisy and lets assume for simplicity that its given by:\n   Description Probabilistic Model     if its open, agent can sense it as such with prob 60% $p(z_t = sense-open | s_t = open) = 0.6$   if its closed, agent can sense it as such with prob 40% $p(z_t = sense-closed | s_t = open) = 0.4$   if its closed, agent senses it open with prob 20% $p(z_t = sense-open | s_t = closed) = 0.2$   if its closed, agent can sense it as such with prob 80% $p(z_t = sense-closed | s_t = closed) = 0.8$    The values in the measurement model above are not necessarily chosen randomly as computer vision algorithms (or LIDAR) may find it easier to detect a closed door from an open door, since with an open door the camera sees the clutter inside the room and the LIDAR may confuse the clutter returns with a closed door.\nTransition Model Lets also assume that the agent is using a arm manipulator to push the door open if its closed. Note So we have the following transition distribution:\n   Transition description Probabilistic Finite State Machine     if its open, a push leaves it open $p(s_t = open | a_t=push, s_{t-1} = open) = 1$   if its open, a push does not close it $p(s_t = closed | a_t=push, s_{t-1} = open) = 0$   if its closed, a push opens it with probability 80% $p(s_t = open | a_t=push, s_{t-1} = closed) = 0.8$   if its closed, a push leaves it closed with probability 20% $p(s_t = closed | a_t=push, s_{t-1} = closed) = 0.2$   if its open, doing nothing leaves it open $p(s_t = open | a_t=inaction, s_{t-1} = open) = 1$   if its open, doing nothing does not close it $p(s_t = closed   if its closed, doing nothing does not open it $p(s_t = open   if its closed, doing nothing leaves it closed $p(s_t = closed    As we mentioned before, by convention the agent first acts and then senses. If you reverse sensing and action you arrive in the same equations with just some index differences.\nLets assume that at $t=1$, the agent takes no action but senses the door is open. The two steps of RSE are as follows:\nRecursive State Estimation at $t=1$ - Step 1: Prediction $$\\mathtt{\\hat{bel}}(s_1) = \\int p(x_1 | a_1, s_0) ds_0 = \\sum_{s_0} p(s_1 | a_1, s_0) \\mathtt{bel}(s_0)$$ $$ = p(s_1 | a_1 = inaction, s_0 = open) \\mathtt{bel}(s_0=open) + p(s_1 | a_1 = inaction, s_0 = closed) \\mathtt{bel}(s_0=closed)$$\nFor all possible values of the state variable $s_1$ we have\n$$\\mathtt{\\hat{bel}}(s_1 = open) = 1 * 0.5 + 0 * 0.5 = 0.5$$\nThe fact that the belief at this point equals the prior belief (stored in the agent) is explained from the fact that inaction shouldn\u0026rsquo;t change the environment state and the environment state does not change itself over time in this specific case.\nRecursive State Estimation at $t=1$ - Step 2: Measurement Update In this step we are using the perception subsystem to adjust the belief with what it is telling us:\n$$ \\mathtt{bel}(s_1) = \\eta p(z_1 = sense-open| s_1) \\mathtt{\\hat{bel}}(s_1) $$\nFor the two possible states at $t=1$ we have\n$$\\mathtt{bel}(s_1=open) = \\eta p(z_1 = sense-open| s_1=open) \\mathtt{\\hat{bel}}(s_1=open)$$ $$ = \\eta 0.6 * 0.5 = \\eta 0.3 $$\n$$\\mathtt{bel}(s_1=closed) = \\eta p(z_1 = sense-open| s_1=closed) \\mathtt{\\hat{bel}}(s_1=closed)$$ $$ = \\eta 0.2 * 0.5 = 0.1$$\nThe normalizing $\\eta$ factor can now be calculated: $\\eta = 1/(0.3 + 0.1) = 2.5$.\nTherefore:\n$$\\mathtt{bel}(s_1=open) = 0.75$$\n$$\\mathtt{bel}(s_1=closed) = 0.25$$\nIn the next time step lets assume that the agent pushes the door and senses that its open. Its easy to verify that\n$$\\mathtt{\\hat{bel}}(s_2 = open) = 0.95$$ $$\\mathtt{\\hat{bel}}(s_2 = closed) = 0.05$$\n$$\\mathtt{bel}(s_2 = open) = 0.983$$ $$\\mathtt{bel}(s_2 = closed) = 0.0017$$\nThis example, although simplistic is indicative of the ability of the Bayes filter to incorporate perception and action into one framework. Although the example was for an embodied agent with a manipulator, the notion of action is optional. Beliefs can be recursively updated even if the action is not taken explicitly by the agent. Your cell phones have the ability to localize themselves using exactly the same Bayesian filter with different sensing (RF signals) despite the fact that they don\u0026rsquo;t move by themselves but are carried by you in their environment.\n"});index.add({'id':42,'href':'/cs-gy-6613-spring-2020/docs/lectures/drl-model/vae/','title':"Generative Modeling and Continuous Variational Auto Encoders (VAE)",'content':""});index.add({'id':43,'href':'/cs-gy-6613-spring-2020/docs/lectures/pgm/localization/','title':"Localization and Tracking",'content':"Localization and Tracking In the recursive state estimation section we have seen the formulation of the Bayes filter and its application in a simple problem of trying to maintain a latent (internal to the agent) belief about the state of the environment $s$ given measurements $z$ and agent actions $a$. Here we are closing the loop: we started with the perception system giving us poses of objects it detects, effectively localizing the objects at the pixel coordinate system (within the image). Now we need to use the Bayes filter to localize the object globally as well as to be able to track it, since it can also move across space, so that we can recognize it over time as the same unique object and be able to assign a symbol to it.\nIn order to localize the object globally we need to localize the agent globally and so we will focus here on a situation where the initial pose of the agent is unknown as it is initially placed somewhere in its environment, but it lacks knowledge of where it is. When we have the global pose of the sensor, for example a camera, we can use straightforward techniques for the global localization of objects e.g. calibrated stereo reconstruction or triangulation, section 14.6.\nEven without global localization we can track an object as it moves within the viewpoint of the agent for the sole purpose to predict its subsequent location within the image and assign a local instance identity to it. For example as an agent moves in the space surrounding the object, its perception subsystem makes a memoryless detection decision but its probabilistic reasoning subsystem should stitch such decisions together to reason as to the object\u0026rsquo;s identity over time: is it the same object that was seen a frame ago? By knowing the pose of the agents sensors (a camera) we should be able to make such inferences.\nLocalization GPM for localization of mobile agent - notice the map $m$ is known. In the text we use for states the letter $s$ instead of $x$ and for actions the letter $a$ instead of $u$.\nFor localization we are also given a map $m$ that can take multiple shapes. Most common maps are as shown below:\nMap assumed known for agent localization - notice that beliefs may be ambiguous for certain maps and locations\nThe Bayes filter for the localization problem must consider the map and it becomes:\nBayes Filter for Localization and Tracking\n $\\mathtt{bel}(s_t)$ = bayes_filter($\\mathtt{bel}(s_{t-1}), a_t, z_t, m)$\nfor all $s_t$ do:\n$→ \\mathtt{\\hat{bel}}(s_t) = \\int p(s_t | a_t, s_{t-1}, m) \\mathtt{bel}(s_{t-1}) ds_{t-1}$ (prediction)\n$→ \\mathtt{bel}(s_t) = \\eta p(z_t | s_t, m) \\mathtt{\\hat{bel}}(s_t)$ (measurement update)\nendfor\n What distinguishes localization from tracking is that for localization, the initial belief is:\n$$ \\mathtt{be}(s_0) = 1 / |S|$$\nwhere $|S|$ is the number of states (poses) that the robot can have.\nTo illustrate the localization problem solution with probabilistic reasoning lets consider an simple environment with a corridor and three doors.\nExample environment for agent localization - a hallway with three identical doors\n  Our initial belief is obviously uniform $\\mathtt{\\hat{bel}}(s_0) = 1 / |S|$ over all poses, as illustrated in row 1.\n  The agent uses its perception that indicates that it is adjacent to one of the doors, which in effect means that the shape of $p(z_1 | s_0, m)$ is as shown in the 2nd row.\n  It multiplies its previous belief with this distribution during measurement update producing the new belief $\\mathtt{bel}(s_1)$ as shown in the 2nd row (black pdf). The resulting belief is multi-modal, reflecting the residual uncertainty of the agent at this point.\n  As the agent moves to the right, it convolves (the integral in the Bayes filter is a convolution) its belief with the transition (motion) model $p(s_2 | a_2, s_1)$. It still does not know where it is - all it knows that its belief needs to follow what its odometry sensors told it which is a move to the right by so many meters. The effect is visualized in row 3 that shows a shifted (by the odometric information) and slightly less peaked (movement reduces confidence) belief.\n  The final measurement update that multiplies $\\mathtt{bel}(s_1)$ with $p(z_2 | s_2, m)$ results into obtaining a belief $\\mathtt{bel}(s_2)$ where most of the probability mass is focused on the correct pose.\n  As the agent moves further to the right, the transition (motion) model convolutions reduce the confidence of the agent as the perception system is effectively inactive due to the lack of distinguishing features in its locale.\n  Tracking Just like localization, tracking can refer to location prediction of the agent itself or the objects that the agents perceives in its environment. For tracking we are given the initial pose $s_0$ that is approximately known. It is common to initialize the belief with a narrow Gaussian distribution around $\\hat{s}_0$.\n$$\\mathtt{bel}(s_0) = \\mathcal{N}(s_0-\\hat s_0, \\Sigma)$$\nLets look at a similar example to the localization problem:\nExample environment for tracking - a hallway with three identical and labeled doors\nThe original belief is a normal distribution centered around door 1 where the agent is initially placed. As the robot moves to the right, its belief is convolved with the Gaussian transition / motion model. The resulting belief is a shifted Gaussian of increased width. Now suppose the robot detects that it is in front of door 2. Folding this measurement probability into the robot’s belief yields the posterior shown in (c). Note that the variance of the resulting belief is smaller than the variances of both the robot’s previous belief and the observation density. This is natural, since integrating two independent estimates should make the robot more certain than each estimate in isolation. After moving down the hallway, the robot’s uncertainty in its position increases again, since the tracker continues to incorporate motion uncertainty into the robot’s belief.\nDuring the lecture we will go through a white board single dimensional problem solved via a specific parametrization of the Bayes filter with Gaussians - this filter has a famous name: Kalman filter.\n"});index.add({'id':44,'href':'/cs-gy-6613-spring-2020/docs/lectures/transfer-learning/','title':"Transfer Learning",'content':""});index.add({'id':45,'href':'/cs-gy-6613-spring-2020/docs/lectures/csp/','title':"Constraint Satisfaction Programming",'content':""});index.add({'id':46,'href':'/cs-gy-6613-spring-2020/docs/lectures/nlp/','title':"Natural Language Processing",'content':""});index.add({'id':47,'href':'/cs-gy-6613-spring-2020/docs/projects/','title':"Projects",'content':"The following projects needs to be delivered by the deadlines.\n Surface Type Classification - Due 2/23/2020 11:59pm Lifelong Learning - Robotic Vision - Due 3/29/2020 11:59pm TBD - Due 5/10/2020 11:59pm  "});index.add({'id':48,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/','title':"Background - Math for ML",'content':""});index.add({'id':49,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/','title':"Background - ML Frameworks",'content':"The Zillow App The Zillow app is based on the end to end machine learning example in Chapter 2 of Geron\u0026rsquo;s book. We can go through this end to end example during a recitation.\nAlthough the ML project checklist provided in Appendix B of Garon\u0026rsquo;s book is extensive (we will go through this list in the lecture as we go through your first ML application) for now focus on the eight steps as shown below.\nSteps in workflow (from here)\n As discussed the data pipeline is responsible for providing the training datasets if the aim is to train (or retrain) a model. For the purposes of this lecture we assume that we deal with small data aka. data fit in the memory of today\u0026rsquo;s typical workstations/laptops (\u0026lt; 16 GB). Therefore you will be given a URL from where compressed data files can be downloaded. For structured data, these files when uncompressed will be typically CSV. For unstructured they will be in various formats depending on the use case. In most instances, ML frameworks that implement training will require certain transformations to optimize the format for the framework at hand (see TFrecords in tensorflow).\n Appendix B of Garon\u0026rsquo;s book goes into more detail on the steps suggested to be followed in an end to end ML project.\n Key Questions  Is the dataset appropriate for training?   Any unexpected ranges, any range heterogeneity, any clipping? Do we face long-tails? What options do we have to glean the data?\n  What will happen if we remove the following line from the split_train_set function?\n1  shuffled_indices = np.random.permutation(len(data))     "});index.add({'id':50,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/agents-slides/','title':"Agents Slides",'content':""});index.add({'id':51,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/ai-pipelines-slides/','title':"Ai Pipelines Slides",'content':""});index.add({'id':52,'href':'/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/systems-approach-slides/','title':"Systems Approach Slides",'content':""});index.add({'id':53,'href':'/cs-gy-6613-spring-2020/docs/lectures/learning-problem/learning-problem-slides/','title':"Learning Problem Slides",'content':""});index.add({'id':54,'href':'/cs-gy-6613-spring-2020/categories/','title':"Categories",'content':""});index.add({'id':55,'href':'/cs-gy-6613-spring-2020/docs/lectures/nlp/word2vec/','title':"Creating Reasonable Embeddings",'content':""});index.add({'id':56,'href':'/cs-gy-6613-spring-2020/','title':"CS-GY-6613 Artificial Intelligence - Spring 2020",'content':"Welcome to CS-GY-6613 ! Logistics Time/location: Brooklyn Campus, Mon 6.00 PM - 8.30 PM at RGSH 315.\nCommunication: We will use Slack for all communications: announcements and questions related to lectures, assignments, and projects. All registered students with NYU email addresses can click here to join - link expires after 30 days.\nInstructor Pantelis Monogioudis, Ph.D (Bell Labs) Head of Applied ML Research Group Murray Hill, NJ\nTeaching Assistants TA\u0026rsquo;s contact will be announced as soon as I receive confirmation from HR that were hired. I am targeting two TAs for this class.\nWhat is this course about This course is all about the algorithms and methods that will enable agents that exhibit forms of intelligence and autonomy.\nGrading  Final (30%) Midterm (30%) Projects (40%)  "});index.add({'id':57,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/decision-trees/','title':"Decision Trees",'content':"Decision Trees Continuing our path into non-parametric methods, the decision tree is one of the most popular ML algorithms. Its popularity stems also from yet another attribute that is becoming very important in the application of ML/AI in mission critical industries such as health: its ability to offer interpretable results and be visualized easily.\n Note: The material below is due to (a) \u0026ldquo;ML with Random Forests and Decision Trees\u0026rdquo; by Scott Hartshorn, (b) Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning by Criminisi et.a.l.\n Introduction A Decision Tree is simply a step by step process to go through to decide a category something belongs to - in the case of classification. They are non-parametric models because they dont use a predetermined set of parameters as in parametric models - rather the tree fits the data very closely and often overfits using as many parameters are required during training.\nFor example, let’s say that you had a basket of fruit in front of you, and you were trying to teach someone who had never seen these types of fruit before how to tell them apart. How could you do it? The answer is shown pictorially below.\nA decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value).\nIf your decision tree is good, you can now pick up an unknown piece of fruit and follow the flow chart to classify it. If your decision tree is bad, you can go down the wrong path and put something in the wrong category. For instance, if you didn’t know yellow apples existed when you built your decision tree, you might have assumed that all yellow fruit are either bananas or plantains.\nIn what follows, we focus on a dataset with $m=88$ and 4 labels: Apples, Oranges, Bananas, Grapefruit. Each example has multiple features: color, width and length.\n   Fruit Colors     Apples Red, Green, or Yellow   Oranges Orange   Bananas Yellow or Green   Grapefruit  Orange or Yellow    Fruit Dataset\nIf we are to draw separation lines on feature space of length ($x_1$) and width ($x_2$) without using an ML algorithm but by hand, we probably would come up with the picture below.\nDraw by hand partition\nNow, lets try to solve the same problem using an algorithm bearing in mind that many real-life data sets might have dozens or hundreds of different features. CART algorithm One of the most popular algorithms that implement decision trees is the Classification and Regression Tree (CART) algorithm.\n NOTE: scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\n At its heart, the algorithm implements a recursive binary partitioning of the input feature space. The feature space in the example above is $\\mathbf{x} = (x_1, x_2, x_3)^T$ denoting length, width and color. Given a training dataset as usual $D={(\\mathbf x_i, y_i}$ for $i={1, \\dots m}$, we need to come up with an close to optimal partitioning for the generalization error.\nWe start at a root node that corresponds to the whole feature space (no partition) and design a test that is in its simplest form a conditional statement against a feature (a comparison if you like). Depending on the binary outcome of the test (either the input examples will satisfy the condition or not) we produce the corresponding child nodes each inheriting a subset of the input population and we repeat the exercise. The recursion stops when we reach the so called leaf nodes e.g. when the remaining examples in these nodes cannot be split further. We will come back at this terminal / leaf nodes later. An example tree and corresponding partition is shown in the two figures below.\nExample tree\nPartition for the example tree\nThe test specification consists of the variables $\\theta_i$ that are are also called thresholds as well as the specific feature $x_k$ that is being selected for the test.\n NOTE: Mind you that its not the example $\\mathbf x$ that is part of the test spec - its the feature.\n Final decision tree for the fruit classification problem\nLets see the three recursions of the algorithm as shown below.\nFirst split\nSecond split\nThird split\nThis brings up the question of how we select the test spec parameters $x_k$ and $\\theta_k$ to minimize a certain metric that is dependent on the type of the problem we deal with - classification or regression.\nSelecting the feature $x_k$ to split To gauge which feature we will choose split requires a review of certain probabilistic concepts namely the concept of entropy. We can develop on top of entropy the concept of information gain that is pictorially explained using an example as shown below\nInformation gain for two possible splits\nThe input dataset in this example has uniform distribution over classes - we have exactly the same number of points in each class. If we split the data horizontally (select feature $x_1$) this produces two sets of data. Each set is associated with a lower entropy (higher information, peakier class histograms) that is defined as usual\n$$H(D) = − \\sum_{c \\in C} p( c ) \\log(p( c ))$$\nThe entropy drops after any resonable split as we exclude labels from the original set and end up with more homogenous sets of labels. The gain of information achieved by splitting the data into two parts (1 / 2 or left / right) is computed as\n$$IG = H(D) − L(x_k, \\theta_k)$$\n$$ = H(D) - \\sum_{j \\in {1,2}} \\frac{|D^j|}{|D|} H(D^j)$$\nwhere $|.|$ is the cardinality operator i.e. the number of elements in the corresponding set and $L$ is the loss function that the algorithm is searching for its minima by trying $(x_k, \\theta_k)$ pairs. Apart from entropy we can also use another measure of impurity of the labels of the corresponding set - the Gini impurity measure. You are expected to understand the Entropy measure as it is used in other algorithms that we cover in this course. In programming, if you use sklearn, you need to explicitly change the default Gini measure to entropy.\nInference Training will result into the heuristically optimal decision tree. Inference is then straightforward as the input data will trasverse the tree and find itself into a leaf node. We can also estimate the probability that an instance belongs to a particular class k or $p(c_k|\\mathbf x)$. First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node.\nApplicability When we use decision trees for regression, the entropy is replaced with the usual MSE. We also have the possibility of parametric clustering in an unsupervised learning setting as shown below where the Gaussian distribution is being used to fit the data before and after the split. The Information Gain concept is generic enough to be applied in both discrete, continuous, supervised and unsupervised problems.\n"});index.add({'id':58,'href':'/cs-gy-6613-spring-2020/docs/','title':"Docs",'content':""});index.add({'id':59,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/tensorflow-introduction/','title':"Introduction to Tensorflow",'content':"This by no means is a tutorial introduction but rather a set of slides that we can use to describe the principle of computational graphs. This can be skipped and consulted later in the course.\nWe will cover slides #1 - #28 as shown below. The slides are from CS 20: Tensorflow for Deep Learning Research and despite the title are appropriate for Tensorflow beginners. Slides beyond #28 will be selectively consulted when we go over gradients and backpropagation.\n"});index.add({'id':60,'href':'/cs-gy-6613-spring-2020/docs/lectures/search/','title':"Lecture 7a - A* Search",'content':""});index.add({'id':61,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/','title':"Linear Algebra for Machine Learning",'content':"Linear Algebra for Machine Learning The corresponding chapter of Ian Goodfellow\u0026rsquo;s Deep Learning is essentially the background you need.\n Another resource is the book with the funny title \u0026ldquo;No Bullshit Guide to Linear Algebra\u0026rdquo; by Ivan Savov.\nKey Points We can now summarize the points to pay attention to, for ML applications. In the following we assume a data matrix $A$ with $m$ rows and $n$ columns. We also assume that the matrix is such that it has $r$ independent rows or columns, called the matrix rank.\nProjections Its important to understand this basic operator and its geometric interpretation as it is met in problems like Ordinary Least Squares but also all over ML and other fields such as compressed sensing. In the following we assume that the reader is familiar with the concept of vector spaces and subspaces.\nLet $S$ be a vector subspace of $\\R^n$. For example in $\\R^3$, $S$ are the lines and planes going through the origin. The projection operator onto $S$ implements a linear transformation: $\\Pi_S: \\R^3 →S$. We will stick to $\\R^3$ to maintain the ability to plot the operations involved. We also define the orthogonal subspace,\n$$S^\\perp ≡ \\{ \\bm w \\in \\R^3 | \\bm w ^T \\bm s = 0, ∀ \\bm s \\in S \\} $$\nThe transformation $\\Pi_S$ projects onto space $S$ in the sense that when you apply this operator, every vector $\\bm u$ in any other space results in the subspace $S$. In our example above,\n$$\\Pi_S(\\bm u) \\in S, \\forall \\bm u \\in \\R^3$$\nThis means that any components of the vector $\\bm u$ that belonged to $S^\\perp$ are gone when applying the projection operator. Effectively, the original space is decomposed into\n$$ \\R^3 = S \\oplus S^\\perp $$\nNow we can treat projections onto specific subspaces such as lines and planes passing through the origin.\nFor a line defined by a direction vector $\\bm u$\n$$l = \\{ (x,y,z) \\in \\R^3 | (x,y,z) = \\bm 0 + t \\bm u \\} $$\nwe can define the projection onto the line\nProjection of $\\bm u$ onto the line $l$\nThe space $S^\\perp ≡ l^\\perp$ is a plane since it consists of all the vectors that are perpendicular to the line. What is shown in the figure as a dashed line is simply the projection of $\\bm u$ on the $l^\\perp$ subspace,\n$$l^\\perp = \\{ (x,y,z) \\in \\R^3 | \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}^T \\bm v = 0\\} $$\nThe orthogonal space of a line with direction vector $\\bm v$ is a plane with a normal vector $\\bm v$. So when we project the $\\bm v$ on the line we get two components one is lying on the line and is the $\\Pi_l \\bm u$ and the other is the vector $\\bm w$ = $\\Pi_{l^\\perp} \\bm u = \\bm u - \\bm v = \\bm u - \\Pi_{\\bm v} \\bm u $. The vector $\\bm w$ is what remains when we remove the projected on $\\bm v$ part from the $\\bm u$.\nThe Four Fundamental Subspaces The fundamental theorem of Linear Algebra specifies the effect of the multiplication operation of the matrix and a vector ($A\\mathbf{x}$). The matrix gives raise to 4 subspaces:\n The column space of $A$, denoted by $\\mathcal{R}(A)$, with dimension $r$. The nullspace of $A$, denoted by $\\mathcal{N}(A)$, with dimension $n-r$. The row space of $A$ which is the column space of $A^T$, with dimension $r$ The left nullspace of $A$, which is the nullspace of $A^T$, denoted by $\\mathcal{N}(A^T)$, with dimension $m-r$.  The real action that the matrix performs is to transform its row space to its column space.\nThe type of matrices that are common in ML are those that the number of rows $m$ representing observations is much larger than the number of columns $n$ that represent features. We will call these matrices \u0026ldquo;tall\u0026rdquo; for obvious reasons. Let us consider one trivial but instructive example of the smallest possible \u0026ldquo;tall\u0026rdquo; matrix:\n$$\\begin{bmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\\\ a_{31} \u0026amp; a_{32} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 5 \u0026amp; 4 \\\\ 2 \u0026amp; 4 \\end{bmatrix}$$\nIn ML we are usually concerned with the problem of learning the weights $x_1, x_2$ that will combine the features and result into the given target variables $\\mathbf{b}$. The notation here is different and we have adopted the notation of many linear algebra textbooks.\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 5 \u0026amp; 4 \\\\ 2 \u0026amp; 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$$\nTo make more explicit the combination of features we can write,\n$$ x_1 \\begin{bmatrix} 1 \\\\ 5 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 0 \\\\ 4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$$\nSince $m=3 \u0026gt; n=2$, we have more equations than unknowns we in general we have no solutions - a system with $m \u0026gt; n$ will be solvable only for certain right hand sides $\\mathbf{b}$. Those are all the vectors $\\mathbf{b}$ that lie in the column space of $A$.\nIn this example, as shown in the picture $\\mathbf{b}$ must lie in the plane spanned by the two columns of $A$. The plane is a subspace of $\\mathbb{R}^m=\\mathbb{R}^3$ in this case.\nNow instead of looking at what properties $\\mathbf{b}$ must have for the system to have a solution, lets look at the dual problem i.e. what weights $\\mathbf{x}$ can attain those $\\mathbf{b}$. The right-hand side $\\mathbf{b}=0$ always allows the solution $\\mathbf{x}=0$ The solutions to $A \\mathbf{x} = \\mathbf{0}$ form a vector space - the nullspace $\\mathcal{N}(A)$. The nullspace is also called the kernel of matrix $A$ and the its dimension $n-r$ is called the nullity.\n$\\mathcal{N}(A)$ is a subspace of $\\mathbb{R}^n=\\mathbb{R}^2$ in this case. For our specific example,\n$$ x_1 \\begin{bmatrix} 1 \\\\ 5 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 0 \\\\ 4 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\nthe only solution that can satisfy this set of homogenous equations is: $\\mathbf{x}=\\mathbf{0}$ and this means that the null space contains only the zero vector and this\nTwo vectors are independent when their linear combination cannot be zero, unless both $x_1$ and $x_2$ are zero. The columns of $A$ are therefore linearly independent and they span the column space. They have therefore all the properties needed for them to constitute a set called the basis for that space and we have two basis vectors (the rank is $r=2$ in this case). The dimension of the column space is in fact the same as the dimension of the row space ($r$) and the mapping from row space to column space is in fact invertible. Every vector $\\mathbf{b}$ comes from one and only one vector $\\mathbf{x}$ of the row space ($\\mathbf{x}_r$). And this vector can be found by the inverse operation - noting that only the inverse $A^{-1}$ is the operation that moves the vector correctly from the column space to the row space. The inverse exists only if $r=m=n$ - this is important as in most ML problems we are dealing with \u0026ldquo;tall\u0026rdquo; matrices with the number of equations much larger than the number of unknowns which makes the system inconsistent (or degenerate).\nProjection onto the column space\nGeometrically you can think about the basis vectors as the axes of the space. However, if the axes are not orthogonal, calculations will tend to be complicated not to mention that we usually attribute to each vector of the basis to have length one (1.0).\nEigenvalues and Eigenvectors The following video gives an intuitive explanation of eigenvalues and eigenvectors and its included here due to its visualizations that it offers. The video must be viewed in conjunction with Strang\u0026rsquo;s introduction\n During the lecture we will go through an example from how your brain processes the sensory input generated by the voice of the lecturer(unless you are already asleep by that time) to combine optimally the sound from both your ears.\nA geometric interpretation of the eigenvectors and eigenvalues is given in the following figure:\n"});index.add({'id':62,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/optimization/','title':"Optimization and Stochastic Gradient Descent",'content':"Optimization and Stochastic Gradient Descent In this lecture we will go over concepts from Ian Goodfellow\u0026rsquo;s chapter 4 below. Stochastic gradient descent is treated also in section 5.9.\n "});index.add({'id':63,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/probability/','title':"Probability and Information Theory Basics",'content':"Book Chapters From Ian Goodfellow\u0026rsquo;s book:\n We will go through the main points during the lecture and treat also MacKay\u0026rsquo;s book (Chapter 2) that is also instructive and a much better in introducing probability concepts. If you are a visual learner, the visual information theory blog post is also a good starting point.\nKey Concepts to understand Probability The pictures below are from MacKays book and despite their conceptual simplicity they hide many questions that we will go over the lecture.\nProbability distributions Probability distribution over the letters of the English alphabet (letter 27 symbolizes space) as measured by reading the Linux FAQ document.\nJoint probability distributions Joint probability $P(x,y)$ distribution over the 27x27 possible bigrams $xy$ found in this document: https://www.tldp.org/FAQ/pdf/Linux-FAQ.pdf\nWhat is the marginal probability $P(x)$ ?\nConditional probability distribution\nConditional probability distribution over the 27x27 possible bigrams $xy$ found in this document: https://www.tldp.org/FAQ/pdf/Linux-FAQ.pdf\nAre $x$ and $y$ independent ?\nProbability Rules If H is the hypothesis governing the probabilities distributions,\nProduct or chain rule:\nThis is obtained from the definition of conditional probability:\n$P(x,y|H) = P(x | y,H)P(y | H) = P(y | x,H)P(x |H)$\nSum rule:\nThis is obtaining by rewriting of the marginal probability denition: $P(x |H) = \\sum_y P(x,y |H) = \\sum_y P(x | y,H)P(y |H)$\nKey probability distributions Multi-variate Gaussian distribution $$f_{\\mathbf X}(x_1,\\ldots,x_k) = \\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right)}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma|}}$$ where where \u0026lt;${\\mathbf x}$ is a real \u0026lsquo;n\u0026rsquo;-dimensional column vector and $|\\boldsymbol\\Sigma|\\equiv \\operatorname{det}\\boldsymbol\\Sigma$ is the determinant of $\\boldsymbol\\Sigma$.\nApart from the definition, you need to connect the geometric interpretation of the bivariate Gaussian distribution to the eigendecomposition in the linear algebra lecture as shown in the Figure 2.7 of Bishop:\nSuch geometric interpretations will be very useful when we study dimensionality reduction via Principal Component Analysis (PCA).\nProbabilistic Modeling   The whole purpose of probabilistic modeling is to introduce uncertainty into our problem statement. There are three types of uncertainties:\n Inherent stochasticity - e.g. impact of wind in self-driving car control systems at moderate to high speed. Incomplete observability - e.g. sensor imperfections causing loss of sensing information Incomplete modeling - e.g. models and algorithms that are not implementable to an analog world and need to be discretized.    Probabilities can be used in two ways.\n Probabilities can describe frequencies of outcomes in random experiments Probabilities can also be used, more generally, to describe degrees of belief in propositions that do not involve random variables. This more general use of probability to quantify beliefs is known as the Bayesian viewpoint. It is also known as the subjective interpretation of probability, since the probabilities depend on assumptions.    The Bayesian theorem is the cornerstone of probabilistic modeling. If $\\mathbf{\\theta}$ denotes the unknown parameters, $D$ denotes the dataset and $\\mathcal{H}$ denotes the hypothesis space - the model we have seen in the learning problem chapter.\n  $$ P(\\mathbf{\\theta} | D, \\mathcal{H}) = \\frac{P( D | \\mathbf{\\theta}, \\mathcal{H}) P(\\mathbf{\\theta} | \\mathcal{H}) }{ P(D|\\mathcal{H})} $$\nThe Bayesian framework allows the introduction of priors from a wide variety of sources (experts, other data, past posteriors, etc.) For example,a medical patient is exhibiting symptoms x, y and z. There are a number of diseases that could be causing all of them, but only a single disease is present. A doctor (the expert) has beliefs about which disease, but a second doctor may have slightly different beliefs.\n NOTE: The Probabilistic Programming \u0026amp; Bayesian Methods for Hackers book is one of the best resources out there containing practical python examples. In addition they have been recoded recently to work in Tensorflow Probability an industrial-strength framework that can bring together Deep Learning and domain-specific probabilistic modeling. The book cant match the rigorousness of Bishop\u0026rsquo;s book but it offers a good treatment on problems and use cases and should be considered complimentary.\n Information-theoretic definitions Entropy An outcome $x_t$ carries information that is a function of the probability of this outcome $P(x_t)$ by,\n$I(x_t) = \\ln \\frac{1}{P(x_t)} = - \\ln P(x_t)$\nThis can be intuitively understood when you compare two outcomes. For example, consider someone is producing the result of the vehicular traffic outside of Holland tunnel on Monday morning. The information that the results is \u0026ldquo;low\u0026rdquo; carries much more information when the result is \u0026ldquo;high\u0026rdquo; since most people expect that there will be horrendous traffic outside of Holland tunnel on Monday mornings. When we want to represent the amount of uncertainty over a distribution (i.e. the traffic in Holland tunnel over all times) we can take the expectation over all possible outcomes i.e.\n$H(P) = - \\mathbb{E} \\ln P(x)$\nand we call this quantity the entropy of the probability distribution $P(x)$. When $x$ is continuous the entropy is known as differential entropy. Continuing the alphabetical example, we can determine the entropy over the distribution of letters in the sample text we met before as,\nThis is 4.1 bits (as the $\\log$ is taken with base 2). This represents the average number of bits required to transmit each letter of this text to a hypothetical receiver. Note that we used the information carried by each \u0026ldquo;outcome\u0026rdquo; (the letter) that our source produced. If the source was binary, we can plot the entropy of such source over the probability p that the outcome is a 1 as shown below,\nThe plot simply was produced by taking the definition of entropy and applying to the binary case,\n$H(p) = - [p \\ln p - (1-p) \\ln(1-p)]$\nAs you can see the maximum entropy is when the outcome is most unpredictable i.e. when a 1 can show up with uniform probability (in this case equal probability to a 0).\nRelative entropy or KL divergence In the ML problem statement, it is evident that the job of the learning algorithm is to come up with a final hypothesis that is close to the unknown target function. In other occasions, we need to approximate a distribution by sampling from another easier to model distribution. As in ML we work with probabilities, we need to have a metric that compares two probability distributions ${P(x),Q(x)}$ in terms of their \u0026ldquo;distance\u0026rdquo; from each other (the quotes will be explained shortly). This is given by the quantity known as relative entropy or KL divergence.\n$KL(P||Q)= \\mathbb{E}[\\ln P(x) - \\ln Q(x)]$\nIf the two distributions are identical, $KL=0$ - in general however $KL(P||Q) \\ge 0$. One key element to understand is that $KL$ is not a true distance metric as its assymetric. Ensure that you understand fully the following figure and caption.\nVery close to the relative entropy is probably one of the most used information theoretic concepts in ML: the cross-entropy. We will motivate cross entropy via a diagram shown below,\nBackground for logistic regression If $\\sigma$ is a probability of an event, then the ratio $\\frac{\\sigma}{1-\\sigma}$ is the corresponding odds, the ratio of the event occurring divided by not occurring. For example, if a race horse runs 100 races and wins 25 times and loses the other 75 times, the probability of winning is 25/100 = 0.25 or 25%, but the odds of the horse winning are 25/75 = 0.333 or 1 win to 3 loses. In the binary classification case, the log odds is given by\n$$ \\mathtt{logit}(\\sigma) = \\alpha = \\ln \\frac{\\sigma}{1-\\sigma} = \\ln \\frac{p(\\mathcal{C}_1|\\mathbf{x})}{p(\\mathcal{C}_2|\\mathbf{x})}$$\nWhat is used in ML though is the logistic function of any number $\\alpha$ that is given by the inverse logit:\n$$\\mathtt{logistic}(\\alpha) = \\sigma(\\alpha) = \\mathtt{logit}^{-1}(\\alpha) = \\frac{1}{1 + \\exp(-\\alpha)} = \\frac{\\exp(\\alpha)}{ \\exp(\\alpha) + 1}$$\nand is plotted below. It maps its argument to the \u0026ldquo;probability\u0026rdquo; space [0,1].\nLogistic sigmoid (red)\nThe sigmoid function satisfies the following symmetry:\n$$\\sigma(-\\alpha) = 1 - \\sigma(\\alpha)$$\nIn addition it offers very convenient derivatives and has been used extensively in deep neural networks (for many architectures has been superceded by RELU). The derivative can be obtained as follows:\nConsider $$ f(x)=\\dfrac{1}{\\sigma(x)} = 1+e^{-x} . $$ Then, on the one hand, the chain rule gives $$ f\u0026rsquo;(x) = \\frac{d}{dx} \\biggl( \\frac{1}{\\sigma(x)} \\biggr) = -\\frac{\\sigma\u0026rsquo;(x)}{\\sigma(x)^2} , $$ and on the other hand, $$ f\u0026rsquo;(x) = \\frac{d}{dx} \\bigl( 1+e^{-x} \\bigr) = -e^{-x} = 1-f(x) = 1 - \\frac{1}{\\sigma(x)} = \\frac{\\sigma(x)-1}{\\sigma(x)} $$\nEquating the two expressions we finally obtain,\n$$\\sigma\u0026rsquo;(x) = \\sigma(x)(1-\\sigma(x))$$\n"});index.add({'id':64,'href':'/cs-gy-6613-spring-2020/docs/projects/imu-classification/','title':"Project 1 - Surface Type Classification",'content':"Project 1 - Surface Type Classification   Your first project description is published in https://www.kaggle.com/c/career-con-2019/overview\n  You must submit your assignment with the results by 11:59pm 2/23/2020. The submission will be done by sharing the github/kaggle notebook with the TA.\n  "});index.add({'id':65,'href':'/cs-gy-6613-spring-2020/docs/projects/continuous-learning/','title':"Project 2 - Continual Learning for Robotic Perception",'content':"Project 2 - Continual Learning for Robotic Perception This project is due March 29 at 11:59pm\nOne of the greatest goals of AI is building an artificial continual learning agent which can construct a sophisticated understanding of the external world from its own experience through the adaptive, goal-oriented and incremental development of ever more complex skills and knowledge. Continual learning is essential in robotics where high dimensional data streams need to be constantly processed and where naïve continual learning strategies have been shown to suffer from catastrophic forgetting.\n   CORe50 Option Rotated MNIST Option     You will use this dataset and evaluate your method for New Classes (NC) scenario. Use the dataset provided here based on this paper   Object recognition (classification) task. Object recognition (classification) task.    This is a very active area in AI right now - see here\n"});index.add({'id':66,'href':'/cs-gy-6613-spring-2020/docs/projects/project-3/','title':"Project 3",'content':""});index.add({'id':67,'href':'/cs-gy-6613-spring-2020/docs/lectures/classification/random-forests/','title':"Random Forests",'content':"Random Forests Random forests is a popular and very strong ML algorithm that belongs to what is called ensemble learning methods. As the name implies they use many classification tree learners to improve on their generalization ability. Each of the trees is called the weak learner - when grouped together they form the strong learner.\nA key aspect of decision forests is the fact that its component trees are all randomly different from one another. This leads to de-correlation between the individual tree predictions and, in turn, to improved generalization. Forest randomness also helps achieve high robustness with respect to noisy data. Randomness is injected into the trees during the training phase. Two of the most popular ways of doing so are:\n Random training data set sampling (when sampling is performed with replacement, this is called bagging), Randomized node optimization.  These two techniques are not mutually exclusive and could be used together.\nIn the sklearn library, the Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity. In addition, we can make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds - also known as Extra-Trees.\nInference Example of three trees receiving the instance $\\mathbf x$ (shown as $\\mathbf v$ in the figure)\nDuring testing the same unlabelled test input data $\\mathbf x$ is pushed through each component tree. At each internal node a test is applied and the data point sent to the appropriate child. The process is repeated until a leaf is reached. At the leaf the stored posterior $p_t(c|\\mathbf x)$ is read off. The forest class posterior $p(c|\\mathbf x)$ is simply the average of all tree (T) posteriors.\n$p(c|\\mathbf x) = \\frac{1}{T} \\sum_{t=1}^T p_t(c|\\mathbf x) $\nImpact of random forest parameters Given a training dataset with two classes (a), different training trees produce different partitions and thus different leaf predictors. The colour of tree nodes and edges indicates the class probability of training points going through them (b) In testing, increasing the forest size $T$ produces smoother class posteriors.\n"});index.add({'id':68,'href':'/cs-gy-6613-spring-2020/tags/','title':"Tags",'content':""});index.add({'id':69,'href':'/cs-gy-6613-spring-2020/docs/lectures/mdp/','title':"The Agent - Environment Interface",'content':""});index.add({'id':70,'href':'/cs-gy-6613-spring-2020/docs/lectures/ml-math/netflix/','title':"The Netflix Prize and Singular Value Decomposition",'content':"Introduction The following are based on the winning submission paper as well as their subsequent publication.\nThe Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users or the films being identified except by numbers assigned for the contest.\nThe competition was held by Netflix, an online DVD-rental and video streaming service, and was open to anyone who is neither connected with Netflix (current and former employees, agents, close relatives of Netflix employees, etc.) nor a resident of certain blocked countries (such as Cuba or North Korea).[1] On September 21, 2009, the grand prize of US $ 1,000,000 was given to the BellKor\u0026rsquo;s Pragmatic Chaos team which bested Netflix\u0026rsquo;s own algorithm for predicting ratings by 10.06%\nThis competition is instructive since:\n Collaborative filtering models try to capture the interactions between users and items that produce the different rating values. However, many of the observed rating values are due to effects associated with either users or items, independently of their interaction. A prime example is that typical CF data exhibit large user and item biases – i.e., systematic tendencies for some users to give higher ratings than others, and for some items to receive higher ratings than others. Observing the posted improvements in RMSE over time, the competition has become of little business value to Netflix after a while. This means that it was unlikely that any minute improvement to RMSE (e.g. 0.1%) will translate to additional revenue. The 10% improvement goal was a lucky number after all. Netflix had no clue as to if this was the right number when they defined the terms. Any small deviation from this number, would have made the competition either too easy or impossibly difficult.  Problem statement You are given the following dataset structure (will be explained in class) shown below,\nAssuming that the rating matrix A is an m x n matrix with m users (500K) and n items (17K movies), this matrix is extremely sparse - it has only 100 million ratings, the remaining 8.4 billion ratings are missing (about 99% of the possible ratings are missing, because a user typically rates only a small portion of the movies).\nGiven the very large data matrix it was only expected that competitors attempted to work out some form of dimensionality reductions and as it turns out this was the basis for the winning algorithm. If you recall the premise of SVD from linear algebra, it is a decomposition that can work with rectangular matrices and can result into a decomposition of the following kind:\n$$A = U \\Sigma V^†$$\nwhere $\\mathbf{U}$ is a $m \\times m$ real unitary matrix, $\\mathbf{\\Sigma}$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $\\mathbf{V}$ is a $n \\times n$ real unitary matrix. Remember that a unitary matrix $U$ is a matrix that its conjugate transpose $U^†$ is also its inverse - its the complex analog of the orthogonal matrix and we have by definition $UU^†=I$.\nThe columns of $U$ are eigenvectors of $AA^T$, and the columns of $V$ are eigenvectors of $A^TA$. The $r$ singular values on the diagonal of $\\Sigma$ are the square roots of the nonzero eigenvalues of both $AA^T$ and $A^TA$.\nIt is interesting to attribute the columns of these matrices with the four fundamental subspaces:\n The column space of $A$ is spanned by the first $r$ columns of $U$. The left nullspace of $A$ are the last $m-r$ columns of $U$. The row space of $A$ are the first $r$ columns of $V$. The nullspace of $A$ are the last $n-r$ columns of V.  We can write the SVD as,\n$$A = U \\sqrt{\\Sigma} \\sqrt{\\Sigma} V^†$$\nand given the span of the subspaces above we can now intuitively think what the terms $\\mathbf{p}_u = U \\sqrt{\\Sigma}$ and $\\mathbf{q}_i = \\sqrt{\\Sigma} V^†$ represent.\nSVD decomposition reveals latent features weighted by the underlying singular values of the data matrix\nThe first term represents the column space aka. it provides a representation of all inter-user factors (also called latent features, latent means hidden). The second term represents the row space aka. it provides a representation of all inter-movie factors. Which brings us to the major point of what the $\\sqrt{\\Sigma}$ is doing to both terms. It represents the significance of those factors and therefore we can very easily use the singular values it contains to \u0026ldquo;compress\u0026rdquo; our representation by selecting the $k$ largest singular values and ignoring the rest.\nGiven these vectors of factors we can now use them to predict the rating:\n*Prediction of a rating is the product between user latent features and movie latent features matrices.\nFor an intuitive description of the SVD solution see here.\n"});index.add({'id':71,'href':'/cs-gy-6613-spring-2020/docs/projects/env/','title':"Your Programming Environment",'content':"Your Programming Environment Starting Jupyter in Google Colab The runtime performance will greatly improve for some projects using the free GPU resources provided by Google Colab. In this course we will make use of these facilities - the good news is that you have an account in Google Colab as most of you have a google account. If not go ahead and create one to be able to login into Google colab. You will need Google Colab for all your projects so that you can demonstrate that your results can be replicated. In addition Colab has many features that come handy.\nI heavily borrowed from Geron\u0026rsquo;s book for the following.\nSetup Anaconda Python When using Anaconda, you need to create an isolated Python environment dedicated to this course. This is recommended as it makes it possible to have a different environment for each project, with potentially different libraries and library versions:\n$ conda create -n cs6613 python=3.6 anaconda $ conda activate cs6613  This creates a fresh Python 3.6 environment called cs6613 (you can change the name if you want to), and it activates it. This environment contains all the scientific libraries that come with Anaconda. This includes all the libraries we will need (NumPy, Matplotlib, Pandas, Jupyter and a few others), except for TensorFlow, so let\u0026rsquo;s install it:\n$ conda install -n cs6613 -c conda-forge tensorflow  This installs the latest version of TensorFlow available for Anaconda (which is usually not the latest TensorFlow version) in the cs6613 environment (fetching it from the conda-forge repository). If you chose not to create an cs6613 environment, then just remove the -n cs6613 option.\nNext, you can optionally install Jupyter extensions. These are useful to have nice tables of contents in the notebooks, but they are not required.\n$ conda install -n cs6613 -c conda-forge jupyter_contrib_nbextensions  Kaggle Assuming you have activated the cs6613 conda environment, follow the directions here to install the Kaggle command line interface (CLI). You will need Kaggle for all your projects. You guessed it right - all the projects in this course are in fact Kaggle competitions. Not only you will get to compete (your ranking relative to others does not matter per se), but as you improve your knowledge over time you can revisit these competitions and see how your score improves.\nYou are all set! Next, jump to the Starting Jupyter section.\nStarting Jupyter locally If you want to use the Jupyter extensions (optional, they are mainly useful to have nice tables of contents), you first need to install them:\n$ jupyter contrib nbextension install --user  Then you can activate an extension, such as the Table of Contents (2) extension:\n$ jupyter nbextension enable toc2/main  Okay! You can now start Jupyter, simply type:\n$ jupyter notebook  This should open up your browser, and you should see Jupyter\u0026rsquo;s tree view, with the contents of the current directory. If your browser does not open automatically, visit localhost:8888. Click on index.ipynb to get started!\nNote: you can also visit http://localhost:8888/nbextensions to activate and configure Jupyter extensions.\nGit / Github Git is the defacto standard when it comes to code version control. Learning basic git commands takes less than half an hour. However, to install git and understand the principle behind git, please go over Chapters 1 and 2 of the ProGit book.\nAs we have discussed in class you need to be able to publish your work in Github so you need to create a Github account. Then you will use the git client for your operating system to interact with github and iterate on your projects. You may be using Kaggle or Colab hosted notebooks but the underlying technology that powers such web-frontends when it comes to committing the code and seeing version numbers in your screen is git.\nIn addition, almost no data science project starts in vacuum - there is almost always software that will be inherited to be refined.\n"});})();