<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Object Detection"><meta property="og:title" content="Object Detection" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/object-detection/" />

<title>Object Detection | CS-GY-6613 Spring 2020</title>
<link rel="icon" href="/cs-gy-6613-spring-2020/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/cs-gy-6613-spring-2020/book.min.ccf2e3f83640a9c2266488b947be5b9ddbdea671b0de107d1d379f1d6f6d0408.css" integrity="sha256-zPLj&#43;DZAqcImZIi5R75bndvepnGw3hB9HTefHW9tBAg=">


<script defer src="/cs-gy-6613-spring-2020/en.search.min.5e1853e93d0d8433e8e62634379b6475cf6363e5a70386ace4f18e6ebff87f4c.js" integrity="sha256-XhhT6T0NhDPo5iY0N5tkdc9jY&#43;WnA4as5PGObr/4f0w="></script>

<link rel="alternate" type="application/rss+xml" href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/object-detection/index.xml" title="CS-GY-6613 Spring 2020" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/cs-gy-6613-spring-2020"><img src="/cs-gy-6613-spring-2020/tandon_long_black.png" alt="Logo" /><span>CS-GY-6613 Spring 2020</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





  

  
  





 
  
    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/syllabus/" >
      Syllabus
  </a>

</li>
      
    
      
        

  <li >
    
      <span>Lecture 1 - Introduction to AI</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/" >
      Course Introduction
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/" >
      A systems approach to AI
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Systems Approach Slides</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/" >
      The Way of Working in AI
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Ai Pipelines Slides</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/" >
      Intelligent Agents and Representations
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Agents Slides</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/learning-problem/" >
      Lecture 2a - The Learning Problem
  </a>


    

    




  
  <ul>
    
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 2b - Regression</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/regression/linear-regression/" >
      Linear Regression
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/" >
      Lecture 2c - Linear Classification
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/knn/" >
      k-Nearest Neighbors (kNN) Classification
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/perceptron/" >
      The Perceptron
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/logistic-regression/" >
      Logistic Regression
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/kernels/" >
      Kernels and the Kernel Trick
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/clustering/" >
      K-means Clustering
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/" >
      Support Vector Machines
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/face-recognition/" >
      Face Recognition - SVM Case Study
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/mnist/" >
      MNIST Classification - SVM Case Study
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/iris/" >
      Iris Classification - SVM Case Study
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/decision-trees/" >
      Decision Trees
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/random-forests/" >
      Random Forests
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/" >
      Lecture 3 - Deep Neural Networks
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-intro/" >
      Introduction to Backpropagation
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-dnn/" >
      Backpropagation in Deep Neural Networks
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Regularization in Deep Neural Networks</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/fashion-mnist-case-study/" >
      Fashion MNIST Case Study
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 4a - Convolutional Neural Networks</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-intro/" >
      Introduction to Convolutional Neural Networks
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-arch/" >
      CNN Architectures
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-sizing/" >
      CNN Sizing
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Attention Mechanisms in CNNs</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/" >
      Lecture 4b - Sequences and Recurrent Neural Networks (RNN)
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/simple-rnn/" >
      Simple RNNs and their Backpropagation
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/lstm/" >
      The Long Short-Term Memory (LSTM) Cell Architecture
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 5 - Scene Understanding</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/scene-understanding-intro/" >
      Introduction to Scene Understanding
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/feature-extraction-resnet/" >
      Feature Extraction via Residual Networks
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/object-detection/"  class="active">
      Object Detection
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Semantic Segmentation</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 6 - Probabilistic Graphical Models</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/pgm-intro/" >
      Introduction to Probabilistic Reasoning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/bayesian-inference/" >
      Inference in Graphical Models
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/recursive-state-estimation/" >
      Recursive State Estimation
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/localization/" >
      Localization and Tracking
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 7 - Planning</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Planning Approaches</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Route Planning</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Behavioral Planning</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Motion Prediction</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Trajectory Generation</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 8 - Markov Decision Processes</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/mdp/mdp-slides/" >
      Mdp Slides
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>The Agent - Environment Interface</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Generative Modeling and Continuous Variational Auto Encoders (VAE)</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Natural Language Processing</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Creating Reasonable Embeddings</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/" >
      Projects
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/imu-classification/" >
      Project 1 - Surface Type Classification
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/continuous-learning/" >
      Project 2 - Continual Learning for Robotic Perception
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/semantic-code-search/" >
      Project 3 - Semantic Code Search
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/projects/env/" >
      Your Programming Environment
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Background - Math for ML</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/" >
      Linear Algebra for Machine Learning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/optimization/" >
      Optimization and Stochastic Gradient Descent
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/probability/" >
      Probability and Information Theory Basics
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/netflix/" >
      The Netflix Prize and Singular Value Decomposition
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/" >
      Background - ML Frameworks
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/numpy-pandas/" >
      Numerical Python (Numpy/Scipy and Pandas) Tutorials
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/tensorflow-introduction/" >
      Introduction to Tensorflow
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Drafts</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Constraint Satisfaction Programming</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Transfer Learning</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  











</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/cs-gy-6613-spring-2020/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Object Detection</strong>

  <label for="toc-control">
    <img src="/cs-gy-6613-spring-2020/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#object-detection">Object Detection</a>
      <ul>
        <li><a href="#region-cnn-rcnn">Region CNN (RCNN)</a>
          <ul>
            <li><a href="#region-proposals">Region Proposals</a>
              <ul>
                <li><a href="#graph-based-segmentation">Graph-based segmentation</a></li>
                <li><a href="#grouping">Grouping</a></li>
              </ul>
            </li>
            <li><a href="#cnn-features--and-svm-classification">CNN Features  and SVM Classification</a></li>
            <li><a href="#training">Training</a></li>
          </ul>
        </li>
        <li><a href="#fast-rcnn">Fast RCNN</a></li>
        <li><a href="#faster-rcnn">Faster RCNN</a></li>
      </ul>
    </li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      
<article class="markdown"><h1 id="object-detection">Object Detection</h1>
<p>In the <a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/scene-understanding-intro/">introductory</a> section, we have seen examples of what object detection is. In this section we will treat the detection <em>pipeline</em> itself, summarized below:</p>
<p><img src="images/object-detection-e2e.png" alt="object-detection-e2e">
<em>Object detection pipeline</em>.</p>
<p>Work on object detection spans 20 years and is impossible to cover every algorithmic approach in this section - the interested reader can trace these developments by reading in <a href="https://arxiv.org/abs/1905.05055">this</a> paper.</p>
<p><img src="images/object-detection-roadmap.png#center" alt="object-detection-roadmap"></p>
<p>As expected, since 2014, deep learning has surpassed classical ML in the detection competitions - we therefore focus only on such architectures. More specifically we will be focusing on the so called <em>two stage</em> detectors that employ two key ingredients:</p>
<ol>
<li>Recognition using <em>regions</em> that we will explain shortly.</li>
<li><a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-intro/">CNNs</a> that we covered earlier.</li>
</ol>
<p>Object detection, involves three main stages: the feature extraction stage, the classification stage and the detection or localization stage. In the literature the feature and classification stages are counted as one, called the classification stage and people refer to such architecture as _two stage.</p>
<p>We also need to insert an additional requirement: to be able to detect objects in almost real time (20 frames per second) - a significant subset of what we call mission critical applications require it. Therefore will focus a specific family that is considered to be the canonical CNN architecture for detection - the family of Region CNNs.</p>
<h2 id="region-cnn-rcnn">Region CNN (RCNN)</h2>
<h3 id="region-proposals">Region Proposals</h3>
<p>We can think about the detection problem as a classification problem of <em>all possible portions</em> (windows/masks) of the input image since an object can be located at any position and scale in the image. It is natural to search therefore everywhere and an obvious method to generate region proposals, is to slide various width-height ratio windows slide around the image and using a metric to declare that the window contains one or more blob of pixels. Obviously, such method is computationally infeasible and we need think of how to reduce this number by having some means of gauging where to look in the image.</p>
<p>RCNN can accommodate a number of efficient algorithms that can produce region proposals. The baseline RCNN employs instead <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf"><em>selective search via hierarchical grouping</em></a>.</p>
<p><img src="images/hierarchical-grouping-alg.png#center" alt="hierarchical-grouping-alg"></p>
<p>The algorithm contains another algorithm that segments the image into initial regions.</p>
<h4 id="graph-based-segmentation">Graph-based segmentation</h4>
<p>We perform segmentation in the image using an <a href="http://cs.brown.edu/people/pfelzens/segment/">efficient graph-based algorithm</a> to obtain the set $R=\{r_1, \dots, r_n \}$ of initial regions. The segmentation algorithm starts by formulating the image as a graph.</p>
<p>Let G = (V, E) be an undirected graph with vertices $v_i \in V$ , the set of elements to be segmented, and edges $(v_i, v_j) ∈ E$ corresponding to pairs of neighboring vertices. Each edge has a corresponding weight $w((v_i, v_j ))$, which is a non-negative measure of the <em>dissimilarity</em> between neighboring elements $v_i$ and $v_j$. In the case of image segmentation, the elements in V are <em>pixels</em> and the weight of an edge is some measure of the dissimilarity between the two pixels connected by that edge (e.g., the difference in intensity, color, motion, location or some other local attribute).</p>
<p>In the graph-based approach, a segmentation $S$ is a partition of $V$ into components such that each component (or region) $C ∈ S$ corresponds to a connected component in a graph $G&rsquo; = (V, E&rsquo;)$, where $E&rsquo; ⊆ E$. In other words, any segmentation is induced by a subset of the edges in $E$. There are different ways to measure the quality of a segmentation but in general we want the elements in a component to be similar, and elements in different components to be dissimilar. This means that edges between two vertices in the same component should have relatively low weights, and edges
between vertices in different components should have higher weights.</p>
<p>For example, a pixel $p_i$ corresponds to a vertex $v_i$ and it has 8 neighboring pixels. We can define the weight to be the absolute value of the dissimilarity between the pixel intensity $I(p_i)$ and its neighbors.</p>
<p>$$w((v_i, v_j )) = |I(p_i) − I(p_j )|$$</p>
<p>Before we compute the weights we use the 2D Gaussian kernel / filter we met in the <a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-intro/">introductory section to CNNs</a> - the end result being a smoothing of the image that helps with noise reduction. For color images we run the algorithm for each of the three channels.</p>
<p>There is one runtime parameter for the algorithm, which is the value of $k$ that is used to compute the threshold function $\tau$ . Recall we use the function $τ(C) =k/|C|$ where $|C|$ is the number of elements in C. Thus k effectively sets a scale of observation, in that a larger k causes a preference for larger components.</p>
<p>The graph algorithm can also accommodate dissimilarity weights between neighbors at a <em>feature space</em> rather at <em>pixel</em> level based on a Euclidean distance metric with other distance functions possible.</p>
<p><img src="images/graph-based-segmentation1.png#center" alt="graph-based-segmentation1">
<em>Graph representation of the segmentation problem</em></p>
<p>Notice that the initial segments may be many and do not necessarily accurately represent distinct objects as shown below:</p>
<p><img src="images/graph-based-segmentation2.png#center" alt="graph-based-segmentation2">
<em>Result of the initial segments produced by the graph-based algorithm, $k=300$</em></p>
<h4 id="grouping">Grouping</h4>
<p>After the initial regions are produced, we use a greedy algorithm to iteratively group regions together. This is what gives the <em>hierarchical</em> in the name of the algorithm.</p>
<p>First the similarities between all neighboring regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbors. The process of grouping the most similar regions is repeated until the whole image becomes a single region.</p>
<p>For the similarity $s(r_i ,rj)$ between region $r_i$ and $r_j$ we apply a variety of complementary measures under the constraint that they are
fast to compute. In effect, this means that the similarities should be based on features that can be propagated through the hierarchy, i.e.
when merging region $r_i$ and $r_j$ into $r_t$, the features of region $r_t$ need to be calculated from the features of $r_i$ and $r_j$ without accessing the image pixels. Such feature similarities include color, texture, size, fill - we create a binary sum of such individual similarities.</p>
<p>$$ s(r_i ,r_j) = a_1 s_{color}(r_i ,r_j)+a_2 s_{texture}(r_i ,r_j) + a_3 s_{size}(r_i ,r_j) + a_4 s_{fill}(r_i ,r_j) $$</p>
<p>where $a_i ∈ {0,1}$ denotes if the similarity measure is used or not.</p>
<p>The end result of the grouping is a hierarchy of regions ranked according to creation time. As earlier created regions may end up being the largest some permutation in the ranking is applied.</p>
<p><img src="images/hierarchical-grouping-example.png#center" alt="hierarchical-grouping-example">
<em>Example of hierarchical grouping</em></p>
<p>The above selective search strategy is diversified (1) by using a variety of colour spaces with different invariance properties, (2) by using different similarity measures $s(r_i, r_j)$, and (3) by varying our initial regions. Each strategy results in a different hierarchy and after a permutation that randomizes the ranking the final list of regions is produced. For RCNN we use 2000 such region proposals.</p>
<p><img src="images/region-proposals.png#center" alt="region-proposals">
<em>Final proposals - in reality we have 2000 of such regions.</em></p>
<h3 id="cnn-features--and-svm-classification">CNN Features  and SVM Classification</h3>
<p>Each of these proposals can be fed into a CNN (e.g. <a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/feature-extraction-resnet/">ResNet</a>). Since regions are of various rectangular shapes, we warp the regions to a fixed size (CNNs can process fixed input tensors) of 227 x 227 pixels and the CNN produces a 4096-dim feature vector from each of the regions. Note that this representation of each region by 4096 elements is considered quite compact and more importantly the features are <em>shared</em> across classes.</p>
<p>Using these features we use a binary <a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/classification/svm/">SVM classifier</a> that produces a positive or negative on whether this feature contains the class of interest or not. We train a separate binary SVM classifier per class (binary classification).</p>
<p>The sharing of features allows us to parctically aggregate into a matrix all features of all regions (2000 x 4096) we well as aggregate the SVM weights into another matrix (4096 x K), where K is the number of classes we have trained for, and do this combined operation via a matrix-matrix product.</p>
<p>After the scoring of each proposed region by the SVM we apply a greedy <a href="https://arxiv.org/pdf/1704.04503.pdf">Non-Maximum Suppression (NMS) algorithm</a> <em>for each class</em> independently, that <em>rejects</em> a region if it has an Intersection over Union (IoU) metric higher than a threshold with a higher scoring region. This threshold was empirically determined to be 0.3 for the task outlined in the paper. But it is expected to be a hyperparameter in practice.</p>
<p>Finally, a bounding box regressor, predicts the location of the bounding boxes using the proposal boxes and nearby ground truth box data so that we can adjust the final detection box and avoid situations that whole objects are detected but partially included in the detection box.</p>
<p><img src="images/iou.png#center" alt="iou"></p>
<p><img src="images/non-max-supression.png#center" alt="non-max-suppression"></p>
<p>The whole process is partially shown below and the solution is called Region-CNN (RCNN).</p>
<p><img src="images/rcnn.png#center" alt="rcnn">
<em>RCNN pipeline</em></p>
<p><img src="images/selective-search.png#center" alt="selective-search">
<em>RCNN detection result after processing multiple region proposals</em></p>
<h3 id="training">Training</h3>
<p>RCNN training is complex as involves a multi-stage pipeline.</p>
<ol>
<li>R-CNN first fine pretrains a CNN on a classification task without using any bounding box ground truth.</li>
<li>It then adapts the trained CNN by replacing the classification layer with another randomly initialized layer of span K+1, where K are the number of classes in the domain of interest (+1 is due to the fact that we consider the background). We continue training with SGD using (a) as inputs the warped images as determined by the region proposals (b) using this time the ground truth bounding boxes and declaring each region proposal with IoU ? 0.5 relative to the ground truth bounding box as true positive.</li>
<li>Then, it fits SVMs to CNN features. These SVMs act as object detectors, replacing the softmax classifier learnt by fine-tuning.</li>
</ol>
<p>.</p>
<h2 id="fast-rcnn">Fast RCNN</h2>
<p><a href="https://arxiv.org/abs/1504.08083">Fast-RCNN</a> is the second generation RCNN that aimed to accelerate RCNN. Apart from the complex training of RCNN, its inference involved a forward pass for each of the 2000 proposals.</p>
<p><img src="images/fast-rcnn-architecture.png#center" alt="fast-rcnn-architecture">
<em>Fast RCNN Architecture</em></p>
<p>A Fast RCNN network takes as input an entire image and a set of proposals $R$. The set of proposals is produced by the selective search alg used in RCNN and its similarly around 2000 per image.</p>
<ol>
<li>
<p>The network first processes the whole image with a CNN having several convolutional (experiments were done for 5-13 layers) and 5 max pooling layers to produce a feature map. The selective search met in RCNN, produces region proposals and <em>for each proposal</em>, a region of interest (RoI) pooling layer (see below) extracts a fixed-length feature vector from the feature map. This is in contrast to the RCNN that fed the different proposals to the CNN. Now we only have one feature map and we elect regions of interest from that.</p>
</li>
<li>
<p>Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.</p>
</li>
</ol>
<p>The key element in the architecture is the <strong>RoI pooling layer</strong>.</p>
<p>What is an RoI? An RoI is a rectangular window into a feature map. Each RoI is
defined by a four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w).</p>
<p>The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g.7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI. RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling</p>
<p>As you may have noticed we have replaced the SVN and the training can happen end to end starting from a pretrained CNN and using a multi-task (classification, regression) loss function. NMS is maintained just like in RCNN to produce the final box.</p>
<h2 id="faster-rcnn">Faster RCNN</h2>
<p>With <a href="https://arxiv.org/abs/1506.01497">Faster RCNN</a>, we are <em>not</em> making changes to Fast RCNN detector itself bur rather to the input of the CNN. The selective search algorithm that is considered slow and computationally expensive is replaced with a neural network called the Region Proposal Network (RPN) that as the name implies proposes regions.</p>
<p><img src="images/faster-rcnn.png" alt="faster-rcnn">
<em>Faster RCNN Architecture - the RPN tells the Fast RCNN detector where to attend to</em></p>
<p>Therefore, in this architecture there is <em>one</em> CNN network that does not only produces a <strong>global</strong> feature map but also it produces proposals from the feature map itself rather than the original image.</p>
<p><img src="images/rpn.png#center" alt="rpn">
<em>Region Proposals as generated by the RPN network</em></p>
<p>It is doing so by sliding a window n x n over the feature map. At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal.</p>
<p>The k proposals are parameterized relative to k reference boxes, which we call
<strong>anchor boxes</strong>. The size can be changed but the original paper used anchor size of (128 × 128, 256 × 256, 512 × 512) and three aspect ratios (1:1, 1:2 and 2:1). An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio. By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a feature map of a size
W × H (typically ∼2,400), there are $W xH x k$ anchors in total.</p>
<p>The RPN network produces a classification score i.e. how confident we are that there is an object for each of the anchor boxes as well as the regression on the anchor box coordinates.</p>
<p><img src="images/faster-rcnn-inference-speedup.png#center" alt="faster-rcnn-inference-speedup"></p>
<p>Finally, the following curve (focus on COCO) presents the relative performance between
<img src="images/object-detection-performance.png#center" alt="object-detection-performance"></p>
</article>
 

      <footer class="book-footer">
        
  <div class="flex justify-between">



  <div>
    
    <a class="flex align-center" href="https://github.com/pantelis/cs-gy-6613-spring-2020/commit/1fd5708880ff17dff0d1045c7056647d5aced071" title='Last modified by MONOGIOUDIS Pantelis | Mar 2, 2020' target="_blank" rel="noopener">
      <img src="/cs-gy-6613-spring-2020/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>Mar 2, 2020</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/pantelis/cs-gy-6613-spring-2020/edit/master/cs-gy-6613-spring-2020/content/docs/lectures/scene-understanding/object-detection/_index.md" target="_blank" rel="noopener">
      <img src="/cs-gy-6613-spring-2020/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>Edit this page</span>
    </a>
  </div>

</div>

 
        
  
 
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#object-detection">Object Detection</a>
      <ul>
        <li><a href="#region-cnn-rcnn">Region CNN (RCNN)</a>
          <ul>
            <li><a href="#region-proposals">Region Proposals</a>
              <ul>
                <li><a href="#graph-based-segmentation">Graph-based segmentation</a></li>
                <li><a href="#grouping">Grouping</a></li>
              </ul>
            </li>
            <li><a href="#cnn-features--and-svm-classification">CNN Features  and SVM Classification</a></li>
            <li><a href="#training">Training</a></li>
          </ul>
        </li>
        <li><a href="#fast-rcnn">Fast RCNN</a></li>
        <li><a href="#faster-rcnn">Faster RCNN</a></li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  <!DOCTYPE html> 

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
  integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
  crossorigin="anonymous">

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
  integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
  crossorigin="anonymous"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
  crossorigin="anonymous" onload="renderMathInElement(document.body);">
</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false}
      ],
      macros: {
        
        "\\pdata": "p_{data}",
        
        "\\ptrain": "\\hat{p}_{data}",
        "\\Ptrain": "\\hat{P}_{data}",
        
        "\\pmodel": "p_{model}",
        "\\Pmodel": "P_{model}",
        "\\ptildemodel": "\tilde p_{model}",
        
        "\\pencode": "p_{encoder}",
        "\\pdecode": "p_{decoder}",
        "\\precons": "p_{reconstruct}"
      }
    });
  });
</script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"
          integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous">
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>

<script>
  pseudocode.renderElement(document.getElementById("forward-search"));
</script>


</html>


</body>

</html>












