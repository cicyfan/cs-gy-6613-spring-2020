<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Model-free Prediction"><meta property="og:title" content="Model-free Prediction" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/drl/prediction/" />

<title>Model-free Prediction | CS-GY-6613 Spring 2020</title>
<link rel="icon" href="/cs-gy-6613-spring-2020/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/cs-gy-6613-spring-2020/book.min.ccf2e3f83640a9c2266488b947be5b9ddbdea671b0de107d1d379f1d6f6d0408.css" integrity="sha256-zPLj&#43;DZAqcImZIi5R75bndvepnGw3hB9HTefHW9tBAg=">


<script defer src="/cs-gy-6613-spring-2020/en.search.min.a8ea8095a1881edc65431e00622ad69104cdca6261af8043f9c60666f3f7eec0.js" integrity="sha256-qOqAlaGIHtxlQx4AYirWkQTNymJhr4BD&#43;cYGZvP37sA="></script>

<link rel="alternate" type="application/rss+xml" href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/drl/prediction/index.xml" title="CS-GY-6613 Spring 2020" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/cs-gy-6613-spring-2020"><img src="/cs-gy-6613-spring-2020/tandon_long_black.png" alt="Logo" /><span>CS-GY-6613 Spring 2020</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





  

  
  





 
  
    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/syllabus/" >
      Syllabus
  </a>

</li>
      
    
      
        

  <li >
    
      <span>Lecture 1 - Introduction to AI</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/course-introduction/" >
      Course Introduction
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/systems-approach/" >
      A systems approach to AI
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Systems Approach Slides</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/ai-pipelines/" >
      The Way of Working in AI
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Ai Pipelines Slides</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ai-intro/agents/" >
      Intelligent Agents and Representations
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Agents Slides</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/learning-problem/" >
      Lecture 2a - The Learning Problem
  </a>


    

    




  
  <ul>
    
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 2b - Regression</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/regression/linear-regression/" >
      Linear Regression
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/" >
      Lecture 2c - Linear Classification
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/knn/" >
      k-Nearest Neighbors (kNN) Classification
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/perceptron/" >
      The Perceptron
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/logistic-regression/" >
      Logistic Regression
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/kernels/" >
      Kernels and the Kernel Trick
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/clustering/" >
      K-means Clustering
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/" >
      Support Vector Machines
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/face-recognition/" >
      Face Recognition - SVM Case Study
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/mnist/" >
      MNIST Classification - SVM Case Study
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/svm/iris/" >
      Iris Classification - SVM Case Study
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/decision-trees/" >
      Decision Trees
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/classification/random-forests/" >
      Random Forests
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/" >
      Lecture 3 - Deep Neural Networks
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-intro/" >
      Introduction to Backpropagation
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/backprop-dnn/" >
      Backpropagation in Deep Neural Networks
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Regularization in Deep Neural Networks</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/dnn/fashion-mnist-case-study/" >
      Fashion MNIST Case Study
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 4 - Convolutional Neural Networks</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-intro/" >
      Introduction to Convolutional Neural Networks
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-arch/" >
      CNN Architectures
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/cnn/cnn-sizing/" >
      CNN Sizing
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Attention Mechanisms in CNNs</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 5 - Scene Understanding</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/scene-understanding-intro/" >
      Introduction to Scene Understanding
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/feature-extraction-resnet/" >
      Feature Extraction via Residual Networks
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/scene-understanding/object-detection/" >
      Object Detection
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Semantic Segmentation</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Lecture 6 - Probabilistic Graphical Models</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/pgm-intro/" >
      Introduction to Probabilistic Reasoning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/bayesian-inference/" >
      Inference in Graphical Models
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/recursive-state-estimation/" >
      Recursive State Estimation
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/pgm/hmm-localization/" >
      Localization and Tracking
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/planning/" >
      Lecture 7 - Planning
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/planning/propositional-logic/" >
      Propositional Logic
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/planning/logical-agents/" >
      Logical Agents
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/planning/classical-planning/" >
      Classical Planning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Linear Temporal Logic</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/planning/search/" >
      Planning with Search
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Autonomous Agent Planning Application</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Online Prediction</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Behavioral Planning</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Trajectory Generation</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/mdp/" >
      Lecture 8 - Markov Decision Processes
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/mdp/mdp-intro/" >
      Introduction to MDP
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/mdp/policy-iteration/" >
      Policy Iteration
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/mdp/value-iteration/" >
      Value Iteration
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/mdp/mdp-slides/" >
      Mdp Slides
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/drl/" >
      Lecture 9 - Reinforcement Learning
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/drl/prediction/"  class="active">
      Model-free Prediction
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/drl/control/" >
      Model-free Control
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/drl/reinforce/" >
      REINFORCE
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/drl/sarsa/" >
      SARSA
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/" >
      Lecture 10 - Recurrent Neural Networks (RNN)
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/introduction/" >
      Introduction to Recurrent Neural Networks (RNN)
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/simple-rnn/" >
      Simple RNNs and their Backpropagation
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/rnn/lstm/" >
      The Long Short-Term Memory (LSTM) Cell Architecture
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/nlp/" >
      Lecture 11/12 - Natural Language Processing
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/nlp/nlp-intro/" >
      Introduction to NLP
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/nlp/word2vec/" >
      Word Embeddings
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Neural Language Models</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Neural Machine Translation</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/" >
      Projects
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/continuous-learning/" >
      Continual Learning for Robotic Perception
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/pneumonia/" >
      Explainable COVID-19 Pneumonia
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/going-back-to-work/" >
      Going Back to Work
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/semantic-code-search/" >
      Semantic Code Search
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/projects/imu-classification/" >
      Surface Type Classification
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/projects/env/" >
      Your Programming Environment
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Background - Math for ML</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/linear-algebra/" >
      Linear Algebra for Machine Learning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/optimization/" >
      Optimization and Stochastic Gradient Descent
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/probability/" >
      Probability and Information Theory Basics
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-math/netflix/" >
      The Netflix Prize and Singular Value Decomposition
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/" >
      Background - ML Frameworks
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/numpy-pandas/" >
      Numerical Python (Numpy/Scipy and Pandas) Tutorials
  </a>

</li>
      
    
      
        <li>

  <a href="/cs-gy-6613-spring-2020/docs/lectures/ml-frameworks/tensorflow-introduction/" >
      Introduction to Tensorflow
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Drafts</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Constraint Satisfaction Programming</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Partially Observed MDP</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Transfer Learning</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Generative Modeling and Continuous Variational Auto Encoders (VAE)</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Embodied AI Simulation</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Knowledge Representations</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Robotic Language Grounding</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/resources/" >
      Resources
  </a>


    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/resources/pgm-study-guide/" >
      What you need to know on Probabilistic Graphical Models
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/resources/mdp-study-guide/" >
      What you need to know on MDP &amp; RL
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      

  <a href="/cs-gy-6613-spring-2020/docs/resources/planning-study-guide/" >
      What you need to know on Planning
  </a>


    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  











</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/cs-gy-6613-spring-2020/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Model-free Prediction</strong>

  <label for="toc-control">
    <img src="/cs-gy-6613-spring-2020/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#model-free-prediction">Model-free Prediction</a>
      <ul>
        <li><a href="#monte-carlo-mc-learning">Monte-Carlo (MC) Learning</a></li>
        <li><a href="#temporal-difference-td-approximations">Temporal Difference (TD) Approximations</a>
          <ul>
            <li><a href="#the-tdlambda">The TD($\lambda$)</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      
<article class="markdown"><h1 id="model-free-prediction">Model-free Prediction</h1>
<p>In this chapter we find optimal policy solutions when the MDP is <em>unknown</em> and we need to <em>learn</em> its underlying value functions - also known as the  <em>model free</em> prediction problem.  The main idea here is to learn value functions via sampling. These methods are in fact also applicable when the MDP is known but its models are simply too large to use the approaches outlined in the  <a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/mdp/">MDP chapter</a>. The two sampling approaches we will cover here are (incremental) Monte-Carlo (MC) and Temporal Difference (TD).</p>
<h2 id="monte-carlo-mc-learning">Monte-Carlo (MC) Learning</h2>
<p>The the MC learning approach for every state at time $t$ we sample one complete trajectory as shown below.</p>
<p><img src="images/mc-value-iteration-tree.png#center" alt="mc-value-iteration-tree">
<em>Backup tree with value iteration based on the MC approach. MC samples a complete trajectory to the goal node T shown with red.</em></p>
<p>There is some rationale of doing so, if we recall that the state-value function that was defined in the introductory <a href="http://pantelis.github.io/cs-gy-6613-spring-2020/docs/lectures/mdp/mdp-intro/">MDP section</a> i.e. the <em>expected</em> return.</p>
<p>$$v_\pi(s) = \mathop{\mathbb{E}_\pi}(G_t | S_t=s)$$</p>
<p>can be approximated by using the <em>sample mean</em> return over a <em>sample</em> episode / trajectory:</p>
<p>$$G_t(\tau) = \sum_{k=0}^{T-1}\gamma^k R_{t+1+k}$$</p>
<p>The value function is therefore approximated in <em>Monte-Carlo</em>, by the sample mean of the returns over multiple episodes / trajectories. In other words, to update each element of the state value function</p>
<ol>
<li>For each time step $t$ that state $S_t$ is visited in an episode
<ul>
<li>Increment a counter $N(S_t)$ of visitations</li>
<li>Calculate the total return $S(S_t) = S(S_t) + G_t$</li>
</ul>
</li>
<li>At the end of multiple episodes, the value is estimated as $V(S_t) = S(S_t) / N(S_t)$</li>
</ol>
<p>As $N(S_t) \rightarrow ∞$ the estimate will converge to $V(S_t) \rightarrow v_\pi(s)$. **Notice that we started using capital letters for the _estimates_ of the value functions.**</p>
<p>But we can also do the following trick, called <em>incremental mean approximation</em>:</p>
<p>$$ \mu_k = \frac{1}{k} \sum_{j=1}^k x_j = \frac{1}{k} \left( x_k + \sum_{j=1}^{k-1} x_j \right)$$
$$ = \frac{1}{k} \left(x_k + (k-1) \mu_{k-1}) \right) =  \mu_{k-1} + \frac{1}{k} ( x_k - \mu_{k-1} )$$</p>
<p>Using the incremental sample mean, we can approximate the value function after each episode if for each state $S_t$ with return $G_t$,
<blockquote class="book-hint danger">
  <p>$$ N(S_t) = N(S_t) +1 $$
$$ V(S_t) = V(S_t) + \alpha \left( G_t - V(S_t) \right)$$</p>
<p>where $\alpha = \frac{1}{N(S_t)}$ can be interpreted as the forgetting factor.</p>

</blockquote>
</p>
<p>$\alpha$ can also be any number $&lt; 1$ to get into a more flexible sample mean - the <em>running mean</em> that will increase the robustness of this approach in non-stationary environments.</p>
<h2 id="temporal-difference-td-approximations">Temporal Difference (TD) Approximations</h2>
<p><img src="images/td-value-iteration-tree.png#center" alt="td-value-iteration-tree">
<em>Backup tree for value iteration with the TD approach. TD samples a single step ahead as shown with red.</em></p>
<p>Instead of getting an estimated value function at the end of multiple episodes, we can use the incremental mean approximation to update the value function after each step.</p>
<p>Going back to the example of crossing the room optimally, we take one step towards the goal and the we  <em>bootstrap</em> the value function of the state we were in from an estimated return for the remaining trajectory. We repeat this as we go along effectively adjusting the value estimate of the starting state from the true returns we have experienced up to now, gradually grounding the whole estimate as we approach the goal.</p>
<p><img src="images/td-driving-to-work-example.png#center" alt="td-driving-to-work-example">
<em>Two value approximation methods: MC (left), TD (right) as converging in their predictions of the value of each of the states in the x-axis. The example is from a hypothetical commute from office back home. In MC you have to wait until the episode ended (reach the goal) to update the value function at each state of the trajectory. In contrast, TD updates the value function at each state based on the estimates of the total travel time. The goal state is &ldquo;arrive home&rdquo;, while the reward function is time.</em></p>
<p>As you can notice in the figure above the solid arrows in the MC case, adjust the predicted value of each state to the <em>actual</em> return while in the TD case the value prediction happens every step in the way. We call TD for this reason an <em>online</em> learning scheme. Another characteristic of TD is that it does not depend on reaching the goal, it <em>continuously</em> learns. MC does depend on the goal and therefore is <em>episodic</em>. This is important in many mission critical applications eg self-driving cars where you dont wait to &ldquo;crash&rdquo; to apply corrections to your state value based on what you experienced.</p>
<p>Mathematically, instead of using the <em>true</em> return, $G_t$, something that it is possible in the MC as we are trully experiencing the world along a trajectory, TD uses a (biased) <em>estimated</em> return called the <em>TD target</em>: $ R_{t+1} + \gamma V(S_{t+1})$ approximating the value function as:</p>
<blockquote class="book-hint danger">
  $$ V(S_t) = V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)$$
</blockquote>

<p>The difference below is called the <em>TD approximation error</em>,</p>
<p>$$\delta_t = R_{t+1} + \gamma (V(S_{t+1}) - V(S_t))$$</p>
<h3 id="the-tdlambda">The TD($\lambda$)</h3>
<p>The TD approach of the previous section, can be extended to multiple steps. Instead of a single look ahead step we can take multiple successive look ahead steps (n), we will call this TD(n) for now, and at the end of the n-th step, we use the value function at that state to backup and get the value function at the state where we started. Effectively after n-steps our return will be:</p>
<p>$$G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + &hellip; + \gamma^{n-1}R_{t+n} + \gamma_n V(S_n)$$</p>
<p>and the TD(n) learning equation becomes</p>
<p>$$ V(S_t) = V(S_t) + \alpha \left( G^{(n)}_t - V(S_t) \right) $$</p>
<p>We now define the so called $\lambda$-return that combines all n-step return $G_t^{(n)}$ via the weighting function shown below as,</p>
<p>$$G_t^{(n)} = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}$$</p>
<p><img src="images/lambda-weighting-function.png#center" alt="lambda-weighting-function">
<em>$\lambda$ weighting function for TD($\lambda$)</em></p>
<p>the TD(n) learning equation becomes</p>
<blockquote class="book-hint danger">
  $$ V(S_t) = V(S_t) + \alpha \left( G^\lambda_t - V(S_t) \right) $$
</blockquote>

<p>When $\lambda=0$ we get TD(0) learning, while when $\lambda=1$ we get learning that is roughly equivalent to MC. It is instructive to see the difference between MC and TD approaches.</p>
<p><img src="images/td-vs-mc.png#center" alt="td-vs-mc">
<em>TD vs MC approaches to $V(s)$ function estimation. TD is converging much faster but it has larger bias compared to MC.</em></p>
</article>
 

      <footer class="book-footer">
        
  <div class="flex justify-between">



  <div>
    
    <a class="flex align-center" href="https://github.com/pantelis/cs-gy-6613-spring-2020/commit/e05bd7e6e4e192b314aaf79b5df0bc49e78e1475" title='Last modified by MONOGIOUDIS Pantelis | May 1, 2020' target="_blank" rel="noopener">
      <img src="/cs-gy-6613-spring-2020/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>May 1, 2020</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/pantelis/cs-gy-6613-spring-2020/edit/master/cs-gy-6613-spring-2020/content/docs/lectures/drl/prediction/_index.md" target="_blank" rel="noopener">
      <img src="/cs-gy-6613-spring-2020/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>Edit this page</span>
    </a>
  </div>

</div>

 
        
  
 
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#model-free-prediction">Model-free Prediction</a>
      <ul>
        <li><a href="#monte-carlo-mc-learning">Monte-Carlo (MC) Learning</a></li>
        <li><a href="#temporal-difference-td-approximations">Temporal Difference (TD) Approximations</a>
          <ul>
            <li><a href="#the-tdlambda">The TD($\lambda$)</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  <!DOCTYPE html> 

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
  integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
  crossorigin="anonymous">

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
  integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
  crossorigin="anonymous"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
  crossorigin="anonymous" onload="renderMathInElement(document.body);">
</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false}
      ],
      macros: {
        
        "\\pdata": "p_{data}",
        
        "\\ptrain": "\\hat{p}_{data}",
        "\\Ptrain": "\\hat{P}_{data}",
        
        "\\pmodel": "p_{model}",
        "\\Pmodel": "P_{model}",
        "\\ptildemodel": "\tilde p_{model}",
        
        "\\pencode": "p_{encoder}",
        "\\pdecode": "p_{decoder}",
        "\\precons": "p_{reconstruct}"
      }
    });
  });
</script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"
          integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous">
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>

<script>
  pseudocode.renderElement(document.getElementById("forward-search"));
</script>


</html>


</body>

</html>












